<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>60分钟入门pytorch</title>
    <url>/2024/04/08/60%E5%88%86%E9%92%9F%E5%85%A5%E9%97%A8pytorch/</url>
    <content><![CDATA[<h4 id="tensor">Tensor</h4>
<p>Tensor,张量，是PyTorch中的核心类，我们平常所说的张量通常是张量类的实例，张量即n维数组，支持GPU加速计算和自动微分。</p>
<ul>
<li><p>初始化tensor的方法</p>
<ul>
<li><ol type="1">
<li>直接从数据创建</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>],[<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br><span class="line"><span class="built_in">type</span>(data),<span class="built_in">type</span>(x_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(<span class="built_in">list</span>, torch.Tensor)</span><br></pre></td></tr></table></figure>
<p>其中，torch.tensor()函数可以将数据转换为torch.Tensor()类型；</p></li>
<li><ol start="2" type="1">
<li>从numpy的array转换过来</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = [[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]</span><br><span class="line">np_array = np.array(data)</span><br><span class="line">x = torch.from_numpy(np_array)</span><br><span class="line"><span class="built_in">type</span>(data),<span class="built_in">type</span>(np_array),<span class="built_in">type</span>(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(<span class="built_in">list</span>, numpy.ndarray, torch.Tensor)</span><br></pre></td></tr></table></figure></li>
<li><ol start="3" type="1">
<li>根据其他tensor样式创建</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_zeros = torch.zeros_like(x_data)</span><br><span class="line">x_ones = torch.ones_like(x_data)</span><br><span class="line">x_rand = torch.rand_like(x_data)</span><br><span class="line">x_zeros,x_ones,x_rand</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(tensor([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>]]),</span><br><span class="line"> tensor([[<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>]]),</span><br><span class="line"> tensor([[<span class="number">0.5286</span>, <span class="number">0.8992</span>],</span><br><span class="line">         [<span class="number">0.7840</span>, <span class="number">0.2935</span>]]))</span><br></pre></td></tr></table></figure>
<p>这三个函数的参数必须是torch.Tensor</p></li>
<li><ol start="4" type="1">
<li>从shape元组创建</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">shape = (<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">x_zeros_ = torch.zeros(shape)</span><br><span class="line">x_ones_ = torch.ones(shape)</span><br><span class="line">x_rand_ = torch.rand(shape)</span><br><span class="line">x_zeros_,x_ones_,x_rand_,x_zeros_.shape,x_ones_.shape,x_rand_.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(tensor([[[<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">          [<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">          [<span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"> </span><br><span class="line">         [[<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">          [<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">          [<span class="number">0.</span>, <span class="number">0.</span>]]]),</span><br><span class="line"> tensor([[[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">1.</span>]],</span><br><span class="line"> </span><br><span class="line">         [[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">1.</span>]]]),</span><br><span class="line"> tensor([[[<span class="number">0.7854</span>, <span class="number">0.6111</span>],</span><br><span class="line">          [<span class="number">0.8331</span>, <span class="number">0.9508</span>],</span><br><span class="line">          [<span class="number">0.2372</span>, <span class="number">0.5213</span>]],</span><br><span class="line"> </span><br><span class="line">         [[<span class="number">0.6088</span>, <span class="number">0.8252</span>],</span><br><span class="line">          [<span class="number">0.0571</span>, <span class="number">0.2459</span>],</span><br><span class="line">          [<span class="number">0.9353</span>, <span class="number">0.4851</span>]]]),</span><br><span class="line"> torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>]),</span><br><span class="line"> torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>]),</span><br><span class="line"> torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>]))</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><p>Tensor的属性：shape形状，dtype元素数据类型，device存储设备</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(x_zeros.shape,x_zeros.dtype,x_zeros.device)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.Size([<span class="number">2</span>, <span class="number">2</span>]) torch.int64 cpu</span><br></pre></td></tr></table></figure></li>
<li><p>转换数据的device</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor = torch.ones((<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(tensor.device)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">  tensor = tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor.device)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cpu</span><br><span class="line">cuda:<span class="number">0</span></span><br></pre></td></tr></table></figure></li>
<li><p>pytorch切片与索引操作和python相同</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor = torch.ones(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">tensor[:,<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure></li>
<li><p>torch.cat([a,b,c],dim=n),按照第n维度连接张量a,b,c,维度为0表示按行连接，维度为1表示按列连接</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习、编程语言</tag>
      </tags>
  </entry>
  <entry>
    <title>C++初级知识</title>
    <url>/2024/04/06/C-%E5%88%9D%E7%BA%A7%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<ul>
<li>C++语言对大小写敏感，所有C++语句须以<code>;</code>结尾，它作为终止符是C++语句结束的标志；</li>
<li>函数名前面的部分叫做函数返回类型，后面的部分叫做形参列表，<code>int main()</code>描述的是<code>main()</code>与操作系统之间的接口，<code>void</code>表示不返回信息或不接受参数；</li>
<li>可以将函数、类等对象封装在名称空间中，调用时指定即可，如<code>std::cout</code>，<code>ros::NodeHandle n;</code>等，在前面提前声明名称空间<code>using namespace std;using namespace ros;</code>,在后续使用时可以省略;</li>
<li><code>cout&lt;&lt;"ABC"</code>的本质：<code>cout</code>是<code>iostream</code>文件中预定义的对象，这条语句的本质是cout对象将字符串”ABC“插入到输出流中；</li>
<li>运算符重载：对于同一个运算符，编译器可以通过上下文来确定运算符的含义从而实现不同的功能；</li>
<li>C++中，<code>endl</code>和<code>\n</code>均可表示换行，其中<code>\n</code>须在字符串中使用；</li>
<li>变量声明：1.分配指定数据类型的内存
2.指定存储在这个内存中的数据的名称；</li>
<li>声明包括定义声明、引用声明；</li>
<li><strong>类</strong>：类是C++中面向对象编程（Object Oriented
Programming，OOP）的核心概念，类描述它能够表示什么信息和可对数据执行哪些操作，对象则是根据类中的数据格式规范创建的实体；</li>
<li><strong>函数初步</strong>：调用函数传递参数给被调用函数，被调用函数返回值以替代调用函数中相应的部分；</li>
<li></li>
</ul>
]]></content>
      <categories>
        <category>C++学习</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2024/03/31/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a
href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a
href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="hello-hexo">Hello Hexo</h2>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
  </entry>
  <entry>
    <title>第一章、机器学习基础</title>
    <url>/2024/04/05/%E7%AC%AC%E4%B8%80%E7%AB%A0%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h3 id="线性模型">1.1 线性模型</h3>
<h4 id="线性回归linear-regression">1.1.1 线性回归（Linear
Regression）</h4>
<p>线性回归简单理解即”拟合一条曲线“，可通过<span
class="math inline">\(x\)</span>的值预测<span
class="math inline">\(y\)</span>值 <span class="math display">\[
\hat{y}=f(x;\hat{w},\hat{b})=x^T\hat{w}+\hat{b}
\]</span></p>
<ul>
<li><p>训练集：用于优化模型参数</p></li>
<li><p>验证集：用于优化模型超参数，如学习率、正则化系数等；</p></li>
<li><p>测试集：用于评估模型性能</p>
<p><strong>线性回归从零开始实现</strong>，<a
href="https://github.com/kumudlakara/Medium-codes/blob/main/linear_regression/house_price_data.txt">数据集下载</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># variables to store mean and standard deviation for each feature</span></span><br><span class="line">mu = []</span><br><span class="line">std = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># load data from the filename</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">filename</span>):</span><br><span class="line">	df = pd.read_csv(filename, sep=<span class="string">&quot;,&quot;</span>, index_col=<span class="literal">False</span>)</span><br><span class="line">	df.columns = [<span class="string">&quot;house size&quot;</span>, <span class="string">&quot;rooms&quot;</span>, <span class="string">&quot;price&quot;</span>]</span><br><span class="line">	data = np.array(df, dtype=<span class="built_in">float</span>)</span><br><span class="line">	plot_data(data[:, :<span class="number">2</span>], data[:, -<span class="number">1</span>])</span><br><span class="line">	normalize(data)</span><br><span class="line">	<span class="keyword">return</span> data[:, :<span class="number">2</span>], data[:, -<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># draw the data[house size,price]</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_data</span>(<span class="params">x, y</span>):</span><br><span class="line">	plt.xlabel(<span class="string">&#x27;house size&#x27;</span>)</span><br><span class="line">	plt.ylabel(<span class="string">&#x27;price&#x27;</span>)</span><br><span class="line">	plt.plot(x[:, <span class="number">0</span>], y, <span class="string">&#x27;bo&#x27;</span>)</span><br><span class="line">	plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># normalize the data</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize</span>(<span class="params">data</span>):</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, data.shape[<span class="number">1</span>] - <span class="number">1</span>):</span><br><span class="line">		mu.append(np.mean(data[:, i]))</span><br><span class="line">		std.append(np.std(data[:, i]))</span><br><span class="line">		data[:, i] = ((data[:, i] - np.mean(data[:, i])) / np.std(data[:, i]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># matrix multiply</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">h</span>(<span class="params">x, theta</span>):</span><br><span class="line">	<span class="keyword">return</span> np.matmul(x, theta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate the cost_function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cost_function</span>(<span class="params">x, y, theta</span>):</span><br><span class="line">	<span class="keyword">return</span> ((h(x, theta) - y).T @ (h(x, theta) - y)) / (<span class="number">2</span> * y.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate the gradient</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">x, y, theta, learning_rate=<span class="number">0.1</span>, num_epochs=<span class="number">10</span></span>):</span><br><span class="line">	m = x.shape[<span class="number">0</span>]</span><br><span class="line">	J_all = []</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">		h_x = h(x, theta)</span><br><span class="line">		cost_ = (<span class="number">1</span> / m) * (x.T @ (h_x - y))</span><br><span class="line">		theta = theta - (learning_rate) * cost_</span><br><span class="line">		J_all.append(cost_function(x, y, theta))</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> theta, J_all</span><br><span class="line"></span><br><span class="line"><span class="comment"># draw the change of the cost</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_cost</span>(<span class="params">J_all, num_epochs</span>):</span><br><span class="line">	plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">	plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">	plt.plot(num_epochs, J_all, <span class="string">&#x27;m&#x27;</span>, linewidth=<span class="string">&quot;5&quot;</span>)</span><br><span class="line">	plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">theta, x</span>):</span><br><span class="line">	x[<span class="number">0</span>] = (x[<span class="number">0</span>] - mu[<span class="number">0</span>]) / std[<span class="number">0</span>]</span><br><span class="line">	x[<span class="number">1</span>] = (x[<span class="number">1</span>] - mu[<span class="number">1</span>]) / std[<span class="number">1</span>]</span><br><span class="line">	y = theta[<span class="number">0</span>] + theta[<span class="number">1</span>] * x[<span class="number">0</span>] + theta[<span class="number">2</span>] * x[<span class="number">1</span>]</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;Price of house: &quot;</span>, y[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x, y = load_data(<span class="string">&quot;house_price_data.txt&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x和y的形状分别为&quot;</span>,x.shape,y.shape)</span><br><span class="line">y = np.reshape(y, (<span class="number">46</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 加一列全1向量</span></span><br><span class="line">x = np.hstack((np.ones((x.shape[<span class="number">0</span>], <span class="number">1</span>)), x))</span><br><span class="line">theta = np.zeros((x.shape[<span class="number">1</span>], <span class="number">1</span>))</span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">num_epochs = <span class="number">10000</span></span><br><span class="line">theta, J_all = gradient_descent(x, y, theta, learning_rate, num_epochs)</span><br><span class="line">J = cost_function(x, y, theta)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Cost: &quot;</span>, J)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Parameters: &quot;</span>, theta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># for testing and plotting cost</span></span><br><span class="line">n_epochs = []</span><br><span class="line">jplot = []</span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;J_all形状&quot;</span>,np.array(J_all).shape)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> J_all:</span><br><span class="line">	jplot.append(i[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">	n_epochs.append(count)</span><br><span class="line">	count += <span class="number">1</span></span><br><span class="line">jplot = np.array(jplot)</span><br><span class="line">n_epochs = np.array(n_epochs)</span><br><span class="line">plot_cost(jplot, n_epochs)</span><br><span class="line"></span><br><span class="line">test(theta, [<span class="number">1203</span>, <span class="number">3</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="逻辑斯蒂回归logistic-regression">1.1.2 逻辑斯蒂回归(Logistic
Regression)</h4>
<p>逻辑斯蒂回归=线性回归+<span
class="math inline">\(sigmoid\)</span>函数，主要用于处理二分类问题，<span
class="math inline">\(sigmoid\)</span>函数可将任何实数映射到0和1之间，以表示属于两类中某一类别的概率，可设置阈值<span
class="math inline">\(\delta\)</span>进行类别的最终判断。 <span
class="math display">\[
f(x;w,b)=sigmoid(y)=sigmoid(x^Tw+b)=\frac{1}{1+e^{-(x^Tw+b)}}
\]</span></p>
<h4 id="交叉熵损失函数cross-entropy-loss-function">1.1.3
交叉熵损失函数(Cross Entropy Loss Function)</h4>
<p><strong>KL散度</strong>，也称相对熵，用于衡量两个概率分布之间的差异，在离散情况下，使用向量<span
class="math inline">\(p=[p_1,p_2,\cdot\cdot\cdot,p_m]^T\)</span>,<span
class="math inline">\(q=[q_1,q_2,\cdot\cdot\cdot,q_m]^T\)</span></p>
<p>表示两个<span class="math inline">\(m\)</span>维的离散概率分布，则
<span class="math display">\[
KL(p,q)=H(p,q)-H(p)=\sum_{j=1}^{m} p_j\cdot ln\frac{p_j}{q_j}
\]</span> 其中交叉熵<span
class="math inline">\(H(p,q)=-\sum_{j=1}^{m}p_j\cdot lnq_j\)</span>,</p>
<p>信息熵<span class="math inline">\(H(p)=-\sum_{j=1}^{m}p_j\cdot
lnp_j\)</span></p>
<p>当概率分布<span
class="math inline">\(p\)</span>固定时，也就是说我们要让<span
class="math inline">\(q\)</span>尽量接近<span
class="math inline">\(p\)</span>时,最小化交叉熵即可，这也就是为什么交叉熵损失函数有效的原因。</p>
<h4 id="softmax分类器">1.1.4 softmax分类器</h4>
<p>softmax分类=线性函数+softmax激活函数</p>
<p>其中线性函数的结果为向量，再通过softmax将这个向量映射到加和为1的概率分布；
<span class="math display">\[
\pi \in R^k = softmax(z\in R^k)=softmax(W\in R^{k\times d} \cdot x\in
R^{d\times1})+b\in R^{k\times1}
\]</span> 其中<span
class="math inline">\(softmax(z)=\frac{1}{\sum\limits_{l=1}^{k}exp(z_l)}[exp(z_1),exp(z_2),\cdot\cdot\cdot,exp(z_k)]\)</span></p>
<p>通常情况下，需要对矩阵<span
class="math inline">\(W\)</span>和向量<span
class="math inline">\(b\)</span>做规范化，使得<span
class="math inline">\(\sum\limits_{j=1}^{k}w_j=0,\sum\limits_{j=1}^{k}b_j=0\)</span>,<span
class="math inline">\(w_j\)</span>为矩阵<span
class="math inline">\(W\)</span>的第<span
class="math inline">\(j\)</span>行，即矩阵<span
class="math inline">\(W\)</span>的每列和为0，向量<span
class="math inline">\(b\)</span>和为0，这可以通过全员减去平均值再除以标准差来达成。</p>
<h5 id="常见的数据标准化方法总结">常见的数据标准化方法总结：</h5>
<ul>
<li><p><span class="math inline">\(min\_max\)</span>: <span
class="math inline">\(x_{new}=\frac{x-x_{min}}{x_{max}-x_{min}}\)</span>,映射到<span
class="math inline">\([0,1]\)</span>;</p></li>
<li><p><span class="math inline">\(z\_score\)</span>: <span
class="math inline">\(x_{new}=\frac{x-\mu}{\delta}\)</span>,映射到标准正态分布；</p></li>
<li><p>正数归一化： <span
class="math inline">\(x_{new}=\frac{x}{x_1+x_2+...+x_n}\)</span>,映射到<span
class="math inline">\([0,1]\)</span>;</p></li>
<li><p>中心化： <span
class="math inline">\(x_{new}=x-\mu\)</span>,使得均值为0</p></li>
</ul>
<h3 id="神经网络简介">1.2 神经网络简介</h3>
<h4 id="全连接层感知机">1.2.1 全连接层（感知机）</h4>
<p><span class="math display">\[
{x}’=\sigma(z)=\sigma(Wx+b)
\]</span></p>
<p>即线性函数+激活函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights);</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br><span class="line">d2l.predict_ch3(net,test_iter)</span><br></pre></td></tr></table></figure>
<h4 id="卷积神经网络">1.2.2 卷积神经网络</h4>
<p>卷积神经网络的输入通常是矩阵或三阶张量，CNN从中提取特征并输出提取的特征向量。</p>
<h3 id="梯度下降gdgradient-descent">1.3 梯度下降（GD，gradient
descent）</h3>
<ul>
<li>目标函数关于某个参数变量的梯度的形状一定与这个参数变量的形状相同；</li>
<li>梯度的方向是函数上升最快的方向，因此其负方向是下降最快的方向；</li>
<li><strong>1.梯度下降</strong>：每epoch计算所有样本的损失函数的平均再做梯度下降；（用于非凸问题存在鞍点，且计算量为SGD的n倍）</li>
<li><strong>2.随机梯度下降</strong>：每epoch从样本集合中选取一个样本计算损失函数再做梯度下降；</li>
<li><strong>3.小批量随机梯度下降</strong>：每epoch从样本集合中随机抽取batch_size个样本计算损失函数求平均再做梯度下降；</li>
<li>反向传播：任何一个计算过程都可以构建其计算图，从计算图尾部用梯度下降向前传播，更新参数；</li>
</ul>
]]></content>
      <categories>
        <category>深度强化学习读书笔记</category>
      </categories>
      <tags>
        <tag>机器学习、强化学习、人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title>斐波那契数列算法总结</title>
    <url>/2024/04/01/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h3 id="斐波那契数列">斐波那契数列：</h3>
<p><span class="math inline">\(\begin{cases}
0, &amp; \text{  } n=0 \\
1,&amp; \text{  } n=1,2 \\
f(n-1)+f(n-2), &amp; \text{  } n&gt;2
\end{cases}\)</span></p>
<h4 id="方法一朴素递归">方法一、朴素递归</h4>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">fib</span><span class="params">(<span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">1</span> || n==<span class="number">2</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">fib</span>(n<span class="number">-1</span>) + <span class="built_in">fib</span>(n<span class="number">-2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>斐波那契数列的通项公式为： <span class="math display">\[
f(n)=\frac{1}{\sqrt{5}} \left [ (\frac{1+\sqrt[]{5} }{2} )^{n}-
(\frac{1-\sqrt[]{5} }{2} )^{n}\right ]
\]</span></p>
<p><img src="/2024/04/01/斐波那契数列算法总结/image-20240401221016279.png"  alt="image-20240401221016279" style="zoom: 55%;" /></p>
<p>如图所示，递归计算的终点都是<span
class="math inline">\(fib(1)\)</span>和<span
class="math inline">\(fib(2)\)</span>，因为它们是直接返回的，因此计算<span
class="math inline">\(fib(n)\)</span>的时间复杂度为计算<span
class="math inline">\(fib(1)\)</span>和<span
class="math inline">\(fib(2)\)</span>的次数，同时也等于<span
class="math inline">\(fib(n)\)</span>本身，因此<span
class="math inline">\(T(n)=O(fib(n))=O(\frac{1}{\sqrt{5}} \left [
(\frac{1+\sqrt[]{5} }{2} )^{n}- (\frac{1-\sqrt[]{5} }{2} )^{n}\right
])=O((\frac{1+\sqrt[]{5} }{2}
)^{n})\)</span>，第二项绝对值小于1，为<span
class="math inline">\(n\)</span>阶无穷小，可舍去，空间复杂度为函数调用栈的高度<span
class="math inline">\(n\)</span>,即<span
class="math inline">\(S(n)=O(n)\)</span>。</p>
<h4 id="方法二尾递归">方法二、尾递归</h4>
<p>首先需要清楚递归和尾递归的区别：</p>
<ul>
<li>1.递归：在调用函数自身后还有事要做，需要保存当前轮次的环境，以供后续返回时使用；</li>
</ul>
<p>如计算自然数前<span class="math inline">\(n\)</span>项和的函数<span
class="math inline">\(sum(n)\)</span>,递归实现方式如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">sum</span><span class="params">(n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> n + <span class="built_in">sum</span>(n<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里的“+",即前面所说”还要做的事“，其计算过程如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sum</span>(<span class="number">5</span>)</span><br><span class="line"><span class="number">5</span> + <span class="built_in">sum</span>(<span class="number">4</span>)</span><br><span class="line"><span class="number">5</span> + (<span class="number">4</span> + <span class="built_in">sum</span>(<span class="number">3</span>))</span><br><span class="line"><span class="number">5</span> + (<span class="number">4</span> + (<span class="number">3</span> + <span class="built_in">sum</span>(<span class="number">2</span>)))</span><br><span class="line"><span class="number">5</span> + (<span class="number">4</span> + (<span class="number">3</span> + (<span class="number">2</span> + <span class="built_in">sum</span>(<span class="number">1</span>))))</span><br><span class="line"><span class="number">5</span> + (<span class="number">4</span> + (<span class="number">3</span> + (<span class="number">2</span> + (<span class="number">1</span> + <span class="built_in">sum</span>(<span class="number">0</span>)))))</span><br><span class="line"><span class="number">5</span> + (<span class="number">4</span> + (<span class="number">3</span> + (<span class="number">2</span> + (<span class="number">1</span> + <span class="number">0</span>))))</span><br><span class="line"><span class="number">5</span> + (<span class="number">4</span> + (<span class="number">3</span> + (<span class="number">2</span> + <span class="number">1</span>)))</span><br><span class="line"><span class="number">5</span> + (<span class="number">4</span> + (<span class="number">3</span> + <span class="number">3</span>))</span><br><span class="line"><span class="number">5</span> + (<span class="number">4</span> + <span class="number">6</span>)</span><br><span class="line"><span class="number">5</span> + <span class="number">10</span></span><br><span class="line"><span class="number">15</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>2.尾递归：每轮直接return，不需要保存当前环境供后续处理。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">sum</span><span class="params">(<span class="type">int</span> n,<span class="type">int</span> total = <span class="number">0</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> total;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(n<span class="number">-1</span>,total+n);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其计算过程如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sum</span>(<span class="number">5</span>, <span class="number">0</span>)</span><br><span class="line"><span class="built_in">sum</span>(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="built_in">sum</span>(<span class="number">3</span>, <span class="number">9</span>)</span><br><span class="line"><span class="built_in">sum</span>(<span class="number">2</span>, <span class="number">12</span>)</span><br><span class="line"><span class="built_in">sum</span>(<span class="number">1</span>, <span class="number">14</span>)</span><br><span class="line"><span class="built_in">sum</span>(<span class="number">0</span>, <span class="number">15</span>)</span><br><span class="line"><span class="number">15</span></span><br></pre></td></tr></table></figure>
<p>利用尾递归求斐波那契数列：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">fib</span><span class="params">(<span class="type">int</span> x1,<span class="type">int</span> x2,<span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">1</span> || n==<span class="number">2</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(n==<span class="number">3</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> x1+x2;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">fib</span>(x2,x1+x2,n<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>计算过程如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">fib</span>(<span class="number">6</span>)=<span class="built_in">fib</span>(<span class="number">1</span>,<span class="number">1</span>,<span class="number">6</span>)</span><br><span class="line"><span class="built_in">fib</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line"><span class="built_in">fib</span>(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">fib</span>(<span class="number">3</span>,<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line"><span class="number">8</span></span><br></pre></td></tr></table></figure>
<p>观察变量<span
class="math inline">\(n\)</span>的变化可知，时间复杂度<span
class="math inline">\(T(n)=O(n)\)</span>,辅助空间只需要<span
class="math inline">\(x1,x2\)</span>,因此空间复杂度<span
class="math inline">\(S(n)=O(1)\)</span>.</p>
<h4 id="方法三非递归迭代">方法三、非递归(迭代)</h4>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">fib</span><span class="params">(<span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(n==<span class="number">1</span>||n==<span class="number">2</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> x=<span class="number">1</span>,y=<span class="number">1</span>;<span class="type">int</span> tmp = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>;i &lt; n<span class="number">-2</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            tmp = x+y;</span><br><span class="line">            x = y;</span><br><span class="line">            y = tmp;  </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> y;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>观察变量<span
class="math inline">\(n\)</span>的变化可知，时间复杂度<span
class="math inline">\(T(n)=O(n)\)</span>,辅助空间只需要<span
class="math inline">\(x,y,i,tmp\)</span>,因此空间复杂度<span
class="math inline">\(S(n)=O(1)\)</span>.</p>
<h4 id="方法四矩阵快速幂">方法四、矩阵快速幂</h4>
<ul>
<li><h5 id="快速幂">快速幂：</h5>
<p>在计算<span class="math inline">\(a^n\)</span>时，若使用<span
class="math inline">\(a^n=a\ \cdot a \ \cdot a...a
(n个)\)</span>方法，则时间复杂度为<span
class="math inline">\(O(n)\)</span>;快速幂的思想是，将<span
class="math inline">\(n\)</span>写成二进制形式</p>
<p><span
class="math inline">\((n_tn_{t-1}...n_1n_0)_2\)</span>,那么<span
class="math inline">\(a^n = a^{n_t\cdot 2^t}*a^{n_{t-1}\cdot
2^{t-1}}*...*a^{n_0\cdot 2^0}\)</span>,其中<span
class="math inline">\(n_i\in\{0,1\}\)</span></p>
<p>因此我们只需要将<span class="math inline">\(2^0\ 2^1....2^{\left
\lfloor log_{2}{n} \right
\rfloor}\)</span>算出，再将二进制位为1对应的幂运算结果相乘即可，时间复杂度为<span
class="math inline">\(O(log n)\)</span>。</p>
<p>比如：<img src="/2024/04/01/斐波那契数列算法总结/image-20240403132154215.png"  alt="image-20240403132154215" style="zoom:67%;" /><img src="/2024/04/01/斐波那契数列算法总结/image-20240403132250525.png"  alt="image-20240403132250525" style="zoom:67%;" /></p>
<p>快速幂分为递归和迭代两种实现方式，两者理论时间复杂度都为<span
class="math inline">\(O(logn)\)</span>,通常情况下，迭代性能较好。</p>
<h5 id="递归版本">(1)递归版本</h5>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">double</span> <span class="title">quickPow</span><span class="params">(<span class="type">double</span> a,<span class="type">long</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (n&lt;<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> (<span class="number">1.0</span>/<span class="built_in">quickPow</span>(a,-n));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> res = <span class="built_in">quickPow</span>(a,n/<span class="number">2</span>);</span><br><span class="line">        <span class="keyword">if</span> (n%<span class="number">2</span>==<span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> res * res *a;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> res * res;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如：计算<span
class="math inline">\(2^5\)</span>的调用及计算过程如下：</p>
<p><img src="/2024/04/01/斐波那契数列算法总结/image-20240403135754758.png"  alt="image-20240403135754758" style="zoom: 25%;" /></p></li>
</ul>
<h5 id="迭代版本">(2)迭代版本</h5>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">double</span> <span class="title">quickPow</span><span class="params">(<span class="type">long</span> <span class="type">long</span> a, <span class="type">long</span>  n)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n&lt;<span class="number">0</span>) <span class="keyword">return</span> <span class="built_in">quickPow</span>(a,-n);</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> res = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (n &gt; <span class="number">0</span>) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (n &amp; <span class="number">1</span>)<span class="comment">//相当于b%2==1</span></span><br><span class="line">        &#123;</span><br><span class="line">            res = res * a;</span><br><span class="line">        &#125;</span><br><span class="line">        a = a * a;</span><br><span class="line">        n &gt;&gt;= <span class="number">1</span>;<span class="comment">//右移一位相当于b=b/2;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>如计算<span class="math inline">\(2^{10}\)</span>的迭代过程如下：</p>
<p><img src="/2024/04/01/斐波那契数列算法总结/image-20240403142939144.png"  alt="image-20240403142939144" style="zoom: 33%;" /></p>
<ul>
<li><h3 id="快速矩阵幂">快速矩阵幂</h3>
<p>由斐波那契数列的递推公式可知：</p>
<p><span
class="math inline">\(\begin{bmatrix}f(n)\\f(n-1)\end{bmatrix}=\begin{bmatrix}1&amp;1\\1&amp;0\end{bmatrix}\begin{bmatrix}f(n-1)\\f(n-2)\end{bmatrix}=\cdot
\cdot \cdot =\begin{bmatrix}
1&amp;1\\1&amp;0\end{bmatrix}^{n-1}\begin{bmatrix}f(1)\\f(0)\end{bmatrix}=\cdot
\cdot \cdot =\begin{bmatrix}
1&amp;1\\1&amp;0\end{bmatrix}^{n-1}\begin{bmatrix}
1\\0\end{bmatrix}\)</span></p>
<p>故，令<span class="math inline">\(A=\begin{bmatrix}
1&amp;1\\1&amp;0\end{bmatrix}^{n-1}\)</span>，则<span
class="math inline">\(f(n)=A[0][0]\)</span></p>
<p>矩阵快速幂和快速幂的方法和思想一致，都是将先前的计算结果保存下来以供后续使用，减小计算量，只需将<span
class="math inline">\(1\)</span>换为单位矩阵，将常熟换为矩阵<span
class="math inline">\(A\)</span>即可.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 矩阵乘法函数</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">multiply</span><span class="params">(<span class="type">long</span> <span class="type">long</span> a[<span class="number">2</span>][<span class="number">2</span>], <span class="type">long</span> <span class="type">long</span> b[<span class="number">2</span>][<span class="number">2</span>])</span> </span>&#123;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> mul[<span class="number">2</span>][<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; j++) &#123;</span><br><span class="line">            mul[i][j] = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; <span class="number">2</span>; k++)</span><br><span class="line">                mul[i][j] += a[i][k] * b[k][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 将乘法结果复制回a矩阵</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; j++) &#123;</span><br><span class="line">            a[i][j] = mul[i][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 快速矩阵幂算法</span></span><br><span class="line"><span class="function"><span class="type">long</span> <span class="type">long</span> <span class="title">matrixPower</span><span class="params">(<span class="type">long</span> <span class="type">long</span> matrix[<span class="number">2</span>][<span class="number">2</span>], <span class="type">long</span> <span class="type">long</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> result[<span class="number">2</span>][<span class="number">2</span>] = &#123;&#123;<span class="number">1</span>, <span class="number">0</span>&#125;, &#123;<span class="number">0</span>, <span class="number">1</span>&#125;&#125;; <span class="comment">// 单位矩阵</span></span><br><span class="line">    <span class="keyword">while</span> (n &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (n &amp; <span class="number">1</span>)</span><br><span class="line">            <span class="built_in">multiply</span>(result, matrix);</span><br><span class="line">        <span class="built_in">multiply</span>(matrix, matrix);</span><br><span class="line">        n &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算斐波那契数</span></span><br><span class="line"><span class="function"><span class="type">long</span> <span class="type">long</span> <span class="title">fibonacci</span><span class="params">(<span class="type">long</span> <span class="type">long</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> matrix[<span class="number">2</span>][<span class="number">2</span>] = &#123;&#123;<span class="number">1</span>, <span class="number">1</span>&#125;, &#123;<span class="number">1</span>, <span class="number">0</span>&#125;&#125;;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">matrixPower</span>(matrix, n - <span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   </span><br><span class="line">    cout &lt;&lt;<span class="built_in">fibonacci</span>(<span class="number">7</span>) &lt;&lt; endl;</span><br><span class="line">    <span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>分析程序知时间复杂度为<span
class="math inline">\(O(logn)\)</span>,空间复杂度为<span
class="math inline">\(O(1)\)</span></p>
<h3 id="总结">总结</h3>
<p>（1)朴素递归：<span
class="math inline">\(T(n)=O(fib(n))=O(\frac{1}{\sqrt{5}} \left [
(\frac{1+\sqrt[]{5} }{2} )^{n}- (\frac{1-\sqrt[]{5} }{2} )^{n}\right
])=O((\frac{1+\sqrt[]{5} }{2} )^{n})\)</span>,<span
class="math inline">\(S(n)=O(n)\)</span></p>
<ol start="2" type="1">
<li>尾递归：<span class="math inline">\(T(n)=O(n)\)</span>,<span
class="math inline">\(S(n)=O(1)\)</span></li>
</ol>
<p>(3)非递归（迭代）：<span
class="math inline">\(T(n)=O(n)\)</span>,<span
class="math inline">\(S(n)=O(1)\)</span></p>
<p>（4) 矩阵快速幂：<span
class="math inline">\(T(n)=O(logn)\)</span>,<span
class="math inline">\(S(n)=O(1)\)</span></p></li>
</ul>
]]></content>
      <categories>
        <category>编程算法</category>
      </categories>
      <tags>
        <tag>算法、数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>第三章、强化学习基本概念</title>
    <url>/2024/04/01/%E7%AC%AC%E4%B8%89%E7%AB%A0%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<h3 id="马尔可夫决策过程">1. 马尔可夫决策过程</h3>
<p>马尔可夫决策过程：<span class="math inline">\(MDP,Markov Decision
Process\)</span> <strong>智能体</strong>：做动作或决策的主体；
<strong>环境</strong>：与智能体交互的对象；</p>
<h3 id="状态动作奖励">2. 状态、动作、奖励</h3>
<p><strong>状态</strong>：对当前时刻环境的概括,记作<span
class="math inline">\(s_t\)</span>，是做决策的依据；如：棋盘上的格局
<strong>状态空间</strong>：所有可能存在的状态的集合，记作<span
class="math inline">\(\mathcal{S}\)</span>;状态空间可离散、可连续；可有限、可无限
<strong>动作</strong>：智能体基于当前状态所做出的决策，动作的选取可以是确定性的、也可以是随机性的（多数情况下为随机性的），即给定一个概率分布（一个加和为1的概率向量），智能体按照这个概率分布选取一个动作
<strong>动作空间</strong>：所有可能动作的集合，记作<span
class="math inline">\(\mathcal{A}\)</span>;同样，离散、连续、有限、无限皆可
<strong>奖励</strong>：智能体在执行一个动作后，环境返回给智能体的一个数值；奖励函数一般由自己设计及定义，记作<span
class="math inline">\(r(s_t,a_t,s_{t+1})\)</span>或<span
class="math inline">\(r(s_t,a_t)\)</span>;我们总是假设奖励函数是有界的，即对于所有<span
class="math inline">\(a_t\in\mathcal{A}\)</span>, <span
class="math inline">\(s_t,s_{t+1}\)</span>,有<span
class="math inline">\(|r(s_t,a_t,s_{t+1})|&lt;\infty\)</span>，否则得到一个正负无穷大的奖励后就没必要继续了。</p>
<h3 id="状态转移">3.状态转移</h3>
<p><strong>状态转移</strong>：智能体从当前<span
class="math inline">\(t\)</span>时刻的状态<span
class="math inline">\(s\)</span>转移到下一刻的状态<span
class="math inline">\(s&#39;\)</span>的过程；我们用<strong>状态转移函数</strong>来描述状态转移，记作：
<span
class="math display">\[p_t(s&#39;|s,a)=P(S_{t+1}=s&#39;|S_t=s,A_t=a)\]</span>
表示发生下述事件的概率：在当前状态<span
class="math inline">\(s\)</span>,智能体执行动作<span
class="math inline">\(a\)</span>,下一刻环境的状态变成<span
class="math inline">\(s&#39;\)</span>，这个值必不恒等于1，因为状态转移存在随机性。
<strong>确定性状态转移</strong>：环境中不存在随机性，下一个状态<span
class="math inline">\(s&#39;\)</span>完全由<span
class="math inline">\(s,a\)</span>决定： <span
class="math display">\[p_t(s&#39;|s,a)=\begin{cases}
  &amp; \text{ 1 , if } \tau_t(s,a) = s&#39; \\
  &amp; \text{ 0 , otherwise }
\end{cases}\]</span>
<strong>随机性状态转移</strong>：环境中存在随机性，比如，在玛丽欧游戏中你可以控制玛丽欧怎么移动，但敌人怎么移动则无法确定，这就是下一刻状态不确定的缘由。</p>
<h3 id="策略">4.策略</h3>
<p><strong>策略</strong>：如何根据观测到的状态做出决策，即如何从动作空间中选取一个动作。
<strong>随机性策略</strong>：<span
class="math inline">\(\pi(a|s)=P(A=a|S=s)\)</span>,即给定当前状态条件下采取各个动作的概率，也就是加和为1的向量。
<strong>确定性策略</strong>：记作<span
class="math inline">\(\mu:\mathcal{S}\to\mathcal{A}\)</span>,即动作<span
class="math inline">\(a\)</span>完全由状态<span
class="math inline">\(s\)</span>决定:<span
class="math inline">\(a=\mu(s)\)</span></p>
<p><strong>智能体与环境交互的流程</strong>：观测到当前状态<span
class="math inline">\(s\)</span>，用策略<span class="math inline">\(\pi
(a|s)\)</span>算出所有动作的概率并随机抽样，得到其中一个动作<span
class="math inline">\(a\)</span></p>
<p>,环境通过状态转移函数<span
class="math inline">\(p_t(s&#39;|s,a)\)</span>(这也是一个概率分布)随机生成新的状态<span
class="math inline">\(s’\)</span>，并向智能体返回一个奖励<span
class="math inline">\(r(s,a,s’)\)</span>。</p>
<h3 id="马尔可夫性质markov-property">5.马尔可夫性质（Markov
property）</h3>
<p>马尔可夫性，即下一时刻状态<span
class="math inline">\(S_{t+1}\)</span>仅仅依赖于当前状态<span
class="math inline">\(S_t\)</span>和动作<span
class="math inline">\(A_t\)</span>,而不依赖于过去的状态和动作： <span
class="math display">\[P(S_{t+1}|S_t,A_t)=P(S_{t+1}|S_1,A_1,S_2,A_2,...,S_t,A_t)\]</span>
<strong>轨迹</strong>：在一个回合（从开始到结束）中智能体观测到的所有状态、动作、奖励：<span
class="math inline">\(s_1,a_1,r_1,s_2,a_2,r_2,s_3,a_3,r_3...\)</span></p>
<h3 id="回报与折扣回报">6.回报与折扣回报</h3>
<p><strong>回报</strong>：从当前时刻开始到本回合结束所有奖励的总和，也叫作累积奖励。假设本回合在时刻<span
class="math inline">\(n\)</span>结束，则<span
class="math inline">\(t\)</span>时刻的回报定义为： <span
class="math display">\[U_t=R_{t\to end}=R_t+R_{t+1}+...+R_n\]</span>
<strong>折扣回报</strong>：越久远的未来的回报越不重要，所以应该随时间乘上相应的折扣率<span
class="math inline">\(\gamma\in[0,1]\)</span>，折扣回报： <span
class="math display">\[U_t=R_t+\gamma \cdot R_{t+1}+\gamma^2 \cdot
R_{t+2}+...\]</span> 可以将其理解为得到了一个新的奖励函数，<span
class="math inline">\(R_{t+i}=\gamma^i\cdot R_{t+i}\)</span></p>
<h3 id="价值函数重中之重">7.价值函数（重中之重！！！）</h3>
<p>价值函数是回报的期望，价值函数值越大，说明现状越有利；
<strong>动作价值函数</strong>： <span
class="math display">\[Q_{\pi}(s_t,a_t)=E_{S_{t+1},A_{t+1},...,S_n,A_n}[U_t|S_t=s_t,A_t=a_t]\]</span>
表示已经观测到了<span
class="math inline">\(S_t,A_t\)</span>的值，即观测到状态<span
class="math inline">\(s_t\)</span>,选中动作<span
class="math inline">\(a_t\)</span>，原来<span
class="math inline">\(U_t\)</span>中的随机性来自<span
class="math inline">\(t+1\)</span>时刻起所有的状态和动作：<span
class="math inline">\(S_{t+1},A_{t+1},...,S_n,A_n\)</span>,而动作价值函数对它们求期望，简单理解就是找出它们的所有情况，算出<span
class="math inline">\(U_t\)</span>求平均，这样就消除它们的影响。 <span
class="math inline">\(\qquad\)</span><span
class="math inline">\(t\)</span>时刻的动作价值函数<span
class="math inline">\(Q_\pi(s_t,a_t)\)</span>依赖于以下三个因素：
（1）当前状态<span
class="math inline">\(s_t\)</span>：当前状态越好，<span
class="math inline">\(Q_\pi(s_t,a_t)\)</span>越大 （2）当前动作<span
class="math inline">\(a_t\)</span>：智能体执行的动作越好，<span
class="math inline">\(Q_\pi(s_t,a_t)\)</span>越大 （3）策略函数<span
class="math inline">\(\pi\)</span>：<span
class="math inline">\(S_{t+1},A_{t+1},...,S_n,A_n\)</span>由策略决定，所以对它们求期望最终的结果受到策略的影响。
<strong>最优动作价值函数</strong>： 为了排除策略<span
class="math inline">\(\pi\)</span>的影响，可以使： <span
class="math display">\[\pi^*=\mathop{argmax}\limits_{\pi}
Q_\pi(s_t,a_t),\forall s_t\in\mathcal{S},a_t\in\mathcal{A}\]</span>
即选取一个当前为任何状态、执行任何动作的情况下都最优的策略，这样策略就确定了，也就排除了策略<span
class="math inline">\(\pi\)</span>的影响，<span
class="math inline">\(Q_*(s_t,a_t)\)</span>就是最优动作价值函数。
<strong>状态价值函数</strong>: <span
class="math display">\[V_\pi(s_t)=E_{A_t\sim\pi(\cdot|s_t)}[Q_\pi(s_t,A_t)]=\sum_{a\in\mathcal{A}}^{}\pi(a|s_t)
\cdot Q_\pi(s_t,a)\]</span>
状态价值函数可以理解为在动作价值函数的基础上，动作<span
class="math inline">\(A_t\)</span>不再确定，而是随机变量，对动作<span
class="math inline">\(A_t\)</span>求期望以消除动作的影响，使得状态价值函数只依赖于策略函数<span
class="math inline">\(\pi\)</span>和状态<span
class="math inline">\(s_t\)</span>的好坏</p>
<p><strong>两者比较</strong>： <span class="math display">\[
Q_{\pi}(s_t,a_t)=E_{S_{t+1},A_{t+1},...,S_n,A_n}[U_t|S_t=s_t,A_t=a_t]
\]</span></p>
<p><span class="math display">\[
V_{\pi}(s_t)=E_{A_t,S_{t+1},A_{t+1},...,S_n,A_n}[U_t|S_t=s_t]
\]</span></p>
<p>强化学习分为（1）基于模型的方法 （2）无模型方法</p>
<p>无模型方法：价值学习、策略学习</p>
<p>基于模型的方法：AlphaGo</p>
]]></content>
      <categories>
        <category>深度强化学习读书笔记</category>
      </categories>
      <tags>
        <tag>人工智能、强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>第二章、蒙特卡洛方法</title>
    <url>/2024/03/31/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E3%80%81%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<ol type="1">
<li><strong>随机变量</strong>记作<span
class="math inline">\(X\)</span>,<strong>观测值</strong>记作<span
class="math inline">\(x\)</span>,观测值只是数字而已，没有随机性,如<span
class="math inline">\(P(X=0)=\frac{1}{2}\)</span>中的<span
class="math inline">\(X\)</span>为大写；</li>
<li>给定随机变量<span
class="math inline">\(X\)</span>,它的<strong>累积分布函数</strong>（即<strong>概率分布函数</strong>）（CDF）是函数<span
class="math inline">\(F_X:R\to[0,1]\)</span>,定义为： <span
class="math display">\[F_X(x)=P(X\le x)\]</span></li>
<li>对于<strong>离散概率分布</strong>，有<strong>概率质量函数</strong><span
class="math inline">\(p(x)\)</span>,假设随机变量<span
class="math inline">\(X\)</span>取值范围是集合<span
class="math inline">\(\chi\)</span> 则有： <span
class="math display">\[\sum_{x\in \chi}^{} p(x)=1\]</span> ,<span
class="math inline">\(X\)</span>的概率质量函数为<span
class="math inline">\(p(\cdot)\)</span>,则函数<span
class="math inline">\(h(X)\)</span>关于变量<span
class="math inline">\(X\)</span>的期望是 <span
class="math display">\[E_{X\sim p(\cdot)}[h(X)]=\sum_{x\in
\chi}^{}p(x)\cdot h(x)\]</span></li>
<li>对于<strong>连续概率发布</strong>，有<strong>概率密度函数</strong><span
class="math inline">\(p(x)\)</span>,随机变量<span
class="math inline">\(X\)</span>的取值范围<span
class="math inline">\(\chi\)</span>是连续集合，则有：<span
class="math display">\[\int_{-\infty }^{x} p(u)du=F_X(x)=P(X\le
x)\]</span><span class="math display">\[\int_{-\infty }^{+\infty}
p(u)du=1\]</span>,<span
class="math inline">\(X\)</span>的概率密度函数为<span
class="math inline">\(p(\cdot)\)</span>,则函数<span
class="math inline">\(h(X)\)</span>关于变量<span
class="math inline">\(X\)</span>的期望是 <span
class="math display">\[E_{X\sim p(\cdot)}[h(X)]=\int_{\chi}p(x)\cdot
h(x)dx\]</span>
<img src="/2024/03/31/第二章、蒙特卡洛方法/image-20240401134538137.png"  alt="image-20240401134538137" style="zoom: 33%;" /></li>
</ol>
<p>总的来说，就是<span
class="math inline">\(\frac{抽中次数}{总抽样数}=精确的理论概率\)</span></p>
<h3 id="例一近似pi值">例一、近似<span
class="math inline">\(\pi\)</span>值</h3>
<p><img src="/2024/03/31/第二章、蒙特卡洛方法/image-20240401134618614.png"  alt="image-20240401134618614" style="zoom:33%;" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">m = <span class="number">0</span></span><br><span class="line">n = <span class="number">100000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    x,y = <span class="number">2</span>*torch.rand(<span class="number">1</span>)-<span class="number">1</span>,<span class="number">2</span>*torch.rand(<span class="number">1</span>)-<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span>  torch.<span class="built_in">pow</span>(x,<span class="number">2</span>)+ torch.<span class="built_in">pow</span>(y,<span class="number">2</span>) &lt;= <span class="number">1</span>:</span><br><span class="line">        m = m+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">pi = <span class="number">4</span> * m / n</span><br><span class="line"><span class="built_in">print</span>(pi)</span><br></pre></td></tr></table></figure>
<p>输出：3.13528 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.2节，蒙特卡洛近似计算圆周率。</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">approxiate_pi</span>(<span class="params">n: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="comment"># 在[-1, 1] x [-1, 1]的空间中随机取n个点。</span></span><br><span class="line">    x_lst = np.random.uniform(-<span class="number">1</span>, <span class="number">1</span>, size=n)</span><br><span class="line">    y_lst = np.random.uniform(-<span class="number">1</span>, <span class="number">1</span>, size=n)</span><br><span class="line">    <span class="comment"># 统计距离圆心距离在1以内的点。</span></span><br><span class="line">    m = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_lst, y_lst):</span><br><span class="line">        <span class="keyword">if</span> x ** <span class="number">2</span> + y ** <span class="number">2</span> &lt;= <span class="number">1</span>:</span><br><span class="line">            m += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 近似计算圆周率。</span></span><br><span class="line">    pi = <span class="number">4</span> * m / n</span><br><span class="line">    <span class="keyword">return</span> pi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    pi = approxiate_pi(<span class="number">100</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;100个点近似的圆周率：&quot;</span>, pi)</span><br><span class="line"></span><br><span class="line">    pi = approxiate_pi(<span class="number">10000</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;10000个点近似的圆周率：&quot;</span>, pi)</span><br><span class="line"></span><br><span class="line">    pi = approxiate_pi(<span class="number">1000000</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;1000000个点近似的圆周率：&quot;</span>, pi)</span><br><span class="line"></span><br></pre></td></tr></table></figure> 输出：100个点近似的圆周率： 3.08
10000个点近似的圆周率： 3.1352 1000000个点近似的圆周率： 3.141</p>
<h3 id="例二计算阴影部分面积">例二、计算阴影部分面积</h3>
<p><img src="/2024/03/31/第二章、蒙特卡洛方法/image-20240401134642296.png"  alt="image-20240401134642296" style="zoom:33%;" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">m = <span class="number">0</span></span><br><span class="line">n = <span class="number">100000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    x,y = <span class="number">2</span>*torch.rand(<span class="number">1</span>),<span class="number">2</span>*torch.rand(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span>  ((x-<span class="number">1</span>)**<span class="number">2</span>+(y-<span class="number">1</span>)**<span class="number">2</span>&lt;=<span class="number">1</span>) &amp; (x**<span class="number">2</span>+y**<span class="number">2</span>&gt;<span class="number">4</span>):</span><br><span class="line">        m = m+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">s = <span class="number">4</span> * m / n</span><br><span class="line"><span class="built_in">print</span>(s)</span><br></pre></td></tr></table></figure>
<p>输出：0.59632</p>
<h3 id="例三计算近似定积分期望">例三、计算近似定积分、期望</h3>
<p><strong>一元函数的定积分</strong>：抽样函数的平均值乘以区间长度，即
<span class="math display">\[
I=\int_{a}^{b}f(x)dx \approx
q_n=(b-a)\cdot\frac{1}{n}\sum\limits_{i=1}^{n}f(x_i)
\]</span>
<strong>多元函数的定积分</strong>：抽样函数的平均值乘以积分集合的体积，即
<span class="math display">\[
I=\int_{\Omega }^{}f(x)dx \approx
q_n=V\cdot\frac{1}{n}\sum\limits_{i=1}^{n}f(x_i)=\int_{\Omega}^{}dx
\cdot\frac{1}{n}\sum\limits_{i=1}^{n}f(x_i)
\]</span> 求<strong>期望</strong>：计算 <span class="math display">\[
E_{X\sim p(\cdot)}[f(X)]=\int_{\Omega}^{}p(x)\cdot f(x)dx
\]</span>
，可按照变量服从的概率分布抽样，求函数平均值即可；当然也可利用定积分，把其中的<span
class="math inline">\(f(x_i)\)</span>换成<span
class="math inline">\(p(x_i)\cdot f(x_i)\)</span>即可；</p>
<p>假设用期望计算<span
class="math inline">\(\int_{0}^{3}x^\frac{2}{3}dx\)</span> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">q=<span class="number">0</span></span><br><span class="line">t=<span class="number">1</span></span><br><span class="line">n=<span class="number">10000</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x ** (<span class="number">2</span>/<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    a = torch.rand(<span class="number">1</span>) * <span class="number">3</span></span><br><span class="line">    q = (<span class="number">1</span>-<span class="number">1</span>/t) * q + (<span class="number">1</span>/t) * f(a)</span><br><span class="line">    t = t+<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;期望&quot;</span>,q)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;积分&quot;</span>,<span class="number">3</span>*q)</span><br></pre></td></tr></table></figure>
输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">期望 tensor([1.2539])</span><br><span class="line">积分 tensor([3.7617])</span><br><span class="line">3.744150881493428</span><br></pre></td></tr></table></figure>
<h3 id="第二章习题2.2">第二章习题2.2</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 方法1</span></span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">n = <span class="number">10000</span></span><br><span class="line">f = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    x = torch.normal(mean=<span class="number">1</span>,std=<span class="number">2</span>,size=(<span class="number">1</span>,))</span><br><span class="line">    f = <span class="number">2</span>*x+<span class="number">10</span>*torch.sqrt(torch.<span class="built_in">abs</span>(x))+<span class="number">3</span>+f</span><br><span class="line"><span class="built_in">print</span>(f/n)</span><br></pre></td></tr></table></figure>
<p>输出：tensor([17.3412])</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">```python</span><br><span class="line"> <span class="comment"># 方法2</span></span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">n = <span class="number">10000</span></span><br><span class="line">q=<span class="number">0</span></span><br><span class="line">t=<span class="number">1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span>  <span class="number">2</span>*x+<span class="number">10</span>*torch.sqrt(torch.<span class="built_in">abs</span>(x))+<span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    x = torch.normal(mean=<span class="number">1</span>,std=<span class="number">2</span>,size=(<span class="number">1</span>,))</span><br><span class="line">    q = (<span class="number">1</span>-<span class="number">1</span>/t)*q+<span class="number">1</span>/t * f(x)</span><br><span class="line">    t = t+<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(q)</span><br></pre></td></tr></table></figure>
<p>输出：tensor([17.3020])</p>
]]></content>
      <categories>
        <category>深度强化学习读书笔记</category>
      </categories>
      <tags>
        <tag>人工智能、强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>第四章、DQN与Q学习</title>
    <url>/2024/04/06/%E7%AC%AC%E5%9B%9B%E7%AB%A0%E3%80%81DQN%E4%B8%8EQ%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<hr />
<p>前面我们知道最优动作价值函数<span
class="math inline">\(Q_*(s_t,a_t)\)</span>可以预知选取<span
class="math inline">\(a_t\)</span>这个动作后回报<span
class="math inline">\(U_t\)</span>的期望的上限，但在实践中我们并不知道<span
class="math inline">\(Q_*\)</span>的函数表达式，因此DQN使用神经网络对其进行近似。</p>
<h4 id="dqndeep-q-network">DQN(Deep Q Network)</h4>
<p>深度Q网络DQN记作<span
class="math inline">\(Q(s,a;w)\)</span>,其结构如图所示，其中<span
class="math inline">\(w\)</span>代表神经网络的参数，学习的目标是对于所有的<span
class="math inline">\(s\)</span>和<span
class="math inline">\(a\)</span>, DQN的预测<span
class="math inline">\(Q(s,a;w)\)</span></p>
<p>尽量接近<span
class="math inline">\(Q_*(s,a)\)</span>;DQN的输入是状态<span
class="math inline">\(s\)</span>,输出是<span
class="math inline">\(|\mathcal{A}
|\)</span>维的向量，每一个元素对应动作空间中每个动作的<span
class="math inline">\(Q\)</span>值；</p>
<p><img src="/2024/04/06/第四章、DQN与Q学习/image-20240407110019564.png"  alt="image-20240407110019564" style="zoom:50%;" /></p>
<h4 id="时间差分tdtemporal-difference算法">时间差分（TD，temporal
difference）算法</h4>
<p>训练DQN常使用TD算法，因此先了解TD算法；</p>
<p>假设原预测为<span
class="math inline">\(\hat{q}\)</span>,部分观测为<span
class="math inline">\(r\)</span>，补充预测为<span
class="math inline">\(\hat{q}’\)</span>，那么<strong>TD目标</strong><span
class="math inline">\(\hat{y}=r+\hat{q}’\)</span>,两者之间的差值<span
class="math inline">\(\delta=(\hat{q}-\hat{y})\)</span>称为<strong>TD误差</strong>，</p>
<p>令损失函数为<span
class="math inline">\(L(w)=\frac{1}{2}\delta^2=\frac{1}{2}(\hat{q}-\hat{y})^2\)</span>,虽然<span
class="math inline">\(\hat{y}=r+\hat{q}’\)</span>的<span
class="math inline">\(\hat{q}’\)</span>包含神经网络参数<span
class="math inline">\(w\)</span>，是<span
class="math inline">\(w\)</span>的函数，但通常情况下对损失函数求参数的梯度时将<span
class="math inline">\(\hat{y}\)</span>看作常数，因此，<span
class="math inline">\(w=w-\alpha\cdot\delta\cdot\nabla
_{w}\hat{q}=w-\alpha\cdot(\hat{q}-\hat{y}{})\cdot\nabla
_{w}\hat{q}\)</span></p>
<h4 id="用td算法之q学习算法训练dqn">用TD算法之Q学习算法训练DQN</h4>
<p><span class="math display">\[
U_t = R_t+\gamma R_{t+1}+...+\gamma^{n-t}R_n
\]</span></p>
<p><span class="math display">\[
U_{t+1} = R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-t-1}R_n
\]</span></p>
<p>故 <span class="math display">\[
U_t=R_t+\gamma U_{t+1}
\]</span> 又因为最优动作价值函数<span
class="math inline">\(Q_*(s_t,a_t)=\underset{\pi }{max}\
E[U_t|S_t=s_t,A_t=a_t]\)</span></p>
<p>经过一系列数学推导： <span class="math display">\[
Q_*(s_t,a_t)=E_{S_{t+1}\sim
p(\cdot|s_t,a_t)}[R_t+\gamma\cdot\underset{A\in\mathcal A}{max}\
Q_*(S_{t+1},A)|S_t=s_t,A_t=a_t ]
\]</span>
通过采样的方法可以对这个期望做蒙特卡洛近似，采样的目的是为了收集四元组<span
class="math inline">\((s_t,a_t,r_t,s_{t+1})\)</span>,这个<strong>四元组</strong>产生的流程是：</p>
<p>在当前状态<span class="math inline">\(s_t\)</span>下执行动作<span
class="math inline">\(a_t\)</span>,这是已知的两个数据，然后环境通过状态转移函数<span
class="math inline">\(p(s_{t+1}|s_t,a_t)\)</span>计算出新的状态<span
class="math inline">\(s_{t+1}\)</span>,(这本质上就是蒙特卡洛近似的过程)，这样，奖励<span
class="math inline">\(r_t=r(s_t,a_t,s_{t+1})\)</span>,至此得到四元组<span
class="math inline">\((s_t,a_t,r_t,s_{t+1})\)</span>；</p>
<p>于是有： <span class="math display">\[
Q_*(s_t,a_t)\approx r_t+\gamma\cdot\underset{a\in\mathcal A}{max}\
Q_*(s_{t+1},a)
\]</span> 我们并不知道<span
class="math inline">\(Q_*\)</span>的值，因此用前面所讲的DQN神经网络对其进行近似：
<span class="math display">\[
Q(s_t,a_t;w)\approx r_t+\gamma\cdot\underset{a\in\mathcal A}{max}\
Q(s_{t+1},a;w)
\]</span> 左边即前面所说的原预测<span
class="math inline">\(\hat{q}\)</span>,右边两项分别为部分观测<span
class="math inline">\(r\)</span>,补充预测<span
class="math inline">\(\hat{q}’\)</span>，两项之和为<strong>TD目标</strong><span
class="math inline">\(\hat{y}=r+\hat{q}’\)</span>，左边减右边即<strong>TD误差</strong>，训练DQN的目的是尽量使得左边趋近右边；</p>
<h4 id="dqn训练的基本流程">DQN训练的基本流程</h4>
<p>首先需要明确的是，训练DQN只需要四元组<span
class="math inline">\((s_t,a_t,r_t,s_{t+1})\)</span>，DQN属于异策略，即控制智能体与环境交互以收集用于训练的四元组数据的<strong>策略</strong>和我们正在优化的<strong>策略</strong>可以不同（DQN中并没有”显式“的要优化的策略，实际上，我们要优化的<span
class="math inline">\(Q(s,a;w)\)</span>可以看成”策略“），前者称为<strong>行为策略</strong>，后者称为<strong>目标策略</strong>；</p>
<ul>
<li><p><strong>第一步：使用行为策略收集训练数据</strong></p>
<p>行为策略可以是任何策略，常用的为<span
class="math inline">\(\epsilon-\)</span>策略： <span
class="math display">\[
\begin{cases}
  argmax_a\ Q(s_t,a;w)&amp; \text{  } 以概率(1-\epsilon)  \\
  均匀抽取\mathcal A中的每一个动作&amp; \text{  } 以概率\epsilon
\end{cases}
\]</span></p></li>
</ul>
<p>把收集到的一条条四元组<span
class="math inline">\((s_t,a_t,r_t,s_{t+1})\)</span>存入缓存，称为”经验回放缓存“；</p>
<ul>
<li><p><strong>第二步：更新参数w</strong></p>
<p>从经验回放缓存中随机抽取一个四元组，记作<span
class="math inline">\((s_j,a_j,r_j,s_{j+1})\)</span>,原参数为<span
class="math inline">\(w_{now}\)</span>,更新后为<span
class="math inline">\(w_{new}\)</span></p>
<p>（1）对DQN神经网络做正向传播，得到Q值，即原预测<span
class="math inline">\(\hat{q_j}=Q(s_j,a_j;w_{now})\)</span>和补充预测<span
class="math inline">\(\hat{q}_{j+1}=\underset{a\in\mathcal
A}{max}Q(s_{j+1},a;w_{now})\)</span>；</p></li>
</ul>
<p>​ (2) 计算TD目标和TD误差： <span class="math display">\[
\hat{y_j}=r_j+\gamma\cdot\hat{q}_{j+1}
\]</span></p>
<p><span class="math display">\[
\delta_j=\hat{q_j}-\hat{y_j}
\]</span></p>
<p>​ （3）对DQN做反向传播，计算梯度 <span class="math display">\[
g_j=\nabla_wQ(s_j,a_j;w_{now})
\]</span> ​ 梯度的形状和<span class="math inline">\(w\)</span>相同；</p>
<p>（4）通过梯度下降更新DQN的参数w <span class="math display">\[
w_{new}=w_{now}-\alpha\cdot\delta_j\cdot g_j
\]</span></p>
<h4 id="传统表格形式的q学习">传统表格形式的Q学习</h4>
<p>DQN其实是神经网络形式的Q学习，现在我们介绍传统的表格形式的Q学习，它只适用于状态集合<span
class="math inline">\(\mathcal S\)</span>和动作集合<span
class="math inline">\(\mathcal A\)</span>都有限的情况。</p>
<p><img src="/2024/04/06/第四章、DQN与Q学习/image-20240407152338113.png"  alt="image-20240407152338113" style="zoom:50%;" /></p>
<p>传统Q学习理解就是，不断更新如上图所示的表格，每次更新表格中一个元素值，使得其中的元素值<span
class="math inline">\(\widetilde{Q}\)</span>不断趋近于<span
class="math inline">\(Q_*\)</span></p>
<ul>
<li><p><strong>第一步：使用行为策略收集训练数据</strong></p>
<p>行为策略可以是任何策略，常用的为<span
class="math inline">\(\epsilon-\)</span>策略： <span
class="math display">\[
\begin{cases}
  argmax_a\ \widetilde{Q}(s_t,a)&amp; \text{  } 以概率(1-\epsilon)  \\
  均匀抽取\mathcal A中的每一个动作&amp; \text{  } 以概率\epsilon
\end{cases}
\]</span></p></li>
</ul>
<p>把收集到的一条条四元组<span
class="math inline">\((s_t,a_t,r_t,s_{t+1})\)</span>存入缓存，称为”经验回放缓存“；</p>
<ul>
<li><p><strong>第二步：更新参数w</strong></p>
<p>从经验回放缓存中随机抽取一个四元组，记作<span
class="math inline">\((s_j,a_j,r_j,s_{j+1})\)</span>,设当前表格为<span
class="math inline">\(\widetilde{Q}_{now}\)</span>,这样的一条四元组可以更新表格中的一个”格子“</p>
<p>即：<span
class="math inline">\(\widetilde{Q}_{now}(s_j,a_j)\)</span>更新为<span
class="math inline">\(\widetilde{Q}_{new}(s_j,a_j)\)</span>,更新过程如下：</p>
<p>（1）旧表格<span
class="math inline">\(\widetilde{Q}_{now}\)</span>中<span
class="math inline">\((s_j,a_j)\)</span>位置上数据为<span
class="math inline">\(\hat{q_j}=\widetilde{Q}_{now}(s_j,a_j)\)</span></p>
<p>第<span class="math inline">\(s_{j+1}\)</span>行中最大值为<span
class="math inline">\(\hat{q}_{j+1}=\underset{a\in\mathcal A}{max}\
\widetilde{Q}_{now}(s_{j+1},a)\)</span>；</p></li>
</ul>
<p>​ (2) 计算TD目标和TD误差： <span class="math display">\[
\hat{y_j}=r_j+\gamma\cdot\hat{q}_{j+1}
\]</span></p>
<p><span class="math display">\[
\delta_j=\hat{q_j}-\hat{y_j}
\]</span></p>
<p>​ （3）更新表格中<span
class="math inline">\((s_j,a_j)\)</span>位置上的元素： <span
class="math display">\[
\widetilde{Q}_{new}(s_j,a_j)=(1-\alpha)\cdot\widetilde{Q}_{now}(s_j,a_j)+\alpha\cdot\hat{y_j}=\widetilde{Q}_{now}(s_j,a_j)-\alpha\cdot\delta_{j}
\]</span>
可以看到传统的表格形式的Q学习在算出各个值之后就直接以简单的比例形式去更新Q值，这种简单的更新方法的效果注定比较一般。</p>
<h4
id="同策略on-policy与异策略off-policy">同策略(on-policy)与异策略(off-policy)</h4>
<ul>
<li><p>行为策略(behavior
policy):控制智能体与环境交互，收集经验数据；</p></li>
<li><p>目标策略(target
policy):我们正在优化的策略；比如在DQN中，目标策略为： <span
class="math display">\[
a_t=\underset{a}{argmax}\ Q(s_t,a;w)
\]</span></p></li>
</ul>
<h4 id="经验回放">经验回放</h4>
<p>将智能体与环境交互的记录暂时保存，然后从中采样和学习的训练方式称为<strong>经验回放</strong>，需要注意的是，经验回放只适用于异策略，如DQN；不适用于同策略，为什么呢，这就像是尝试在当前的工作项目上使用你几年前旧工作的解决方案。</p>
<p><a href="https://Acoder123wew.github.io/">DQN代码实现</a></p>
]]></content>
      <categories>
        <category>深度强化学习读书笔记</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
</search>
