<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>60分钟入门pytorch</title>
    <url>/2024/04/08/60%E5%88%86%E9%92%9F%E5%85%A5%E9%97%A8pytorch/</url>
    <content><![CDATA[<h4 id="tensor">Tensor</h4>
<p>Tensor,张量，是PyTorch中的核心类，我们平常所说的张量通常是张量类的实例，张量即n维数组，支持GPU加速计算和自动微分。</p>
<ul>
<li><p>初始化tensor的方法</p>
<ul>
<li><ol type="1">
<li>直接从数据创建</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>],[<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br><span class="line"><span class="built_in">type</span>(data),<span class="built_in">type</span>(x_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(<span class="built_in">list</span>, torch.Tensor)</span><br></pre></td></tr></table></figure>
<p>其中，torch.tensor()函数可以将数据转换为torch.Tensor()类型；</p></li>
<li><ol start="2" type="1">
<li>从numpy的array转换过来</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = [[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]</span><br><span class="line">np_array = np.array(data)</span><br><span class="line">x = torch.from_numpy(np_array)</span><br><span class="line"><span class="built_in">type</span>(data),<span class="built_in">type</span>(np_array),<span class="built_in">type</span>(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(<span class="built_in">list</span>, numpy.ndarray, torch.Tensor)</span><br></pre></td></tr></table></figure></li>
<li><ol start="3" type="1">
<li>根据其他tensor样式创建</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_zeros = torch.zeros_like(x_data)</span><br><span class="line">x_ones = torch.ones_like(x_data)</span><br><span class="line">x_rand = torch.rand_like(x_data)</span><br><span class="line">x_zeros,x_ones,x_rand</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(tensor([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>]]),</span><br><span class="line"> tensor([[<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>]]),</span><br><span class="line"> tensor([[<span class="number">0.5286</span>, <span class="number">0.8992</span>],</span><br><span class="line">         [<span class="number">0.7840</span>, <span class="number">0.2935</span>]]))</span><br></pre></td></tr></table></figure>
<p>这三个函数的参数必须是torch.Tensor</p></li>
<li><ol start="4" type="1">
<li>从shape元组创建</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">shape = (<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">x_zeros_ = torch.zeros(shape)</span><br><span class="line">x_ones_ = torch.ones(shape)</span><br><span class="line">x_rand_ = torch.rand(shape)</span><br><span class="line">x_zeros_,x_ones_,x_rand_,x_zeros_.shape,x_ones_.shape,x_rand_.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(tensor([[[<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">          [<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">          [<span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"> </span><br><span class="line">         [[<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">          [<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">          [<span class="number">0.</span>, <span class="number">0.</span>]]]),</span><br><span class="line"> tensor([[[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">1.</span>]],</span><br><span class="line"> </span><br><span class="line">         [[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">1.</span>]]]),</span><br><span class="line"> tensor([[[<span class="number">0.7854</span>, <span class="number">0.6111</span>],</span><br><span class="line">          [<span class="number">0.8331</span>, <span class="number">0.9508</span>],</span><br><span class="line">          [<span class="number">0.2372</span>, <span class="number">0.5213</span>]],</span><br><span class="line"> </span><br><span class="line">         [[<span class="number">0.6088</span>, <span class="number">0.8252</span>],</span><br><span class="line">          [<span class="number">0.0571</span>, <span class="number">0.2459</span>],</span><br><span class="line">          [<span class="number">0.9353</span>, <span class="number">0.4851</span>]]]),</span><br><span class="line"> torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>]),</span><br><span class="line"> torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>]),</span><br><span class="line"> torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>]))</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><p>Tensor的属性：shape形状，dtype元素数据类型，device存储设备</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(x_zeros.shape,x_zeros.dtype,x_zeros.device)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.Size([<span class="number">2</span>, <span class="number">2</span>]) torch.int64 cpu</span><br></pre></td></tr></table></figure></li>
<li><p>转换数据的device</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor = torch.ones((<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(tensor.device)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">  tensor = tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor.device)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cpu</span><br><span class="line">cuda:<span class="number">0</span></span><br></pre></td></tr></table></figure></li>
<li><p>pytorch切片与索引操作和python相同</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor = torch.ones(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">tensor[:,<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure></li>
<li><p>torch.cat([a,b,c],dim=n),按照第n维度连接张量a,b,c,维度为0表示按行连接，维度为1表示按列连接</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure></li>
<li><p>torch.mul(A,B)和torch.mul(A,number)分别表示相同形状的矩阵按元素相乘和矩阵乘以常数,*运算符可实现相同的功能；</p></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.mul(tensor,tensor),torch.mul(tensor,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>),</span><br><span class="line"> tensor([[<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">         [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li><p>相应地，矩阵乘法为torch.matmul(A,B),A@B</p></li>
<li><p>带有<code>_</code>后缀的操作为原地操作</p></li>
<li><p>.numpy()可以将torch.Tensor类型转换为numpy.ndarray类型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor = tensor.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(tensor))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(tensor.numpy()))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;torch.Tensor&#x27;</span>&gt;</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;numpy.ndarray&#x27;</span>&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>使用t =
torch.from_numpy(n)将n这个numpy.ndarray转换为torch.Tensor时，n的变化会影响到t</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n = np.ones(<span class="number">5</span>)</span><br><span class="line">t = torch.from_numpy(n)</span><br><span class="line">np.add(n, <span class="number">1</span>, out=n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t: tensor([<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>], dtype=torch.float64)</span><br><span class="line">n: [<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习、编程语言</tag>
      </tags>
  </entry>
  <entry>
    <title>C++初级知识</title>
    <url>/2024/04/06/C-%E5%88%9D%E7%BA%A7%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<ul>
<li>C++语言对大小写敏感，所有C++语句须以<code>;</code>结尾，它作为终止符是C++语句结束的标志；</li>
<li>函数名前面的部分叫做函数返回类型，后面的部分叫做形参列表，<code>int main()</code>描述的是<code>main()</code>与操作系统之间的接口，也就是说，主函数main()的返回值返回给操作系统，<code>void</code>表示不返回信息或不接受参数；</li>
<li>可以将函数、类等对象封装在名称空间中，调用时指定即可，如<code>std::cout</code>，<code>ros::NodeHandle n;</code>等，在前面提前声明名称空间<code>using namespace std;using namespace ros;</code>,在后续使用时可以省略名称空间，直接使用<code>cout</code>;</li>
<li><code>cout&lt;&lt;"ABC"</code>的本质：<code>cout</code>是<code>iostream</code>文件中预定义的对象，这条语句的本质是cout对象将字符串”ABC“插入到输出流中；</li>
<li>运算符重载：对于同一个运算符，编译器可以通过上下文来确定运算符的含义从而实现不同的功能；</li>
<li>C++中，<code>endl</code>和<code>\n</code>均可表示换行，其中<code>\n</code>须在字符串中使用；</li>
<li>变量声明：1.分配指定数据类型的内存
2.指定存储在这个内存中的数据的名称；</li>
<li>声明包括定义声明、引用声明；</li>
<li><strong>类</strong>：类是C++中面向对象编程（Object Oriented
Programming，OOP）的核心概念，类描述它能够表示什么信息和可对数据执行哪些操作，对象则是根据类中的数据格式规范创建的实体；</li>
<li><strong>函数初步</strong>：调用函数传递参数给被调用函数，被调用函数返回值以替代调用函数中相应的部分；</li>
<li><strong>函数原型</strong>：在调用某个函数之前，应提供其原型，要么在源代码文件中main()函数前输入函数原型，要么<code>#include</code>该函数所在的头文件；</li>
<li>C++不允许将函数定义嵌套在另一个函数定义中，每个函数的定义都是独立的；</li>
<li>在任何一门编程语言中，对变量、函数、对象等，都需要尽可能使用一目了然的命名，推荐使用”驼峰命名法“；</li>
</ul>
]]></content>
      <categories>
        <category>C++学习</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
      </tags>
  </entry>
  <entry>
    <title>C++处理数据</title>
    <url>/2024/04/08/C-%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<p>ccc</p>
<ul>
<li><p><strong>C++变量命名规则</strong>：只能以字母、数字、下划线<code>_</code>命名，开头字符不能是数字、不能使用C++关键字，尽量不要使用以下划线<code>_</code>开头的变量命名，它们一般被保留给实现，使用它们通常不会导致编译器错误，但经常会导致行为的不确定性；</p></li>
<li><p><strong>C++整型</strong>：按照宽度递增的顺序，整型包括：short、int、long、long
long，其中每种类型都有signed有符号版本、unsigned无符号版本，因此总共有8种整型可供选择（char通常表示字符，这里不纳入）；</p></li>
<li><p>经过<code>sizeof()</code>,<code>_MIN,_MAX</code>显示，我的计算机显示如下</p>
<ul>
<li>short:2B,[-32768,32767];</li>
<li>int:4B,[-2147483648,2147483647];</li>
<li>long:4B,[-2147483648,2147483647];</li>
<li>long long:8B,[-9223372036854775808,-9223372036854775807]</li>
</ul></li>
<li><p><strong>变量初始化</strong>：<code>int a&#123;7&#125;  int a=&#123;7&#125; int a =7</code>都是将int类型的变量a设置为7.<code>int a=&#123;&#125;,int a&#123;&#125;</code>都是将int类型的变量a设置为0.</p></li>
<li><p><code>unsigned</code>是<code>unsigned int</code>的缩写，两者完全一样；</p></li>
<li><p><strong>整型字面值</strong>：（1）第一位为1~9，则基数为10（十进制，dec）；</p>
<ul>
<li><p>​ （2）第一位为0，第二位为1~7,则基数为8（八进制,oct）；</p>
<p>​
（3）前2位为0x或0X，则基数为16（十六进制,hex），A~F分别表示10到15；</p></li>
</ul></li>
<li><p><code>cout &lt;&lt; dec(oct/hex);</code>不显示任何消息，而是修改cout显示整数的方式是10(8/16)进制；</p></li>
<li><p>数字后缀<code>L</code>表示long类型，<code>U</code>表示unsigned
int类型，<code>UL</code>表示unsigned long类型，<code>LL</code>表示long
long类型，<code>ULL</code>表示unsigned long
long类型；若无后缀且能存的下，则默认使用int类型存储；</p></li>
<li></li>
</ul>
]]></content>
      <categories>
        <category>C++学习</category>
      </categories>
      <tags>
        <tag>编程语言、C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2024/03/31/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a
href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a
href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="hello-hexo">Hello Hexo</h2>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
  </entry>
  <entry>
    <title>《RL:Introduction》_chap1,2</title>
    <url>/2024/04/18/%E3%80%8ARL-Introduction%E3%80%8B-chap1-2/</url>
    <content><![CDATA[<h3 id="chap1-导论">chap1 导论</h3>
<ul>
<li><p><strong>试错</strong>和<strong>延迟收益</strong>是强化学习中最重要最显著的特征；</p></li>
<li><p>马尔可夫决策过程包含：感知、动作、目标；</p></li>
<li><p>强化学习区别于<strong>有监督学习</strong>：从带标注训练数据集中学习；<strong>无监督学习</strong>：寻找未标注数据集中的隐含结构；有监督学习、无监督学习、强化学习是并列的三种机器学习范式。</p></li>
<li><p><strong>弱方法</strong>：基于一般原则的方法，比如搜索、学习；<strong>强方法</strong>：基于知识的方法；</p></li>
<li><p><strong>贪心</strong>：选择价值最高的动作；<strong>试探</strong>：偶尔随机选择其他动作；</p></li>
<li><p>状态价值的更新：将早先的状态的价值向后面的状态的价值方向移动一个增量，即：
<span class="math display">\[
V(S_t)\gets V(S_t)+\alpha\cdot[V(S_{t+1}-V(S_t))]
\]</span> <span
class="math inline">\(0&lt;\alpha&lt;1\)</span>为步长参数；</p></li>
<li><p>表格型求解方法：状态空间<span
class="math inline">\(\mathcal{S}\)</span>和动作空间<span
class="math inline">\(\mathcal{A}\)</span>的维度都较低，可用表格形式表示价值函数，使用简单的比例变换更新参数；与之相对的是深度强化学习中的各种近似手段，并用神经网络的梯度下降更新参数。</p></li>
</ul>
<hr />
<h3 id="chap2-多臂赌博机multi-arm-bandits">chap2 多臂赌博机(Multi-arm
Bandits)</h3>
<h4 id="一个k臂赌博机问题">2.1 一个k臂赌博机问题</h4>
<p>重复地在<span
class="math inline">\(k\)</span>个动作中进行选择，每次做出选择之后，都将得到一个标量的奖励值，目标是在某一段时间内最大化总回报的期望。将动作<span
class="math inline">\(a\)</span>被选择之后得到的奖励的平均值记作<span
class="math inline">\(q_*(a)=E[R_t|A_t=a]\)</span>,称为<strong>动作<span
class="math inline">\(a\)</span>的价值</strong>，将其估计值记作<span
class="math inline">\(Q_t(a)\)</span>，我们希望<span
class="math inline">\(Q_t(a)\)</span>接近<span
class="math inline">\(q_*(a)\)</span>.</p>
<p>将<span class="math inline">\(t\)</span>时刻最大<span
class="math inline">\(Q\)</span>值对应的动作称为<strong>贪心</strong>动作，选择贪心动作的过程我们称之为<strong>开发</strong>，选择<strong>非贪心</strong>动作的过程我们称之为<strong>试探</strong>，一个好的动作应该在贪心和试探中权衡。</p>
<h4 id="动作-价值方法">2.2 动作-价值方法</h4>
<ul>
<li><p><strong>采样平均方法</strong>：通过计算实际奖励（收益）的平均值来估计动作的价值
<span class="math display">\[
Q_t(a)=\frac{t时刻前通过执行动作a得到的收益总和}{t时刻前执行动作a的次数}=\frac{\sum_{i=1}^{t-1}R_i\cdot
1_{A_i=a}}{\sum_{i=1}^{t-1}1_{A_i=a}}
\]</span></p></li>
<li><p>贪心：<span class="math inline">\(A_t=\underset{a}{argmax}\
Q_t(a)\)</span></p></li>
<li><p><span class="math inline">\(\epsilon\)</span>-贪心： <span
class="math display">\[
\begin{cases}
  argmax_a\ Q_t(a)&amp; \text{  } 以概率(1-\epsilon)  \\
  均匀抽取\mathcal A中的每一个动作&amp; \text{  } 以概率\epsilon
\end{cases}
\]</span></p></li>
</ul>
<hr />
<h4 id="臂测试平台">2.3 10臂测试平台</h4>
<p>动作<span class="math inline">\(a\)</span>的真实值<span
class="math inline">\(q_*(a)\)</span>从<span
class="math inline">\(N(0,1)\)</span>分布生成，动作<span
class="math inline">\(a\)</span>的实际收益<span
class="math inline">\(Q_t(a)\)</span>(每个t在选择完动作后都要生成一次)从<span
class="math inline">\(N(q_*(a),1)\)</span>生成，通常情况下，在一个赌博机问题中，需要进行1000时刻的交互，对一个学习算法，需要在2000个不同的赌博问题中试验。</p>
<hr />
<h4 id="增量式实现">2.4 增量式实现</h4>
<p>设<span class="math inline">\(R_i\)</span>表示第<span
class="math inline">\(i\)</span>次选择某动作后获得的收益，<span
class="math inline">\(Q_{n}\)</span>表示该动作被选择<span
class="math inline">\(n-1\)</span>次后该动作价值的估计值，则： <span
class="math display">\[
Q_n=\frac{R_1+R_2+...+R_{n-1}}{n-1}
\]</span> 这种实现方式需要一直存储并维护<span
class="math inline">\(\{R_i\}\)</span>序列，并且每增加一次收益都需要重新求和。</p>
<p>下面推导递推式以简化存储和计算流程： <span class="math display">\[
Q_{n+1}=\frac{1}{n}\sum\limits_{i=1}^{n}R_i
\]</span></p>
<p><span class="math display">\[
=\frac{1}{n}(R_n+\sum\limits_{i=1}^{n-1}R_i)
\]</span></p>
<p><span class="math display">\[
=\frac{1}{n}(R_n+(n-1)\frac{1}{n-1}\sum\limits_{i=1}^{n-1}R_i)
\]</span></p>
<p><span class="math display">\[
=\frac{1}{n}(R_n+(n-1)Q_n)
\]</span></p>
<p><span class="math display">\[
=\frac{1}{n}(R_n+nQ_n-Q_n)
\]</span></p>
<p><span class="math display">\[
=Q_n+\frac{1}{n}[R_n-Q_n]
\]</span></p>
<p>故实际计算只需要存储当前的<span
class="math inline">\(Q_n\)</span>和<span
class="math inline">\(n\)</span>值，再根据新来的<span
class="math inline">\(R_n\)</span>计算更新即可。</p>
<p><strong>更新公式</strong>的一般形式为： <span class="math display">\[
新估计值\gets 旧估计值+步长参数\cdot(目标-旧估计值)
\]</span>
这是表格型强化学习方法的通用公式，区别于深度强化学习中的梯度更新方法。</p>
<hr />
<h4 id="跟踪一个非平稳问题">2.5 跟踪一个非平稳问题</h4>
<p>非平稳：收益的概率分布随着时间<span
class="math inline">\(t\)</span>变化，比如给未来的收益打折扣、或者本书所说的：给近期的收益赋予比过去很久的收益更高的权值。可以使用常数步长来达到这个目标，当使用常数步长时：</p>
<p><img src="/2024/04/18/《RL-Introduction》-chap1-2/c07fdd4a93bb35af2c8f53b2ae0b041.jpg"  alt="c07fdd4a93bb35af2c8f53b2ae0b041" style="zoom: 50%;" /></p>
<p>上式中<span class="math inline">\(0&lt;1-\alpha&lt;1\)</span>,<span
class="math inline">\(n-i\)</span>越大，表明该收益越久远，便赋予更低的权值。</p>
<p>若随着时刻逐步改变步长，即<span
class="math inline">\(Q_{n+1}=Q_n+\alpha_n(a)[R_n-Q_n]\)</span>时，<span
class="math inline">\(Q_{n+1}\)</span>收敛需要满足以下两个条件： <span
class="math display">\[
\sum\limits_{n=1}^{\infty}\alpha_n(a)=\infty \ and \
\sum\limits_{n=1}^{\infty}\alpha_n^2(a)&lt;\infty
\]</span> 同样，可以推导更新公式：</p>
<p><img src="/2024/04/18/《RL-Introduction》-chap1-2/image-20240417215254394.png"  alt="image-20240417215254394" style="zoom: 50%;" /></p>
<hr />
<h4 id="乐观初始值">2.6 乐观初始值</h4>
<p>由以上推导过程可知，<span
class="math inline">\(Q_{n+1}\)</span>均依赖于<span
class="math inline">\(Q_1\)</span>,根据规定<span
class="math inline">\(Q_2=R_1\)</span>,而<span
class="math inline">\(Q_1\)</span>则是我们预设的超参数，它会给我们的估计带来偏差，但这种偏差有时会带来好处；</p>
<h4 id="置于置信度上界的动作选择">2.7 置于置信度上界的动作选择</h4>
<p>虽然“试探”那些非贪心的动作是必须的，但经典的<span
class="math inline">\(\epsilon\)</span>-贪心算法的问题在于它的<span
class="math inline">\(\epsilon\)</span>行为是均等的，即平等地选择动作空间中的每一个动作，但其实我们应该优先选择那些价值接近贪心动作且不确定性更大的动作。为此，可使用<strong>置信度上界(upper
confidence bound)</strong>进行动作选择： <span class="math display">\[
A_t=\underset{a}{argmax}\ [Q_t(a)+c\sqrt{\frac{ln\ t}{N_t(a)}}]
\]</span> 其中，<span class="math inline">\(ln\ t\)</span>表示时刻<span
class="math inline">\(t\)</span>的自然对数，<span
class="math inline">\(N_t(a)\)</span>表示在时刻<span
class="math inline">\(t\)</span>之前动作<span
class="math inline">\(a\)</span>被选择的次数，<span
class="math inline">\(c\)</span>是一个大于0的常数，用于控制试探的程度。</p>
<p>若<span class="math inline">\(N_t(a)=0\)</span>，则动作<span
class="math inline">\(a\)</span>的价值最大，这保证了智能体会首先遍历动作空间中的所有动作。</p>
<hr />
<h4 id="梯度赌博机算法">2.8 梯度赌博机算法</h4>
<p>偏好函数：一个不基于动作的收益的数值化值，记作<span
class="math inline">\(H_t(a)\)</span>,偏好函数越大，动作就越频繁地被选择。</p>
<p>每个动作都维护这样一个偏好函数，并通过<span
class="math inline">\(softmax\)</span>计算每个动作被选择的概率(<span
class="math inline">\(k\)</span>为动作数量）： <span
class="math display">\[
Pr\{A_t=a\}=\frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}}=\pi_t(a)
\]</span> 基于随机梯度上升的思想，偏好函数通过下列公式更新：</p>
<p>若在时刻<span class="math inline">\(t\)</span>,动作<span
class="math inline">\(A_t\)</span>被选择，则动作<span
class="math inline">\(A_t\)</span>的偏好函数的更新方式为：<span
class="math inline">\(H_{t+1}(A_t)=H_t(A_t)+\alpha\cdot
(R_t-\bar{R_t})(1-\pi_t(A_t))\)</span></p>
<p>而其他动作的偏好函数的更新方式为：<span
class="math inline">\(H_{t+1}(a)=H_t(a)-\alpha(R_t-\bar{R_t})\pi_t(a)\)</span>；</p>
<h4 id="关联搜索上下文相关的赌博机">2.9
关联搜索（上下文相关的赌博机）</h4>
<p>即区分出不同的情境，以采用与对应情景相适应的旋转。</p>
<h4 id="总结">总结</h4>
<ul>
<li><p><span
class="math inline">\(\epsilon\)</span>-贪心方法：在一小段时间内进行随机的动作选择；参数为：<span
class="math inline">\(\epsilon\)</span></p></li>
<li><p>UCB方法：通过巧妙的公式，优先选择那些之前被选择的次数较少的动作；参数为：<span
class="math inline">\(c\)</span></p></li>
<li><p>梯度赌博机算法：偏好函数值经过softmax变换得到每个动作被执行的概率；参数为：<span
class="math inline">\(\alpha\)</span></p></li>
<li><p>乐观设置初值：<span class="math inline">\(Q_1\)</span></p>
<p><img src="/2024/04/18/《RL-Introduction》-chap1-2/image-20240418203018992.png"  alt="image-20240418203018992" style="zoom:50%;" /></p></li>
</ul>
<p>如图所示，图中每一个点表示特定算法(不同颜色标注)在特定参数下(横坐标值)执行1000步的平均收益。</p>
]]></content>
      <categories>
        <category>RLintroduction</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>《RL-Introduction》-chap4</title>
    <url>/2024/04/22/%E3%80%8ARL-Introduction%E3%80%8B-chap4/</url>
    <content><![CDATA[<hr />
<h3 id="chap4-动态规划">chap4 动态规划</h3>
<ul>
<li>动态规划（Dynamic
Programming，DP）是一类在用马尔可夫决策过程（MDP)描述的完备环境中计算最优策略的优化方法；</li>
<li>动态规划假定环境为有限MDP，即状态集合<span
class="math inline">\(\mathcal{S}\)</span>,动作集合<span
class="math inline">\(\mathcal{A}\)</span>,收益集合<span
class="math inline">\(\mathcal{R}\)</span>均有限，且整个系统的的动态特性完全由四参数概率分布函数</li>
</ul>
<p><span class="math inline">\(p(s&#39;,r;s,a)\)</span>给出；</p>
<ul>
<li><p>核心思想：使用价值函数来结构化地组织对最优策略的搜索； <span
class="math display">\[
v_*(s)=\underset{a}{max}E[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s,A_t=a]
\]</span></p>
<p><span class="math display">\[
=\underset{a}{max}\sum\limits_{s&#39;,r}^{}p(s&#39;,r|s,a)[r+\gamma
v_*(s&#39;)]
\]</span></p>
<p><span class="math display">\[
q_*(s,a)=E[R_{t+1}+\gamma\ \underset{a&#39;}{max}\
q_*(S_{t+1},a&#39;)|S_t=s,A_t=a]
\]</span></p>
<p><span class="math display">\[
=\sum\limits_{s&#39;,r}^{}p(s&#39;,r|s,a)[r+\gamma\
\underset{a&#39;}{max}\ q_*(s&#39;,a&#39;)]
\]</span></p>
<p><span class="math display">\[
也可以进一步=\sum\limits_{s&#39;,r}^{}p(s&#39;,r|s,a)[r+\gamma\
v_*(s&#39;)]
\]</span></p></li>
</ul>
<h4 id="策略评估预测">4.1 策略评估（预测）</h4>
<p>对于任意<span class="math inline">\(s\in\mathcal{S}\)</span>,有：</p>
<p><img src="/2024/04/22/《RL-Introduction》-chap4/image-20240422194722256.png"  alt="image-20240422194722256" style="zoom:50%;" /></p>
<p>只要<span
class="math inline">\(0&lt;\gamma&lt;1\)</span>或者在任何状态，智能体按<span
class="math inline">\(\pi\)</span>执行动作之后都能到达终止状态，则能保证<span
class="math inline">\(v_\pi(s),\forall
s\in\mathcal{S}\)</span>唯一存在；</p>
<p>对<span
class="math inline">\(\mathcal{S}\)</span>中的每一个状态，都列出上式，可得到<span
class="math inline">\(|\mathcal{S}|\)</span>个方程组成的<span
class="math inline">\(|\mathcal{S}|\)</span>元方程组，因此可以通过解线性方程组得到状态价值。也可以先给<span
class="math inline">\(|\mathcal{S}|\)</span>个状态赋予初值（终止状态的状态价值必须赋0），然后根据式<span
class="math inline">\(v_\pi(s)=\sum\limits_{a}\pi(a|s)\sum\limits_{s&#39;,r}p(s&#39;,r|s,a)[r+\gamma
v_\pi(s&#39;)]\)</span>一直迭代，直到满足精度要求。这种更新方法称为“期望更新”，这种算法被称作<strong>迭代策略评估</strong>。</p>
<p>注意具体实现时，有2种实现方式，（1）定义两个<span
class="math inline">\(|\mathcal{S}|\)</span>长数组存储新旧状态价值new_value,value，等式左边为new_value,右边为value，做完一轮更新后再value=new_value;
(2)只需定义一个value数组，左边为新值，右边新旧皆有可能，这被称为“就地更新”，它依然能够收敛，甚至速度更快。</p>
<h4 id="策略改进">4.2 策略改进</h4>
<p>状态价值函数<span
class="math inline">\(v_\pi(s)\)</span>表示从状态<span
class="math inline">\(s\)</span>开始，智能体按照策略<span
class="math inline">\(\pi\)</span>进行决策所获得的回报的期望值；我们的目标正是改进策略，那么为什么要计算给定策略下的价值呢，其实这是为了寻找更好的策略：动作价值函数<span
class="math inline">\(q_\pi(s,a)\)</span>表示，在状态<span
class="math inline">\(s\)</span>下，执行动作<span
class="math inline">\(a\)</span>之后（这个动作<span
class="math inline">\(a\)</span>与<span
class="math inline">\(\pi\)</span>无关），再使用策略<span
class="math inline">\(\pi\)</span>控制智能体，得到的回报的期望；两者的关系为：<span
class="math inline">\(q_\pi(s,a)=E[R_{t+1}+\gamma
v_{\pi}(S_{t+1})|S_t=s,A_t=a]=\sum\limits_{s&#39;,r}p(s&#39;,r|s,a)[r+\gamma
v_\pi(s&#39;)]\)</span></p>
<p><strong>策略改进定理</strong>：</p>
<p>若<span class="math inline">\(\forall
s\in\mathcal{S}\)</span>,有<span
class="math inline">\(q_\pi(s,\pi^\prime(s))\ge
v_\pi(s)\)</span>,则<span class="math inline">\(\forall
s\in\mathcal{S}\)</span>,有<span
class="math inline">\(v_{\pi^\prime}(s)\ge v_\pi(s)\)</span>,即策略<span
class="math inline">\(\pi^\prime\)</span>要好于策略<span
class="math inline">\(\pi\)</span>;</p>
<p>因此我们可以基于<span
class="math inline">\(v_\pi\)</span>的值构造这样一个贪心策略： <span
class="math display">\[
\pi&#39;(s)=\underset{a}{argmax}\ q_\pi(s,a)
\]</span></p>
<p><span class="math display">\[
=\underset{a}{argmax}\ E[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=a]
\]</span></p>
<p><span class="math display">\[
=\underset{a}{argmax}\sum\limits_{s&#39;,r}p(s&#39;,r|s,a)[r+\gamma
v_\pi(s&#39;)]
\]</span></p>
<h4 id="策略迭代">4.3 策略迭代</h4>
<p>对于一个策略，我们可以通过策略评估迭代计算得到它的状态价值函数，然后据此通过策略改进得到一个更好的新的策略，通过这样的链式过程，我们可以得到一个不断改进的策略和价值函数序列：</p>
<p><img src="/2024/04/22/《RL-Introduction》-chap4/image-20240423153753485.png"  alt="image-20240423153753485" style="zoom:50%;" /></p>
<p>其中<span
class="math inline">\(E(evaluation)\)</span>代表策略评估，<span
class="math inline">\(I(improve)\)</span>代表策略改进；(<span
class="math inline">\(old-action\ne\pi(s)\)</span>表示动作发生变动，依然未收敛)</p>
<p><img src="/2024/04/22/《RL-Introduction》-chap4/image-20240423161144756.png"  alt="image-20240423161144756" style="zoom: 50%;" /></p>
<h4 id="价值迭代">4.4 价值迭代</h4>
<p><img src="/2024/04/22/《RL-Introduction》-chap4/image-20240423165227621.png"  alt="image-20240423165227621" style="zoom: 50%;" /></p>
<p>可见，右边比左边收敛快得多。（左侧为随机策略（0.25x4)的<span
class="math inline">\(v_\pi\)</span>值期望更新，右边为根据<span
class="math inline">\(v_\pi\)</span>值的策略改进，箭头所指方向均为最佳动作）；</p>
<p>价值迭代其实就是贝尔曼最优方程的更新形式： <span
class="math display">\[
v_{k+1}(s)=\underset{a}{max}\ E[R_{t+1}+\gamma v_k(S_{t+1})|S_t=s,A_t=a]
\]</span></p>
<p><span class="math display">\[
=\underset{a}{max}\sum\limits_{s&#39;,r}^{}p(s&#39;,r|s,a)[r+\gamma
v_k(s&#39;)]
\]</span></p>
<p>需要注意的是，价值迭代中不存在显式的策略，我们只维护一个状态价值函数;最后，从中恢复最优策略即可。</p>
<p><img src="/2024/04/22/《RL-Introduction》-chap4/image-20240425214400132.png"  alt="image-20240425214400132" style="zoom:50%;" /></p>
<h4 id="异步动态规划">4.5 异步动态规划</h4>
<ul>
<li>异步DP算法属于就地迭代算法，其不以系统遍历状态集的形式来组织算法；所谓的“不系统”，指的是更新状态的顺序非常灵活，不必是<span
class="math inline">\(s_1,s_1,...,s_{\mathcal{|S|}}\)</span>,我们可以调整更新的顺序，使得价值信息能更有效地在状态间传播，如果某些状态与最优行为无关，甚至可以完全跳过这些状态进行更新，如此一来可以加快收敛速度；</li>
<li>一个智能体实际在MDP中进行真实交互时，我们可以在智能体访问特定状态时将其更新，即将DP算法的更新聚焦到部分与智能体最相关的状态集。</li>
</ul>
<h4 id="广义策略迭代">4.6 广义策略迭代</h4>
<p>在策略迭代中，有两个同时进行的相互作用的流程，分别是<strong>策略评估</strong>：使得价值函数与当前策略一致即<span
class="math inline">\(v\to v_\pi\)</span>;</p>
<p><strong>策略改进</strong>：根据当前价值函数贪心地更新策略。两者相互作用，最终到达最优价值函数和一个最优策略。</p>
<p>​
<img src="/2024/04/22/《RL-Introduction》-chap4/image-20240423222446830.png"  alt="image-20240423222446830" style="zoom:50%;" /><img src="/2024/04/22/《RL-Introduction》-chap4/image-20240423222526572.png"  alt="image-20240423222526572" style="zoom: 50%;" /></p>
<hr />
<h4 id="动态规划的效率">4.7 动态规划的效率</h4>
<ul>
<li>动态规划算法能保证在多项式时间内找到一个最优策略；</li>
<li>对于状态空间很大的问题，通常使用异步DP算法；</li>
</ul>
<h4 id="总结">总结</h4>
<ul>
<li>策略评估：对一个策略的价值函数进行迭代计算；</li>
<li>策略改进：给定某个策略的价值函数，计算一个改进的策略；</li>
<li>策略迭代：策略评估和策略改进交替进行；</li>
<li>价值迭代：</li>
<li>异步DP算法</li>
</ul>
]]></content>
      <categories>
        <category>RLintroduction</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>post《RL:Introduction》_chap3</title>
    <url>/2024/04/18/post%E3%80%8ARL-Introduction%E3%80%8B-chap3/</url>
    <content><![CDATA[<hr />
<h3 id="chap3-有限马尔可夫决策过程有限mdp">chap3
有限马尔可夫决策过程（有限MDP）</h3>
<h4 id="智能体-环境交互接口">3.1 “智能体-环境”交互接口</h4>
<ul>
<li><p>智能体与环境交互的流程：在每个时刻<span
class="math inline">\(t\)</span>，智能体处于状态<span
class="math inline">\(S_t\in\mathcal{S}\)</span>,选择一个动作<span
class="math inline">\(A_t\in\mathcal{A(s)}\)</span>并执行，环境返回一个收益<span
class="math inline">\(R_{t+1}\in\mathcal{R}\subset
R\)</span>,并进入一个新状态<span
class="math inline">\(S_{t+1}\)</span>(需要注意的是：1.这里用<span
class="math inline">\(R_{t+1}\)</span>而不是<span
class="math inline">\(R_t\)</span>表示<span
class="math inline">\(A_t\)</span>导致的收益，注意区分；2.这里假定<span
class="math inline">\(R_{t+1}\)</span>只是<span
class="math inline">\(S_t,A_t\)</span>的函数（实际上还和<span
class="math inline">\(S_{t+1}\)</span>）有关）；经过一系列交互，得到一个轨迹序列：</p>
<p><span
class="math inline">\(S_0,A_0,R_1,S_1,A_1,R_2,...,S_n,A_n\)</span></p></li>
<li><p>给定前继状态和前继动作的值，<span
class="math inline">\(s&#39;\in\mathcal{S},r\in\mathcal{R}\)</span>在<span
class="math inline">\(t\)</span>时刻出现的概率为<span
class="math inline">\(p(s&#39;,r|s,a)\dot{=}Pr\{S_t=s&#39;,R_t=r|S_{t-1}=s,A_{t-1}=a\}\)</span></p></li>
</ul>
<p>则： <span class="math display">\[
\sum\limits_{s&#39;\in\mathcal{S}}\sum\limits_{r\in\mathcal{R}}^{}p(s&#39;,r|s,a)=1,\forall
s\in\mathcal{S},a\in\mathcal{A}(s)
\]</span> <strong>马尔可夫性</strong>：<span
class="math inline">\(S_t\)</span>和<span
class="math inline">\(R_t\)</span>的每个可能的值出现的概率只取决于前一个状态<span
class="math inline">\(S_{t-1}\)</span>和前一个动作<span
class="math inline">\(A_{t-1}\)</span>，而与更早之前的状态和动作完全无关。</p>
<p>由上面的概率公式可以推导出其他函数：</p>
<ul>
<li>状态转移函数：<span
class="math inline">\(p(s&#39;|s,a)\dot{=}Pr\{S_t=s&#39;|S_{t-1}=s,A_{t-1}=a\}=\sum\limits_{r\in\mathcal{R}}p(s&#39;,r|s,a)\)</span></li>
<li>"状态-动作"二元组<span
class="math inline">\((s,a)\)</span>的期望收益:<span
class="math inline">\(r(s,a)\dot{=}E[R_t|S_{t-1}=s,A_{t-1}=a]=\sum\limits_{r\in\mathcal{R}}r\sum\limits_{s&#39;\in\mathcal{S}}p(s&#39;,r|s,a)\)</span></li>
<li>"状态-动作-后继状态"三元组的期望收益：<span
class="math inline">\(r(s,a,s&#39;)\dot{=}E[R_t|S_{t-1}=s,A_{t-1}=a,S_t=s&#39;]=\sum\limits_{r\in\mathcal{R}}r\frac{p(s&#39;,r|s,a)}{p(s&#39;|s,a)}\)</span></li>
</ul>
<p><strong>MDP转移图</strong>：如图所示，空心圆表示状态节点，圆内标注状态。实心圆表示动作节点，表示“状态-动作”二元组<span
class="math inline">\((s_t,a_t)\)</span></p>
<p>一个状态在选择完动作后经过对应的动作节点指向最终的状态，每个箭头代表着三元组<span
class="math inline">\((s_t,a_t,s_{t+1})\)</span>,转移的连线上标注转移概率</p>
<p><span
class="math inline">\(p(s_{t+1}|s_t,a_t)\)</span>和转移的期望收益<span
class="math inline">\(r(s_t,a_t,s_{t+1})\)</span>。</p>
<p>显然，所有经过同一个动作节点的出度之和为1，即<span
class="math inline">\(\sum\limits_{s&#39;}^{}p(s&#39;|s,a)=1\)</span></p>
<p><img src="/2024/04/18/post《RL-Introduction》-chap3/image-20240419112116709.png"  alt="image-20240419112116709" style="zoom:50%;" /></p>
<h4 id="目标和收益">3.2 目标和收益</h4>
<p>收益的设定只能用来传达“什么”是你想要实现的目标，而不是“如何”实现这个目标。目标可以归结为：最大化智能体接收到的收益（标量信号）的累积和的概率期望值。</p>
<h4 id="回报和分幕">3.3 回报和分幕</h4>
<ul>
<li><p><strong>幕</strong>：一般又称“回合”，指智能体与环境交互直到终结状态的整个过程；具有这样性质的任务被称为“分幕式任务”；与之对应的是“持续性任务”。</p></li>
<li><p>折扣回报： <span class="math display">\[
G_t=R_{t+1}+\gamma
R_{t+2}+\gamma^2R_{t+3}+...=\sum\limits_{k=0}^{\infty}\gamma^kR_{t+k+1}
\]</span> 其中，<span
class="math inline">\(0\le\gamma\le1\)</span>,为折扣率； <span
class="math display">\[
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+...
\]</span></p>
<p><span class="math display">\[
=R_{t+1}+\gamma\cdot(R_{t+2}+\gamma R_{t+3}+...)
\]</span></p>
<p><span class="math display">\[
=R_{t+1}+\gamma\cdot G_{t+1}
\]</span></p></li>
</ul>
<h4 id="分幕式和持续性任务的统一表示法">3.4
分幕式和持续性任务的统一表示法</h4>
<p>将回报统一表示为 <span class="math display">\[
G_t\dot{=}\sum\limits_{k=t+1}^{T}\gamma^{k-t-1}R_k=\sum\limits_{k=0}^{T-t-1}\gamma^{k}R_{t+k+1}
\]</span> 其中允许<span class="math inline">\(T=\infty\)</span>或<span
class="math inline">\(\gamma=1\)</span>（但二者不能同时满足，否则回报就无界了），<span
class="math inline">\(T=\infty\)</span>表示持续性任务，<span
class="math inline">\(\gamma=1\)</span>表示无折扣；</p>
<h4 id="策略和价值函数">3.5 策略和价值函数</h4>
<ul>
<li><p><strong>价值函数</strong>：其中状态价值函数用于评估智能体所处的状态的好坏，动作价值函数用于评估智能体在给定状态下执行给定动作的好坏；好坏的概念由回报的期望定义；</p></li>
<li><p><strong>策略</strong>:<span
class="math inline">\(\pi(a|s)\)</span>表示从状态到每个动作的选择概率之间的映射；</p></li>
<li><p><strong>状态价值函数</strong>：<span
class="math inline">\(v_{\pi}(s)\)</span>，表示从状态<span
class="math inline">\(s\)</span>开始，智能体按照策略<span
class="math inline">\(\pi\)</span>进行决策所获得的回报的期望值； <span
class="math display">\[
v_\pi(s)=E_{\pi}[G_t|S_t=s]=E_{\pi}[\sum\limits_{k=0}^{\infty}\gamma^kR_{t+k+1}|S_t=s],\forall
s\in\mathcal{S}
\]</span> 其中，<span
class="math inline">\(E_{\pi}[\cdot]\)</span>并不表示对策略求期望（事实上也不可能），而表示在给定策略<span
class="math inline">\(\pi\)</span>时括号中随机变量的期望值；事实上，是在对<span
class="math inline">\(S_{t+1},A_{t+1}，S_{t+2},A_{t+2},...,S_n,A_n\)</span>求期望。</p></li>
<li><p><strong>动作价值函数</strong>：<span
class="math inline">\(q_{\pi}(s,a)\)</span>,表示从状态<span
class="math inline">\(s\)</span>，执行动作<span
class="math inline">\(a\)</span>之后，所有可能的决策序列的期望回报 <span
class="math display">\[
q_\pi(s,a)=E_{\pi}[G_t|S_t=s,A_t=a]=E_{\pi}[\sum\limits_{k=0}^{\infty}\gamma^kR_{t+k+1}|S_t=s,A_t=a],\forall
s\in\mathcal{S},\forall a\in{\mathcal A}
\]</span></p></li>
<li><p>状态价值函数和动作价值函数的关系： <span class="math display">\[
v_{\pi}(s)=E_{a\sim\pi(\cdot|s)}[q_{\pi}(s,a)]=\sum_{a\in\mathcal{A}}\pi(a|s)q_{\pi}(s,a)
\]</span></p>
<p><span class="math display">\[
q_{\pi}(s,a) = E_{r,s^\prime}[r+\gamma
v_\pi(s^\prime)]=\sum_{r,s^\prime}p(s^\prime,r|s,a)[r+\gamma
v_{\pi}(s^\prime)]
\]</span></p></li>
<li><p>状态价值函数的递推关系：</p>
<p><img src="/2024/04/18/post《RL-Introduction》-chap3/643348098b69aaa72f678f47b4575e7.jpg"  alt="643348098b69aaa72f678f47b4575e7" style="zoom:50%;" /></p></li>
</ul>
<p>这被称为<span
class="math inline">\(v_\pi\)</span>的贝尔曼方程，它用等式表达了当前状态价值和后继状态价值之间的关系；</p>
<ul>
<li><p>动作价值函数的递推关系 <span class="math display">\[
q_\pi(s,a) = E_\pi[R_{t+1}+\gamma G_{t+1}|S_t=s,A_t=a] \\
\qquad \qquad \qquad \quad =
\sum_{s^\prime,r}p(s^\prime,r|s,a)[r+\gamma\sum_{a^\prime}\pi(a^\prime|s^\prime)
q_\pi(s^\prime,a^\prime)]
\]</span></p></li>
<li><p><span class="math inline">\(v_\pi\)</span>的回溯图：</p>
<p><img src="/2024/04/18/post《RL-Introduction》-chap3/image-20240419204326521.png"  alt="image-20240419204326521" style="zoom:50%;" /></p></li>
</ul>
<p>起始状态为<span class="math inline">\(s\)</span>，智能体通过策略<span
class="math inline">\(\pi(\cdot|s)\)</span>选择一个动作<span
class="math inline">\(a\)</span>并执行，然后通过状态转移函数（或者更通用地，四参数函数p)转移到一个新的状态<span
class="math inline">\(s&#39;\)</span>,环境再返回一个收益<span
class="math inline">\(r\)</span>.贝尔曼方程从左到右的写法就是这样一个过程，只是它对所有的可能性采用其出现概率进行了加权平均。贝尔曼方程也说明，起始状态的价值=当前动作的收益+后继状态的折扣期望价值。</p>
<hr />
<ul>
<li><strong>网格问题</strong>：</li>
</ul>
<p><img src="/2024/04/18/post《RL-Introduction》-chap3/image-20240420104313200.png"  alt="image-20240420104313200" style="zoom:50%;" /></p>
<p><strong>规定</strong>：智能体在网格中移动，每个格子代表状态，在每个状态中均有四个可选的动作，分别为上下左右，若动作使得智能体移出边界，则智能体位置保持原格不动，并且获得-1的收益；若智能体处于A状态，则无论选择什么动作，智能体都会转移到A’状态并获得+10收益；而若智能体处于B状态，则无论选择什么动作，智能体都会转移到B‘状态并获得+5收益；其余情况随意移动，并且收益为0；右图则是通过下式列出线性方程组求解得到的每个格子的状态价值函数；</p>
<p><img src="/2024/04/18/post《RL-Introduction》-chap3/643348098b69aaa72f678f47b4575e7.jpg"  alt="643348098b69aaa72f678f47b4575e7" style="zoom:50%;" /></p>
<h4 id="最优策略和最优价值函数">3.6 最优策略和最优价值函数</h4>
<ul>
<li><p>最优策略：总会存在至少一个策略，它不劣于其他所有的策略，我们称之为<strong>最优策略</strong>；</p></li>
<li><p>最优状态价值函数： <span class="math display">\[
v_*(s)=\underset{\pi}{max}\ v_{\pi}(s)
\]</span> 它表示了，在状态<span
class="math inline">\(s\)</span>下，之后按照最优策略去决策的期望回报</p></li>
<li><p>最优动作价值函数： <span class="math display">\[
q_*(s,a)=\underset{\pi}{max}\ q_{\pi}(s,a)
\]</span> 它表示了，在状态<span
class="math inline">\(s\)</span>下，先采取动作<span
class="math inline">\(a\)</span>,之后按照最优策略去决策的期望回报</p></li>
</ul>
<p>（注意：状态价值和动作价值求max选出来的最优策略可以是完全一致的）</p>
<ul>
<li><p><span class="math inline">\(v_*\)</span>和<span
class="math inline">\(q_*\)</span>的关系： <span class="math display">\[
q_*(s,a)=E[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s,A_t=a]
\]</span></p></li>
<li><p><strong>贝尔曼最优方程</strong>：</p>
<ul>
<li><p><span class="math inline">\(v_*\)</span>的贝尔曼最优方程 <span
class="math display">\[
v_*(s)
\]</span></p>
<p><span class="math display">\[
=\underset{a\in\mathcal{A(s)}}{max}q_{\pi_*}(s,a)
\]</span></p>
<p><span class="math display">\[
=\underset{a}{max}E_{\pi_*}[G_t|S_t=s,A_t=a]
\]</span></p>
<p><span class="math display">\[
=\underset{a}{max}E_{\pi_*}[R_{t+1}+\gamma G_{t+1}|S_t=s,A_t=a]
\]</span></p>
<p><span class="math display">\[
=\underset{a}{max}E[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s,A_t=a]
\]</span></p>
<p><span class="math display">\[
=\underset{a}{max}\sum\limits_{s&#39;,r}^{}p(s&#39;,r|s,a)[r+\gamma
v_*(s&#39;)]
\]</span></p></li>
</ul>
<p>注意：这里<span class="math inline">\(R_{t+1}\)</span>和<span
class="math inline">\(a\)</span>相关，因此max放外面；</p>
<ul>
<li><p><span class="math inline">\(q_*\)</span>的贝尔曼最优方程 <span
class="math display">\[
q_*(s,a)=E[R_{t+1}+\gamma\ \underset{a&#39;}{max}\
q_*(S_{t+1},a&#39;)|S_t=s,A_t=a]
\]</span></p>
<p><span class="math display">\[
=\sum\limits_{s&#39;,r}^{}p(s&#39;,r|s,a)[r+\gamma\
\underset{a&#39;}{max}\ q_*(s&#39;,a&#39;)]
\]</span></p>
<p><span class="math display">\[
也可以进一步=\sum\limits_{s&#39;,r}^{}p(s&#39;,r|s,a)[r+\gamma\
v_*(s&#39;)]
\]</span></p>
<p>注意：这里<span class="math inline">\(R_{t+1}\)</span>和<span
class="math inline">\(a&#39;\)</span>无关，因此max放里面；</p>
<p><img src="/2024/04/18/post《RL-Introduction》-chap3/image-20240420141223501.png"  alt="image-20240420141223501" style="zoom:50%;" /></p></li>
</ul></li>
</ul>
<h4 id="分析">3.7 分析</h4>
<ul>
<li>若最优状态价值函数<span
class="math inline">\(v_*(s)\)</span>给定，<span
class="math inline">\(v_*(s)=\underset{a\in\mathcal{A(s)}}{max}q_{\pi_*}(s,a)\)</span>,满足max条件的动作可能有一个或多个，那么那些只将这些动作的概率设为非零值的策略就是最优策略；</li>
<li>若给定最优动作价值函数<span
class="math inline">\(q_*(s,a)\)</span>，则选取动作只需：<span
class="math inline">\(a_*=\underset{a\in\mathcal{A}}{max}\
q_*(s,a)\)</span></li>
</ul>
]]></content>
      <categories>
        <category>RLintroduction</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>《RL:Introduction》-chap5</title>
    <url>/2024/04/30/%E3%80%8ARL-Introduction%E3%80%8B-chap5/</url>
    <content><![CDATA[<hr />
<hr />
<h3 id="chap5-蒙特卡洛方法">chap5 蒙特卡洛方法</h3>
<ul>
<li>动态规划算法（DP)需要生成所有可能的状态转移的概率分布<span
class="math inline">\(p(s&#39;,r|s,a)\)</span>，从马尔可夫决策过程的知识中计算价值函数；</li>
<li>蒙特卡洛方法通过生成状态转移的一些样本，从马尔可夫决策过程采样样本的经验回报中学习价值函数；</li>
</ul>
<h4 id="蒙特卡洛预测">5.1 蒙特卡洛预测</h4>
<p>状态价值函数的定义为：<span
class="math inline">\(v_\pi(s)=E_\pi[G_t|S_t=s]\)</span>,因此可以通过对所有经过这个状态之后产生的回报进行平均，从而近似状态价值函数。</p>
<ul>
<li>首次访问型MC算法：用<span
class="math inline">\(s\)</span>的所有<strong>首次</strong>访问的回报的平均值估计<span
class="math inline">\(v_\pi(s)\)</span>;</li>
<li>每次访问型MC算法：使用<span
class="math inline">\(s\)</span>的<strong>所有次</strong>访问的回报的平均值估计<span
class="math inline">\(v_\pi(s)\)</span></li>
</ul>
<p>以上算法都能收敛到<span class="math inline">\(v_\pi(s)\)</span></p>
<p><img src="/2024/04/30/《RL-Introduction》-chap5/image-20240430141719947.png"  alt="image-20240430141719947" style="zoom: 50%;" /></p>
<p>注意：在计算回报时，可通过公式<span
class="math inline">\(G_t=R_{t+1}+\gamma
G_{t+1}\)</span>反向计算，但当遍历状态时应从前往后以保证加入集合<span
class="math inline">\(Return(S_t)\)</span>的回报值在当前幕下为首次访问；每次访问型MC算法只需将Unless判断删除即可。</p>
<ul>
<li><p>DP算法的回溯图显示所有可能的一步转移，MC算法包含采样到的当前幕中到结束的的所有转移。</p>
<p><img src="/2024/04/30/《RL-Introduction》-chap5/bbc89e3036b55ef4374d7d67dd31f4f-1714631064257-2.png"  alt="bbc89e3036b55ef4374d7d67dd31f4f" style="zoom:50%;" /><img src="/2024/04/30/《RL-Introduction》-chap5/image-20240502142551710.png"  alt="image-20240502142551710" style="zoom:50%;" /></p></li>
<li><p>MC算法对于一个状态的估计完全不依赖于对其他状态的估计，没有使用自举思想；</p></li>
</ul>
<h4 id="动作价值的蒙特卡洛估计">5.2 动作价值的蒙特卡洛估计</h4>
<ul>
<li>通过对状态-动作二元组<span
class="math inline">\((s,a)\)</span>回报的蒙特卡洛采样来近似动作价值函数；</li>
<li>一些状态-动作二元组<span
class="math inline">\((s,a)\)</span>采样次数不够，可通过<strong>试探性出发</strong>：将指定的状态-动作二元组作为起点开始一幕采样同时保证所有”状态-动作“二元组都有非零的概率被选为起点；或者只考虑那些在每个状态下所有动作都有非零概率被选中的策略。</li>
</ul>
<h4 id="蒙特卡洛控制">5.3 蒙特卡洛控制</h4>
<p><img src="/2024/04/30/《RL-Introduction》-chap5/6dc8e84f02cdd04053baa83024c1e53.png"  alt="6dc8e84f02cdd04053baa83024c1e53" style="zoom:50%;" /></p>
<p>其中，<span
class="math inline">\(E,I\)</span>分别代表策略评估和策略改进，最终收敛到最优策略<span
class="math inline">\(\pi_*\)</span>,最优动作价值函数<span
class="math inline">\(q_*\)</span>.这里策略改进的方法也是在当前价值函数上贪心地选择动作，即<span
class="math inline">\(\pi(s)=\underset{a}{argmax}\ q(s,a)\)</span>;</p>
<p>蒙特卡洛算法的收敛需要满足两个假设：（1）试探性出发假设，保证所有状态-动作二元组都有足够的数据支持（2）在进行策略评估时有无限多幕的样本序列进行试探，<strong>改进</strong>：想法设法地在每次策略评估中对<span
class="math inline">\(q_{\pi_k}\)</span>做出尽量好的逼近<strong>或</strong>不再要求在策略改进前就完成策略评估；</p>
<h4
id="基于试探性出发的蒙特卡洛蒙特卡洛es">基于试探性出发的蒙特卡洛（蒙特卡洛ES）</h4>
<p><img src="/2024/04/30/《RL-Introduction》-chap5/b29bdc510abe9fef8c0daf82b934cc5.png"  alt="b29bdc510abe9fef8c0daf82b934cc5" style="zoom:50%;" /></p>
<h4 id="没有试探性出发假设的蒙特卡洛控制">5.4
没有试探性出发假设的蒙特卡洛控制</h4>
<ul>
<li><strong>同轨策略</strong>：用于生成采样数据序列的策略和用于实际决策的待评估和改进的策略是相同的；</li>
<li><strong>离轨策略</strong>：用于生成采样数据序列的策略和用于实际决策的待评估和改进的策略是不同的；</li>
</ul>
<p>由于缺乏试探性假设，需要将之前的贪心策略改为<span
class="math inline">\(\epsilon\)</span>-贪心策略，同轨策略的首次访问型MC控制算法如下：</p>
<p><img src="/2024/04/30/《RL-Introduction》-chap5/image-20240502174330393.png"  alt="image-20240502174330393" style="zoom:50%;" /></p>
<p>其中，当<span
class="math inline">\(Q(S_t,a)\)</span>有多个最大值时，任意选取；</p>
<p>根据策略改进定理，对一个<span
class="math inline">\(\epsilon\)</span>-软性策略<span
class="math inline">\(\pi\)</span>(即对于某个<span
class="math inline">\(\varepsilon&gt;0\)</span>,所有的状态和动作都有<span
class="math inline">\(\pi(a|s)\ge\frac{\varepsilon}{|\mathcal{A}(s)|}\)</span>)，任何一个根据<span
class="math inline">\(q_{\pi}\)</span>生成的贪心策略<span
class="math inline">\(\pi&#39;\)</span>都是对其的一个改进，下面证明：
<span class="math display">\[
q_\pi(s,\pi&#39;(s))=\sum\limits_{a}^{}\pi&#39;(a|s)q_\pi(s,a)
\]</span></p>
<p><span class="math display">\[
=\frac{\varepsilon}{|\mathcal{A(s)}|}\sum\limits_{a}^{}q_\pi(s,a)+(1-\varepsilon)\underset{a}{max}\
q_\pi(s,a)
\]</span></p>
<p><span class="math display">\[
\ge\frac{\varepsilon}{|\mathcal{A(s)}|}\sum\limits_{a}^{}q_\pi(s,a)+(1-\varepsilon)\sum_{a}^{}\frac{\pi(a|s)-\frac{\varepsilon}{|\mathcal{A}(s)|}}{1-\varepsilon}q_\pi(s,a)
\]</span></p>
<p>其中<span
class="math inline">\(\sum_{a}^{}\frac{\pi(a|s)-\frac{\varepsilon}{|\mathcal{A}(s)|}}{1-\varepsilon}=\frac{|\mathcal{A(s)}|(\pi(a|s)-\frac{\varepsilon}{|\mathcal{A(s)}|})}{1-\varepsilon}=1\)</span>即表示各个动作价值函数加和为1的权值，加权和必然小于等于其中的最大值，</p>
<p>将<span class="math inline">\(1-\varepsilon\)</span>消掉，得到：
<span class="math display">\[
=\frac{\varepsilon}{|\mathcal{A}(s)|}\sum\limits_{a}^{}q_\pi(s,a)-\frac{\varepsilon}{|\mathcal{A(s)}|}\sum\limits_{a}^{}q_\pi(s,a)+\sum\limits_{a}\pi(a|s)q_\pi(s,a)=v_\pi(s)
\]</span> 故<span class="math inline">\(q_\pi(s,\pi&#39;(s))\ge
v_\pi(s)\)</span>,再由策略改进定理易知对于所有的<span
class="math inline">\(s\)</span>，有<span
class="math inline">\(v_{\pi&#39;}(s)\ge v_\pi(s)\)</span>,即策略<span
class="math inline">\(\pi&#39;\)</span>好于策略<span
class="math inline">\(\pi\)</span>,上式等号成立当且仅当<span
class="math inline">\(\pi\)</span>和<span
class="math inline">\(\pi&#39;\)</span>都是最优的<span
class="math inline">\(\epsilon-\)</span>软性策略；</p>
<ul>
<li>上述方法只能获得<span
class="math inline">\(\epsilon\)</span>-软性策略集合中的最优策略；</li>
</ul>
<h4 id="基于重要度采样的离轨策略">5.5 基于重要度采样的离轨策略</h4>
<ul>
<li>行动策略：用于生成行动样本的策略；</li>
<li>目标策略：用来学习并优化的策略；</li>
<li>离轨策略学习：学习所用的数据离开了待学习的目标策略；</li>
</ul>
<h4 id="增量式实现">5.6 增量式实现</h4>
]]></content>
      <categories>
        <category>RLintroduction</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>斐波那契数列算法总结</title>
    <url>/2024/04/01/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h3 id="斐波那契数列">斐波那契数列：</h3>
<p><span class="math inline">\(\begin{cases}
0, &amp; \text{  } n=0 \\
1,&amp; \text{  } n=1,2 \\
f(n-1)+f(n-2), &amp; \text{  } n&gt;2
\end{cases}\)</span></p>
<h4 id="方法一朴素递归">方法一、朴素递归</h4>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">fib</span><span class="params">(<span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">1</span> || n==<span class="number">2</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">fib</span>(n<span class="number">-1</span>) + <span class="built_in">fib</span>(n<span class="number">-2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>斐波那契数列的通项公式为： <span class="math display">\[
f(n)=\frac{1}{\sqrt{5}} \left [ (\frac{1+\sqrt[]{5} }{2} )^{n}-
(\frac{1-\sqrt[]{5} }{2} )^{n}\right ]
\]</span></p>
<p><img src="/2024/04/01/斐波那契数列算法总结/image-20240401221016279.png"  alt="image-20240401221016279" style="zoom: 55%;" /></p>
<p>如图所示，递归计算的终点都是<span
class="math inline">\(fib(1)\)</span>和<span
class="math inline">\(fib(2)\)</span>，因为它们是直接返回的，因此计算<span
class="math inline">\(fib(n)\)</span>的时间复杂度为计算<span
class="math inline">\(fib(1)\)</span>和<span
class="math inline">\(fib(2)\)</span>的次数，同时也等于<span
class="math inline">\(fib(n)\)</span>本身，因此<span
class="math inline">\(T(n)=O(fib(n))=O(\frac{1}{\sqrt{5}} \left [
(\frac{1+\sqrt[]{5} }{2} )^{n}- (\frac{1-\sqrt[]{5} }{2} )^{n}\right
])=O((\frac{1+\sqrt[]{5} }{2}
)^{n})\)</span>，第二项绝对值小于1，为<span
class="math inline">\(n\)</span>阶无穷小，可舍去，空间复杂度为函数调用栈的高度<span
class="math inline">\(n\)</span>,即<span
class="math inline">\(S(n)=O(n)\)</span>。</p>
<h4 id="方法二尾递归">方法二、尾递归</h4>
<p>首先需要清楚递归和尾递归的区别：</p>
<ul>
<li>1.递归：在调用函数自身后还有事要做，需要保存当前轮次的环境，以供后续返回时使用；</li>
</ul>
<p>如计算自然数前<span class="math inline">\(n\)</span>项和的函数<span
class="math inline">\(sum(n)\)</span>,递归实现方式如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">sum</span><span class="params">(n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> n + <span class="built_in">sum</span>(n<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里的“+",即前面所说”还要做的事“，其计算过程如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sum</span>(<span class="number">5</span>)</span><br><span class="line"><span class="number">5</span> + <span class="built_in">sum</span>(<span class="number">4</span>)</span><br><span class="line"><span class="number">5</span> + (<span class="number">4</span> + <span class="built_in">sum</span>(<span class="number">3</span>))</span><br><span class="line"><span class="number">5</span> + (<span class="number">4</span> + (<span class="number">3</span> + <span class="built_in">sum</span>(<span class="number">2</span>)))</span><br><span class="line"><span class="number">5</span> + (<span class="number">4</span> + (<span class="number">3</span> + (<span class="number">2</span> + <span class="built_in">sum</span>(<span class="number">1</span>))))</span><br><span class="line"><span class="number">5</span> + (<span class="number">4</span> + (<span class="number">3</span> + (<span class="number">2</span> + (<span class="number">1</span> + <span class="built_in">sum</span>(<span class="number">0</span>)))))</span><br><span class="line"><span class="number">5</span> + (<span class="number">4</span> + (<span class="number">3</span> + (<span class="number">2</span> + (<span class="number">1</span> + <span class="number">0</span>))))</span><br><span class="line"><span class="number">5</span> + (<span class="number">4</span> + (<span class="number">3</span> + (<span class="number">2</span> + <span class="number">1</span>)))</span><br><span class="line"><span class="number">5</span> + (<span class="number">4</span> + (<span class="number">3</span> + <span class="number">3</span>))</span><br><span class="line"><span class="number">5</span> + (<span class="number">4</span> + <span class="number">6</span>)</span><br><span class="line"><span class="number">5</span> + <span class="number">10</span></span><br><span class="line"><span class="number">15</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>2.尾递归：每轮直接return，不需要保存当前环境供后续处理。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">sum</span><span class="params">(<span class="type">int</span> n,<span class="type">int</span> total = <span class="number">0</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> total;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(n<span class="number">-1</span>,total+n);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其计算过程如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sum</span>(<span class="number">5</span>, <span class="number">0</span>)</span><br><span class="line"><span class="built_in">sum</span>(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="built_in">sum</span>(<span class="number">3</span>, <span class="number">9</span>)</span><br><span class="line"><span class="built_in">sum</span>(<span class="number">2</span>, <span class="number">12</span>)</span><br><span class="line"><span class="built_in">sum</span>(<span class="number">1</span>, <span class="number">14</span>)</span><br><span class="line"><span class="built_in">sum</span>(<span class="number">0</span>, <span class="number">15</span>)</span><br><span class="line"><span class="number">15</span></span><br></pre></td></tr></table></figure>
<p>利用尾递归求斐波那契数列：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">fib</span><span class="params">(<span class="type">int</span> x1,<span class="type">int</span> x2,<span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">1</span> || n==<span class="number">2</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(n==<span class="number">3</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> x1+x2;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">fib</span>(x2,x1+x2,n<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>计算过程如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">fib</span>(<span class="number">6</span>)=<span class="built_in">fib</span>(<span class="number">1</span>,<span class="number">1</span>,<span class="number">6</span>)</span><br><span class="line"><span class="built_in">fib</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line"><span class="built_in">fib</span>(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">fib</span>(<span class="number">3</span>,<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line"><span class="number">8</span></span><br></pre></td></tr></table></figure>
<p>观察变量<span
class="math inline">\(n\)</span>的变化可知，时间复杂度<span
class="math inline">\(T(n)=O(n)\)</span>,辅助空间只需要<span
class="math inline">\(x1,x2\)</span>,因此空间复杂度<span
class="math inline">\(S(n)=O(1)\)</span>.</p>
<h4 id="方法三非递归迭代">方法三、非递归(迭代)</h4>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">fib</span><span class="params">(<span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(n==<span class="number">1</span>||n==<span class="number">2</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> x=<span class="number">1</span>,y=<span class="number">1</span>;<span class="type">int</span> tmp = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>;i &lt; n<span class="number">-2</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            tmp = x+y;</span><br><span class="line">            x = y;</span><br><span class="line">            y = tmp;  </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> y;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>观察变量<span
class="math inline">\(n\)</span>的变化可知，时间复杂度<span
class="math inline">\(T(n)=O(n)\)</span>,辅助空间只需要<span
class="math inline">\(x,y,i,tmp\)</span>,因此空间复杂度<span
class="math inline">\(S(n)=O(1)\)</span>.</p>
<h4 id="方法四矩阵快速幂">方法四、矩阵快速幂</h4>
<ul>
<li><h5 id="快速幂">快速幂：</h5>
<p>在计算<span class="math inline">\(a^n\)</span>时，若使用<span
class="math inline">\(a^n=a\ \cdot a \ \cdot a...a
(n个)\)</span>方法，则时间复杂度为<span
class="math inline">\(O(n)\)</span>;快速幂的思想是，将<span
class="math inline">\(n\)</span>写成二进制形式</p>
<p><span
class="math inline">\((n_tn_{t-1}...n_1n_0)_2\)</span>,那么<span
class="math inline">\(a^n = a^{n_t\cdot 2^t}*a^{n_{t-1}\cdot
2^{t-1}}*...*a^{n_0\cdot 2^0}\)</span>,其中<span
class="math inline">\(n_i\in\{0,1\}\)</span></p>
<p>因此我们只需要将<span class="math inline">\(2^0\ 2^1....2^{\left
\lfloor log_{2}{n} \right
\rfloor}\)</span>算出，再将二进制位为1对应的幂运算结果相乘即可，时间复杂度为<span
class="math inline">\(O(log n)\)</span>。</p>
<p>比如：<img src="/2024/04/01/斐波那契数列算法总结/image-20240403132154215.png"  alt="image-20240403132154215" style="zoom:67%;" /><img src="/2024/04/01/斐波那契数列算法总结/image-20240403132250525.png"  alt="image-20240403132250525" style="zoom:67%;" /></p>
<p>快速幂分为递归和迭代两种实现方式，两者理论时间复杂度都为<span
class="math inline">\(O(logn)\)</span>,通常情况下，迭代性能较好。</p>
<h5 id="递归版本">(1)递归版本</h5>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">double</span> <span class="title">quickPow</span><span class="params">(<span class="type">double</span> a,<span class="type">long</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (n&lt;<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> (<span class="number">1.0</span>/<span class="built_in">quickPow</span>(a,-n));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> res = <span class="built_in">quickPow</span>(a,n/<span class="number">2</span>);</span><br><span class="line">        <span class="keyword">if</span> (n%<span class="number">2</span>==<span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> res * res *a;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> res * res;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如：计算<span
class="math inline">\(2^5\)</span>的调用及计算过程如下：</p>
<p><img src="/2024/04/01/斐波那契数列算法总结/image-20240403135754758.png"  alt="image-20240403135754758" style="zoom: 25%;" /></p></li>
</ul>
<h5 id="迭代版本">(2)迭代版本</h5>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">double</span> <span class="title">quickPow</span><span class="params">(<span class="type">long</span> <span class="type">long</span> a, <span class="type">long</span>  n)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n&lt;<span class="number">0</span>) <span class="keyword">return</span> <span class="built_in">quickPow</span>(a,-n);</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> res = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (n &gt; <span class="number">0</span>) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (n &amp; <span class="number">1</span>)<span class="comment">//相当于b%2==1</span></span><br><span class="line">        &#123;</span><br><span class="line">            res = res * a;</span><br><span class="line">        &#125;</span><br><span class="line">        a = a * a;</span><br><span class="line">        n &gt;&gt;= <span class="number">1</span>;<span class="comment">//右移一位相当于b=b/2;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>如计算<span class="math inline">\(2^{10}\)</span>的迭代过程如下：</p>
<p><img src="/2024/04/01/斐波那契数列算法总结/image-20240403142939144.png"  alt="image-20240403142939144" style="zoom: 33%;" /></p>
<ul>
<li><h3 id="快速矩阵幂">快速矩阵幂</h3>
<p>由斐波那契数列的递推公式可知：</p>
<p><span
class="math inline">\(\begin{bmatrix}f(n)\\f(n-1)\end{bmatrix}=\begin{bmatrix}1&amp;1\\1&amp;0\end{bmatrix}\begin{bmatrix}f(n-1)\\f(n-2)\end{bmatrix}=\cdot
\cdot \cdot =\begin{bmatrix}
1&amp;1\\1&amp;0\end{bmatrix}^{n-1}\begin{bmatrix}f(1)\\f(0)\end{bmatrix}=\cdot
\cdot \cdot =\begin{bmatrix}
1&amp;1\\1&amp;0\end{bmatrix}^{n-1}\begin{bmatrix}
1\\0\end{bmatrix}\)</span></p>
<p>故，令<span class="math inline">\(A=\begin{bmatrix}
1&amp;1\\1&amp;0\end{bmatrix}^{n-1}\)</span>，则<span
class="math inline">\(f(n)=A[0][0]\)</span></p>
<p>矩阵快速幂和快速幂的方法和思想一致，都是将先前的计算结果保存下来以供后续使用，减小计算量，只需将<span
class="math inline">\(1\)</span>换为单位矩阵，将常熟换为矩阵<span
class="math inline">\(A\)</span>即可.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 矩阵乘法函数</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">multiply</span><span class="params">(<span class="type">long</span> <span class="type">long</span> a[<span class="number">2</span>][<span class="number">2</span>], <span class="type">long</span> <span class="type">long</span> b[<span class="number">2</span>][<span class="number">2</span>])</span> </span>&#123;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> mul[<span class="number">2</span>][<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; j++) &#123;</span><br><span class="line">            mul[i][j] = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; <span class="number">2</span>; k++)</span><br><span class="line">                mul[i][j] += a[i][k] * b[k][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 将乘法结果复制回a矩阵</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; j++) &#123;</span><br><span class="line">            a[i][j] = mul[i][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 快速矩阵幂算法</span></span><br><span class="line"><span class="function"><span class="type">long</span> <span class="type">long</span> <span class="title">matrixPower</span><span class="params">(<span class="type">long</span> <span class="type">long</span> matrix[<span class="number">2</span>][<span class="number">2</span>], <span class="type">long</span> <span class="type">long</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> result[<span class="number">2</span>][<span class="number">2</span>] = &#123;&#123;<span class="number">1</span>, <span class="number">0</span>&#125;, &#123;<span class="number">0</span>, <span class="number">1</span>&#125;&#125;; <span class="comment">// 单位矩阵</span></span><br><span class="line">    <span class="keyword">while</span> (n &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (n &amp; <span class="number">1</span>)</span><br><span class="line">            <span class="built_in">multiply</span>(result, matrix);</span><br><span class="line">        <span class="built_in">multiply</span>(matrix, matrix);</span><br><span class="line">        n &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算斐波那契数</span></span><br><span class="line"><span class="function"><span class="type">long</span> <span class="type">long</span> <span class="title">fibonacci</span><span class="params">(<span class="type">long</span> <span class="type">long</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> matrix[<span class="number">2</span>][<span class="number">2</span>] = &#123;&#123;<span class="number">1</span>, <span class="number">1</span>&#125;, &#123;<span class="number">1</span>, <span class="number">0</span>&#125;&#125;;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">matrixPower</span>(matrix, n - <span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   </span><br><span class="line">    cout &lt;&lt;<span class="built_in">fibonacci</span>(<span class="number">7</span>) &lt;&lt; endl;</span><br><span class="line">    <span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>分析程序知时间复杂度为<span
class="math inline">\(O(logn)\)</span>,空间复杂度为<span
class="math inline">\(O(1)\)</span></p>
<h3 id="总结">总结</h3>
<p>（1)朴素递归：<span
class="math inline">\(T(n)=O(fib(n))=O(\frac{1}{\sqrt{5}} \left [
(\frac{1+\sqrt[]{5} }{2} )^{n}- (\frac{1-\sqrt[]{5} }{2} )^{n}\right
])=O((\frac{1+\sqrt[]{5} }{2} )^{n})\)</span>,<span
class="math inline">\(S(n)=O(n)\)</span></p>
<ol start="2" type="1">
<li>尾递归：<span class="math inline">\(T(n)=O(n)\)</span>,<span
class="math inline">\(S(n)=O(1)\)</span></li>
</ol>
<p>(3)非递归（迭代）：<span
class="math inline">\(T(n)=O(n)\)</span>,<span
class="math inline">\(S(n)=O(1)\)</span></p>
<p>（4) 矩阵快速幂：<span
class="math inline">\(T(n)=O(logn)\)</span>,<span
class="math inline">\(S(n)=O(1)\)</span></p></li>
</ul>
]]></content>
      <categories>
        <category>编程算法</category>
      </categories>
      <tags>
        <tag>算法、数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>《RL:Introduction》-chap6</title>
    <url>/2024/05/04/%E3%80%8ARL-Introduction%E3%80%8B-chap6/</url>
    <content><![CDATA[<hr />
<h3 id="chap6-时序差分学习">chap6 时序差分学习</h3>
<ul>
<li><strong>预测问题（策略评估问题）</strong>：如何对于一个给定的策略<span
class="math inline">\(\pi\)</span>估计它的价值函数<span
class="math inline">\(v_\pi\)</span>;</li>
<li><strong>控制问题</strong>：找到最优的策略；</li>
</ul>
<h4 id="时序差分预测">6.1 时序差分预测</h4>
<p>蒙特卡洛方法以时刻t观测到的真实的回报<span
class="math inline">\(G_t\)</span>为目标去更新<span
class="math inline">\(V(S_t)\)</span>: <span class="math display">\[
V(S_t)\gets V(S_t)+\alpha [G_t-V(S_t)]
\]</span> TD方法以<span class="math inline">\(R_{t+1}+\gamma
V(S_{t+1})\)</span>为目标去更新<span
class="math inline">\(V(S_{t})\)</span>： <span class="math display">\[
V(S_t)\gets V(S_t)+\alpha [R_{t+1}+\gamma V(S_{t+1})-V(S_t)]
\]</span> 这种方法被称为<span
class="math inline">\(TD(0)\)</span>或单步<span
class="math inline">\(TD\)</span>;TD方法结合了蒙特卡洛方法和DP方法的优势。</p>
<p>其中TD目标<span class="math inline">\(R_{t+1}+\gamma
V(S_{t+1})\)</span>和现状<span
class="math inline">\(V(S_t)\)</span>之间的差异被称为TD误差：即<span
class="math inline">\(\delta_t=R_{t+1}+\gamma
V(S_{t+1})-V(S_t)\)</span></p>
<ul>
<li><p>若价值函数数组<span
class="math inline">\(V\)</span>在一幕内没有改变，则蒙特卡洛误差可写成TD误差之和：即<span
class="math inline">\(G_t-V(S_t)=\sum\limits_{k=t}^{T-1}\gamma^{k-t}\delta_k\)</span></p></li>
<li><p>蒙特卡洛方法需要得到<span
class="math inline">\(G_t\)</span>再更新，即需要知道最终结果；而TD方法只需要一步即可进行更新；</p></li>
</ul>
<h4 id="时序差分预测方法的优势">6.2 时序差分预测方法的优势</h4>
<ul>
<li>TD方法不需要
描述收益和下一个状态的联合概率分布的模型，即不需要知道<span
class="math inline">\(p(s&#39;,r|s,a)\)</span>；</li>
<li>TD方法下一时刻即可更新；</li>
<li>在实践中，TD方法在随机任务上通常比常量<span
class="math inline">\(\alpha\)</span>MC方法收敛得更快；</li>
</ul>
<h4 id="td0的最优性">6.3 TD(0)的最优性</h4>
<p>未完待续</p>
<h4 id="sarsa同轨策略下的时序差分控制">6.4
Sarsa：同轨策略下的时序差分控制</h4>
<p>通过五元组数据<span
class="math inline">\((S_t,A_t,R_{t+1},S_{t+1},A_{t+1})\)</span>对动作价值函数进行更新：
<span class="math display">\[
Q(S_t,A_t)\gets Q(S_t,A_t)+\alpha [R_{t+1}+\gamma
Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]
\]</span> 每当从非终止状态<span
class="math inline">\(S_t\)</span>出现一次转移之后，就进行上式的一次更新。特别地，如果<span
class="math inline">\(S_{t+1}\)</span>是终止状态，则<span
class="math inline">\(Q(S_{t+1},A_{t+1})=0\)</span>;</p>
<p>Sarsa算法的流程如图所示：</p>
<p><img src="/2024/05/04/《RL-Introduction》-chap6/image-20240504203035657.png"  alt="image-20240504203035657" style="zoom: 33%;" /></p>
<h4 id="q-learning-离轨策略下的时序差分控制">6.5 Q-learning:
离轨策略下的时序差分控制</h4>
<p>Q学习的更新公式为： <span class="math display">\[
Q(S_t,A_t)\gets Q(S_t,A_t)+\alpha [R_{t+1}+\gamma\ \underset{a}{max}\
Q(S_{t+1},a)-Q(S_t,A_t)]
\]</span> <span
class="math inline">\(Q\)</span>值最终会收敛到最优动作价值函数<span
class="math inline">\(q_*\)</span>；</p>
<p>Q学习算法的流程如图所示：</p>
<p><img src="/2024/05/04/《RL-Introduction》-chap6/image-20240504211634752.png"  alt="image-20240504211634752" style="zoom: 33%;" /></p>
<p>Q学习的回溯图：</p>
<p><img src="/2024/05/04/《RL-Introduction》-chap6/image-20240504213502124.png"  alt="image-20240504213502124" style="zoom:33%;" /></p>
<h4 id="期望sarsa">6.6 期望Sarsa</h4>
<p>期望Sarsa算法更新规则为： <span class="math display">\[
Q(S_t,A_t)\gets Q(S_t,A_t)+\alpha [R_{t+1}+\gamma
E[Q(S_{t+1},A_{t+1})|S_{t+1}]-Q(S_t,A_t)]
\]</span></p>
<p><span class="math display">\[
\gets Q(S_t,A_t)+\alpha [R_{t+1}+\gamma
\sum\limits_{a}^{}\pi(a|S_{t+1})Q(S_{t+1},a)-Q(S_t,A_t)]
\]</span></p>
<p>期望Sarsa的回溯图：</p>
<p><img src="/2024/05/04/《RL-Introduction》-chap6/image-20240504215509521.png"  alt="image-20240504215509521" style="zoom:33%;" /></p>
<p>在相同数量的经验下，期望Sarsa的表现更佳；</p>
<h4 id="最大化偏差与双学习">6.7 最大化偏差与双学习</h4>
<p><span class="math inline">\(\underset{a}{max}\
Q(S_{t+1},a)\)</span>这样一个表达式实质上包含两个操作：（1）确定价值最大的动作<span
class="math inline">\(a^*=\underset{a}{argmax}\
Q\)</span>;(2)估计价值<span class="math inline">\(Q(a^*)\)</span></p>
<p>若确定价值最大的动作和估计它的价值这两个流程采用相同的样本（多幕序列），则会产生最大化偏差问题，双Q学习则将样本划分为两个集合，并用它们学习两个独立的对真实价值<span
class="math inline">\(q(a),\forall a\in A\)</span>的估计<span
class="math inline">\(Q_1(a)\)</span>和<span
class="math inline">\(Q_2(a)\)</span>,并使用两者分别承担选择和估计的任务，即</p>
<p><span class="math inline">\(Q_2(A^*)=Q_2(argmax_a\
Q_1(a))\)</span>或<span class="math inline">\(Q_1(A^*)=Q_1(argmax_a\
Q_2(a))\)</span></p>
<p>将双学习应用于Q学习得到双Q学习的更新公式： <span
class="math display">\[
Q_1(S_t,A_t)\gets Q_1(S_t,A_t)+\alpha [R_{t+1}+\gamma
Q_2(S_{t+1},argmax_a Q_1(S_{t+1},a))-Q_1(S_t,A_t)]
\]</span> 双Q学习的完整算法流程：</p>
<p><img src="/2024/05/04/《RL-Introduction》-chap6/image-20240505114803514.png"  alt="image-20240505114803514" style="zoom: 33%;" /></p>
<h4 id="游戏后位状态和其他特殊例子">6.8
游戏、后位状态和其他特殊例子</h4>
<ul>
<li>后位状态：</li>
<li>后位状态价值函数：</li>
</ul>
]]></content>
      <categories>
        <category>RLintroduction</category>
      </categories>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>第一章、机器学习基础</title>
    <url>/2024/04/05/%E7%AC%AC%E4%B8%80%E7%AB%A0%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h3 id="线性模型">1.1 线性模型</h3>
<h4 id="线性回归linear-regression">1.1.1 线性回归（Linear
Regression）</h4>
<p>线性回归简单理解即”拟合一条曲线“，可通过<span
class="math inline">\(x\)</span>的值预测<span
class="math inline">\(y\)</span>值 <span class="math display">\[
\hat{y}=f(x;\hat{w},\hat{b})=x^T\hat{w}+\hat{b}
\]</span></p>
<ul>
<li><p>训练集：用于优化模型参数</p></li>
<li><p>验证集：用于优化模型超参数，如学习率、正则化系数等；</p></li>
<li><p>测试集：用于评估模型性能</p>
<p><strong>线性回归从零开始实现</strong>，<a
href="https://github.com/kumudlakara/Medium-codes/blob/main/linear_regression/house_price_data.txt">数据集下载</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># variables to store mean and standard deviation for each feature</span></span><br><span class="line">mu = []</span><br><span class="line">std = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># load data from the filename</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">filename</span>):</span><br><span class="line">	df = pd.read_csv(filename, sep=<span class="string">&quot;,&quot;</span>, index_col=<span class="literal">False</span>)</span><br><span class="line">	df.columns = [<span class="string">&quot;house size&quot;</span>, <span class="string">&quot;rooms&quot;</span>, <span class="string">&quot;price&quot;</span>]</span><br><span class="line">	data = np.array(df, dtype=<span class="built_in">float</span>)</span><br><span class="line">	plot_data(data[:, :<span class="number">2</span>], data[:, -<span class="number">1</span>])</span><br><span class="line">	normalize(data)</span><br><span class="line">	<span class="keyword">return</span> data[:, :<span class="number">2</span>], data[:, -<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># draw the data[house size,price]</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_data</span>(<span class="params">x, y</span>):</span><br><span class="line">	plt.xlabel(<span class="string">&#x27;house size&#x27;</span>)</span><br><span class="line">	plt.ylabel(<span class="string">&#x27;price&#x27;</span>)</span><br><span class="line">	plt.plot(x[:, <span class="number">0</span>], y, <span class="string">&#x27;bo&#x27;</span>)</span><br><span class="line">	plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># normalize the data</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize</span>(<span class="params">data</span>):</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, data.shape[<span class="number">1</span>] - <span class="number">1</span>):</span><br><span class="line">		mu.append(np.mean(data[:, i]))</span><br><span class="line">		std.append(np.std(data[:, i]))</span><br><span class="line">		data[:, i] = ((data[:, i] - np.mean(data[:, i])) / np.std(data[:, i]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># matrix multiply</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">h</span>(<span class="params">x, theta</span>):</span><br><span class="line">	<span class="keyword">return</span> np.matmul(x, theta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate the cost_function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cost_function</span>(<span class="params">x, y, theta</span>):</span><br><span class="line">	<span class="keyword">return</span> ((h(x, theta) - y).T @ (h(x, theta) - y)) / (<span class="number">2</span> * y.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate the gradient</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">x, y, theta, learning_rate=<span class="number">0.1</span>, num_epochs=<span class="number">10</span></span>):</span><br><span class="line">	m = x.shape[<span class="number">0</span>]</span><br><span class="line">	J_all = []</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">		h_x = h(x, theta)</span><br><span class="line">		cost_ = (<span class="number">1</span> / m) * (x.T @ (h_x - y))</span><br><span class="line">		theta = theta - (learning_rate) * cost_</span><br><span class="line">		J_all.append(cost_function(x, y, theta))</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> theta, J_all</span><br><span class="line"></span><br><span class="line"><span class="comment"># draw the change of the cost</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_cost</span>(<span class="params">J_all, num_epochs</span>):</span><br><span class="line">	plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">	plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">	plt.plot(num_epochs, J_all, <span class="string">&#x27;m&#x27;</span>, linewidth=<span class="string">&quot;5&quot;</span>)</span><br><span class="line">	plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">theta, x</span>):</span><br><span class="line">	x[<span class="number">0</span>] = (x[<span class="number">0</span>] - mu[<span class="number">0</span>]) / std[<span class="number">0</span>]</span><br><span class="line">	x[<span class="number">1</span>] = (x[<span class="number">1</span>] - mu[<span class="number">1</span>]) / std[<span class="number">1</span>]</span><br><span class="line">	y = theta[<span class="number">0</span>] + theta[<span class="number">1</span>] * x[<span class="number">0</span>] + theta[<span class="number">2</span>] * x[<span class="number">1</span>]</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;Price of house: &quot;</span>, y[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x, y = load_data(<span class="string">&quot;house_price_data.txt&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x和y的形状分别为&quot;</span>,x.shape,y.shape)</span><br><span class="line">y = np.reshape(y, (<span class="number">46</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 加一列全1向量</span></span><br><span class="line">x = np.hstack((np.ones((x.shape[<span class="number">0</span>], <span class="number">1</span>)), x))</span><br><span class="line">theta = np.zeros((x.shape[<span class="number">1</span>], <span class="number">1</span>))</span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">num_epochs = <span class="number">10000</span></span><br><span class="line">theta, J_all = gradient_descent(x, y, theta, learning_rate, num_epochs)</span><br><span class="line">J = cost_function(x, y, theta)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Cost: &quot;</span>, J)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Parameters: &quot;</span>, theta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># for testing and plotting cost</span></span><br><span class="line">n_epochs = []</span><br><span class="line">jplot = []</span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;J_all形状&quot;</span>,np.array(J_all).shape)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> J_all:</span><br><span class="line">	jplot.append(i[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">	n_epochs.append(count)</span><br><span class="line">	count += <span class="number">1</span></span><br><span class="line">jplot = np.array(jplot)</span><br><span class="line">n_epochs = np.array(n_epochs)</span><br><span class="line">plot_cost(jplot, n_epochs)</span><br><span class="line"></span><br><span class="line">test(theta, [<span class="number">1203</span>, <span class="number">3</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="逻辑斯蒂回归logistic-regression">1.1.2 逻辑斯蒂回归(Logistic
Regression)</h4>
<p>逻辑斯蒂回归=线性回归+<span
class="math inline">\(sigmoid\)</span>函数，主要用于处理二分类问题，<span
class="math inline">\(sigmoid\)</span>函数可将任何实数映射到0和1之间，以表示属于两类中某一类别的概率，可设置阈值<span
class="math inline">\(\delta\)</span>进行类别的最终判断。 <span
class="math display">\[
f(x;w,b)=sigmoid(y)=sigmoid(x^Tw+b)=\frac{1}{1+e^{-(x^Tw+b)}}
\]</span></p>
<h4 id="交叉熵损失函数cross-entropy-loss-function">1.1.3
交叉熵损失函数(Cross Entropy Loss Function)</h4>
<p><strong>KL散度</strong>，也称相对熵，用于衡量两个概率分布之间的差异，在离散情况下，使用向量<span
class="math inline">\(p=[p_1,p_2,\cdot\cdot\cdot,p_m]^T\)</span>,<span
class="math inline">\(q=[q_1,q_2,\cdot\cdot\cdot,q_m]^T\)</span></p>
<p>表示两个<span class="math inline">\(m\)</span>维的离散概率分布，则
<span class="math display">\[
KL(p,q)=H(p,q)-H(p)=\sum_{j=1}^{m} p_j\cdot ln\frac{p_j}{q_j}
\]</span> 其中交叉熵<span
class="math inline">\(H(p,q)=-\sum_{j=1}^{m}p_j\cdot lnq_j\)</span>,</p>
<p>信息熵<span class="math inline">\(H(p)=-\sum_{j=1}^{m}p_j\cdot
lnp_j\)</span></p>
<p>当概率分布<span
class="math inline">\(p\)</span>固定时，也就是说我们要让<span
class="math inline">\(q\)</span>尽量接近<span
class="math inline">\(p\)</span>时,最小化交叉熵即可，这也就是为什么交叉熵损失函数有效的原因。</p>
<h4 id="softmax分类器">1.1.4 softmax分类器</h4>
<p>softmax分类=线性函数+softmax激活函数</p>
<p>其中线性函数的结果为向量，再通过softmax将这个向量映射到加和为1的概率分布；
<span class="math display">\[
\pi \in R^k = softmax(z\in R^k)=softmax(W\in R^{k\times d} \cdot x\in
R^{d\times1})+b\in R^{k\times1}
\]</span> 其中<span
class="math inline">\(softmax(z)=\frac{1}{\sum\limits_{l=1}^{k}exp(z_l)}[exp(z_1),exp(z_2),\cdot\cdot\cdot,exp(z_k)]\)</span></p>
<p>通常情况下，需要对矩阵<span
class="math inline">\(W\)</span>和向量<span
class="math inline">\(b\)</span>做规范化，使得<span
class="math inline">\(\sum\limits_{j=1}^{k}w_j=0,\sum\limits_{j=1}^{k}b_j=0\)</span>,<span
class="math inline">\(w_j\)</span>为矩阵<span
class="math inline">\(W\)</span>的第<span
class="math inline">\(j\)</span>行，即矩阵<span
class="math inline">\(W\)</span>的每列和为0，向量<span
class="math inline">\(b\)</span>和为0，这可以通过全员减去平均值再除以标准差来达成。</p>
<h5 id="常见的数据标准化方法总结">常见的数据标准化方法总结：</h5>
<ul>
<li><p><span class="math inline">\(min\_max\)</span>: <span
class="math inline">\(x_{new}=\frac{x-x_{min}}{x_{max}-x_{min}}\)</span>,映射到<span
class="math inline">\([0,1]\)</span>;</p></li>
<li><p><span class="math inline">\(z\_score\)</span>: <span
class="math inline">\(x_{new}=\frac{x-\mu}{\delta}\)</span>,映射到标准正态分布；</p></li>
<li><p>正数归一化： <span
class="math inline">\(x_{new}=\frac{x}{x_1+x_2+...+x_n}\)</span>,映射到<span
class="math inline">\([0,1]\)</span>;</p></li>
<li><p>中心化： <span
class="math inline">\(x_{new}=x-\mu\)</span>,使得均值为0</p></li>
</ul>
<h3 id="神经网络简介">1.2 神经网络简介</h3>
<h4 id="全连接层感知机">1.2.1 全连接层（感知机）</h4>
<p><span class="math display">\[
{x}’=\sigma(z)=\sigma(Wx+b)
\]</span></p>
<p>即线性函数+激活函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights);</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br><span class="line">d2l.predict_ch3(net,test_iter)</span><br></pre></td></tr></table></figure>
<h4 id="卷积神经网络">1.2.2 卷积神经网络</h4>
<p>卷积神经网络的输入通常是矩阵或三阶张量，CNN从中提取特征并输出提取的特征向量。</p>
<h3 id="梯度下降gdgradient-descent">1.3 梯度下降（GD，gradient
descent）</h3>
<ul>
<li>目标函数关于某个参数变量的梯度的形状一定与这个参数变量的形状相同；</li>
<li>梯度的方向是函数上升最快的方向，因此其负方向是下降最快的方向；</li>
<li><strong>1.梯度下降</strong>：每epoch计算所有样本的损失函数的平均再做梯度下降；（用于非凸问题存在鞍点，且计算量为SGD的n倍）</li>
<li><strong>2.随机梯度下降</strong>：每epoch从样本集合中选取一个样本计算损失函数再做梯度下降；</li>
<li><strong>3.小批量随机梯度下降</strong>：每epoch从样本集合中随机抽取batch_size个样本计算损失函数求平均再做梯度下降；</li>
<li>反向传播：任何一个计算过程都可以构建其计算图，从计算图尾部用梯度下降向前传播，更新参数；</li>
</ul>
]]></content>
      <categories>
        <category>深度强化学习读书笔记</category>
      </categories>
      <tags>
        <tag>机器学习、强化学习、人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title>第七章、策略梯度方法</title>
    <url>/2024/04/14/%E7%AC%AC%E4%B8%83%E7%AB%A0%E3%80%81%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<hr />
<h4 id="策略网络">1. 策略网络</h4>
<p>神经网络<span
class="math inline">\(\pi(a|s;\theta)\)</span>称为策略网络，输入状态<span
class="math inline">\(s\)</span>,输出<span
class="math inline">\(\mathcal{A}\)</span>维向量，其中每个元素代表每个动作的概率值，决策时按照这个概率质量分布执行动作。</p>
<h4 id="策略学习的目标函数">2.策略学习的目标函数</h4>
<p>首先还是复习相关的基本概念：</p>
<ul>
<li><p>回报：<span class="math inline">\(U_t=R_t+\gamma\cdot
R_{t+1}+...+\gamma^{n-t}\cdot R_n\)</span>,由于<span
class="math inline">\(r_t=r(s_t,a_t,s_{t+1})\)</span>,故<span
class="math inline">\(U_t\)</span>依赖于<span
class="math inline">\(t\)</span>时刻之后的所有状态和动作：<span
class="math inline">\(S_t,A_t,S_{t+1},A_{t+1},...,\)</span></p></li>
<li><p>动作价值函数：<span
class="math inline">\(t\)</span>时刻的状态<span
class="math inline">\(s_t\)</span>和动作<span
class="math inline">\(a_t\)</span>看成已知观测值，然后将<span
class="math inline">\(U_t\)</span>对之后的状态和动作求期望： <span
class="math display">\[
Q_{\pi}(s_t,a_t)=E[U_t|S_t=s_t,A_t=a_t]
\]</span></p></li>
<li><p>状态价值函数：<span
class="math inline">\(t\)</span>时刻的状态<span
class="math inline">\(s_t\)</span>看成已知观测值，将<span
class="math inline">\(Q_{\pi}(s_t,A_t)\)</span>对使用策略网络<span
class="math inline">\(\pi(\cdot|s_t;\theta)\)</span>决策后的<span
class="math inline">\(A_t\)</span>求期望： <span class="math display">\[
V_{\pi}(s_t)=E_{A_t\sim\pi(\cdot|s_t;\theta)}[Q_{\pi}(s_t,A_t)]
\]</span> 状态价值取决于当前状态<span
class="math inline">\(s_t\)</span>以及策略网络参数<span
class="math inline">\(\theta\)</span>(因为策略网络的结构已经固定)</p></li>
</ul>
<p>为了使得目标函数排除当前状态<span
class="math inline">\(s_t\)</span>的影响，可对其求期望，即我们定义目标函数为：
<span class="math display">\[
J(\theta)=E_S(V_\pi(S))
\]</span> 优化问题为： <span class="math display">\[
\underset{\theta}{max}\ J(\theta)
\]</span> 可通过梯度上升来最大化<span
class="math inline">\(J(\theta)\)</span>,即 <span
class="math display">\[
\theta_{new}=\theta_{now}+\beta\cdot \nabla_{\theta}J(\theta_{now})
\]</span>
其中的梯度部分，可以通过以下展示的<strong>策略梯度定理</strong>来计算：</p>
<ul>
<li>策略梯度定理：设目标函数为<span
class="math inline">\(J(\theta)=E_{S\sim
d(\cdot)}[V_{\pi}(S)]\)</span>,设<span
class="math inline">\(d(s)\)</span>为马尔可夫链稳态发布的概率质量（密度）函数，那么
<span class="math display">\[
\frac{\partial J(\theta)}{\partial \theta}
=(1+\gamma+\gamma^2+...+\gamma^{n-1})\cdot E_{S\sim d(\cdot)}[E_{A\sim
\pi(\cdot|S;\theta)}[\frac{\partial\
ln\pi(A|S;\theta)}{\partial\theta}\cdot Q_{\pi}(S,A)]]
\]</span>
前面的系数无关紧要，可以省略，因为做梯度下降时前面都要乘以学习率<span
class="math inline">\(\beta\)</span>.</li>
</ul>
<h4 id="reinforce">3.REINFORCE</h4>
<p>首先使用蒙特卡洛方法对策略梯度进行近似， <span
class="math display">\[
\nabla_{\theta}J(\theta)=E_S[E_{A\sim\pi(\cdot|S;\theta)}[Q_{\pi(S,A)}\cdot\nabla_{\theta}ln\pi(A|S;\theta)]]
\]</span> 每当要做梯度下降时，从环境中观测出一个状态<span
class="math inline">\(s\)</span>,根据当前最新的策略网络，随机抽样得出一个动作<span
class="math inline">\(a\sim\pi(\cdot|s;\theta)\)</span></p>
<p>计算梯度： <span class="math display">\[
g(s,a;\theta)=Q_{\pi}(s,a)\cdot \nabla_{\theta}ln\pi(a|s;\theta)
\]</span>
它是第一个式子的无偏估计，但在实际应用中依然不知道式中第一项<span
class="math inline">\(Q_\pi(s,a)\)</span>的值，要么用实际观测到的回报<span
class="math inline">\(u_t\)</span>代替（即REINFORCE)；要么像SARSA一样使用神经网络<span
class="math inline">\(q(s,a;w)\)</span>近似（即actor—critic）；而对于第二项，可以通过策略网络反向传播计算；</p>
<p><strong>REINFORCE训练流程</strong>：</p>
<p>（1）REINFORCE属于同策略，用策略网络<span
class="math inline">\(\pi(\cdot|S;\theta_{now})\)</span>控制智能体从头开始一个回合，收集得到训练数据：
<span class="math display">\[
s_1,a_1,r_1,s_2,a_2,r_2,...,s_n,a_n,r_n
\]</span> (2) 计算所有回报： <span class="math display">\[
u_t=\sum\limits_{k=t}^{n}\gamma^{k-t}\cdot r_k,\forall t=1,2...,n
\]</span> (3)用<span
class="math inline">\(\{(s_t,a_t)\}_{t=1}^{n}\)</span>作为数据，做反向传播计算：
<span class="math display">\[
\nabla_\theta ln\pi(a_t|s_t;\theta_{now}),\forall t=1,2...n
\]</span> (4)做随机梯度上升更新策略网络参数： <span
class="math display">\[
\theta_{new}=\theta_{now}+\beta\cdot\sum\limits_{t=1}^{n}\gamma^{t-1}\cdot
u_t\cdot\nabla_\theta ln\pi(a_t|s_t;\theta_{now})
\]</span></p>
<h4 id="actor-critic">4.actor-critic</h4>
<p><span class="math display">\[
g(s,a;\theta)=Q_{\pi}(s,a)\cdot \nabla_{\theta}ln\pi(a|s;\theta)
\]</span></p>
<p>actor-critic方法使用神经网络近似<span
class="math inline">\(Q_\pi(s,a)\)</span>,这个神经网络称为“价值网络”，记作<span
class="math inline">\(q(s,a;w)\)</span>,价值网络的输入是状态，输出是<span
class="math inline">\(\mathcal{A}\)</span>维向量，它的每个元素代表每个动作的价值。actor-critic方法中，策略网络<span
class="math inline">\(\pi(a|s;\theta)\)</span>基于状态<span
class="math inline">\(s\)</span>做出动作<span
class="math inline">\(a\)</span>,相当于演员；价值网络<span
class="math inline">\(q(s,a;w)\)</span>评价在状态<span
class="math inline">\(s\)</span>下做出动作<span
class="math inline">\(a\)</span>的评分。如图所示：</p>
<p><img src="/2024/04/14/第七章、策略梯度方法/image-20240414202442043.png"  alt="image-20240414202442043" style="zoom:50%;" /></p>
<p><strong>actor-critic的训练流程</strong>:</p>
<p>训练价值网络的目的是让其输出更接近于动作价值函数，使用损失函数的梯度下降；而训练策略网络的目的是最大化<span
class="math inline">\(J(\theta)\)</span>,使用目标函数的梯度上升；</p>
<p>(1)观测到当前状态<span
class="math inline">\(s_t\)</span>,根据策略网络做决策：<span
class="math inline">\(a_t\sim\pi(\cdot|s_t;\theta_{now})\)</span>,<strong>并让智能体执行动作<span
class="math inline">\(a_t\)</span></strong>;</p>
<p>(2)从环境中观测到新的状态<span
class="math inline">\(s_{t+1}\)</span>和奖励<span
class="math inline">\(r_t\)</span>;</p>
<p>(3)根据新的状态再次做决策：<span
class="math inline">\(\widetilde{a}_{t+1}\sim\pi(\cdot|s_{t+1};\theta_{now})\)</span>,<strong>但不让智能体执行动作</strong><span
class="math inline">\(\widetilde{a}_{t+1}\)</span>,因为算出<span
class="math inline">\(\widetilde{a}_{t+1}\)</span>只是为了计算出训练价值网络时要用到的TD目标；</p>
<p>(4)让价值网络打分： <span class="math display">\[
\hat{q}_t=q(s_t,a_t;w_{now}),\hat{q}_{t+1}=q(s_{t+1},\widetilde{a}_{t+1};w_{now})
\]</span> (5)计算TD目标和TD误差： <span class="math display">\[
\hat{y}_t=r_t+\gamma\cdot \hat{q}_{t+1},\delta_t=\hat{q}_t-\hat{y}_t
\]</span> (6)更新价值网络： <span class="math display">\[
w_{new}=w_{now}-\alpha\cdot\delta_t\cdot\nabla_wq(s_t,a_t;w_{now})
\]</span> (7)更新策略网络： <span class="math display">\[
\theta_{new}=\theta_{now}+\beta\cdot\hat{q}_t\cdot\nabla_\theta
ln\pi(a_t|s_t;\theta_{now})
\]</span> 注：actor-critic的价值网络存在自举问题，对于<span
class="math inline">\(\hat{q}_{t+1}\)</span>，可通过目标网络近似；</p>
]]></content>
      <categories>
        <category>深度强化学习读书笔记</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>第三章、强化学习基本概念</title>
    <url>/2024/04/01/%E7%AC%AC%E4%B8%89%E7%AB%A0%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<h3 id="马尔可夫决策过程">1. 马尔可夫决策过程</h3>
<p>马尔可夫决策过程：<span class="math inline">\(MDP,Markov Decision
Process\)</span> <strong>智能体</strong>：做动作或决策的主体；
<strong>环境</strong>：与智能体交互的对象；</p>
<h3 id="状态动作奖励">2. 状态、动作、奖励</h3>
<p><strong>状态</strong>：对当前时刻环境的概括,记作<span
class="math inline">\(s_t\)</span>，是做决策的依据；如：棋盘上的格局
<strong>状态空间</strong>：所有可能存在的状态的集合，记作<span
class="math inline">\(\mathcal{S}\)</span>;状态空间可离散、可连续；可有限、可无限
<strong>动作</strong>：智能体基于当前状态所做出的决策，动作的选取可以是确定性的、也可以是随机性的（多数情况下为随机性的），即给定一个概率分布（一个加和为1的概率向量），智能体按照这个概率分布选取一个动作
<strong>动作空间</strong>：所有可能动作的集合，记作<span
class="math inline">\(\mathcal{A}\)</span>;同样，离散、连续、有限、无限皆可
<strong>奖励</strong>：智能体在执行一个动作后，环境返回给智能体的一个数值；奖励函数一般由自己设计及定义，记作<span
class="math inline">\(r(s_t,a_t,s_{t+1})\)</span>或<span
class="math inline">\(r(s_t,a_t)\)</span>;我们总是假设奖励函数是有界的，即对于所有<span
class="math inline">\(a_t\in\mathcal{A}\)</span>, <span
class="math inline">\(s_t,s_{t+1}\)</span>,有<span
class="math inline">\(|r(s_t,a_t,s_{t+1})|&lt;\infty\)</span>，否则得到一个正负无穷大的奖励后就没必要继续了。</p>
<h3 id="状态转移">3.状态转移</h3>
<p><strong>状态转移</strong>：智能体从当前<span
class="math inline">\(t\)</span>时刻的状态<span
class="math inline">\(s\)</span>转移到下一刻的状态<span
class="math inline">\(s&#39;\)</span>的过程；我们用<strong>状态转移函数</strong>来描述状态转移，记作：
<span
class="math display">\[p_t(s&#39;|s,a)=P(S_{t+1}=s&#39;|S_t=s,A_t=a)\]</span>
表示发生下述事件的概率：在当前状态<span
class="math inline">\(s\)</span>,智能体执行动作<span
class="math inline">\(a\)</span>,下一刻环境的状态变成<span
class="math inline">\(s&#39;\)</span>，这个值必不恒等于1，因为状态转移存在随机性。
<strong>确定性状态转移</strong>：环境中不存在随机性，下一个状态<span
class="math inline">\(s&#39;\)</span>完全由<span
class="math inline">\(s,a\)</span>决定： <span
class="math display">\[p_t(s&#39;|s,a)=\begin{cases}
  &amp; \text{ 1 , if } \tau_t(s,a) = s&#39; \\
  &amp; \text{ 0 , otherwise }
\end{cases}\]</span>
<strong>随机性状态转移</strong>：环境中存在随机性，比如，在玛丽欧游戏中你可以控制玛丽欧怎么移动，但敌人怎么移动则无法确定，这就是下一刻状态不确定的缘由。</p>
<h3 id="策略">4.策略</h3>
<p><strong>策略</strong>：如何根据观测到的状态做出决策，即如何从动作空间中选取一个动作。
<strong>随机性策略</strong>：<span
class="math inline">\(\pi(a|s)=P(A=a|S=s)\)</span>,即给定当前状态条件下采取各个动作的概率，也就是加和为1的向量。
<strong>确定性策略</strong>：记作<span
class="math inline">\(\mu:\mathcal{S}\to\mathcal{A}\)</span>,即动作<span
class="math inline">\(a\)</span>完全由状态<span
class="math inline">\(s\)</span>决定:<span
class="math inline">\(a=\mu(s)\)</span></p>
<p><strong>智能体与环境交互的流程</strong>：观测到当前状态<span
class="math inline">\(s\)</span>，用策略<span class="math inline">\(\pi
(a|s)\)</span>算出所有动作的概率并随机抽样，得到其中一个动作<span
class="math inline">\(a\)</span></p>
<p>,环境通过状态转移函数<span
class="math inline">\(p_t(s&#39;|s,a)\)</span>(这也是一个概率分布)随机生成新的状态<span
class="math inline">\(s’\)</span>，并向智能体返回一个奖励<span
class="math inline">\(r(s,a,s’)\)</span>。</p>
<h3 id="马尔可夫性质markov-property">5.马尔可夫性质（Markov
property）</h3>
<p>马尔可夫性，即下一时刻状态<span
class="math inline">\(S_{t+1}\)</span>仅仅依赖于当前状态<span
class="math inline">\(S_t\)</span>和动作<span
class="math inline">\(A_t\)</span>,而不依赖于过去的状态和动作： <span
class="math display">\[P(S_{t+1}|S_t,A_t)=P(S_{t+1}|S_1,A_1,S_2,A_2,...,S_t,A_t)\]</span>
<strong>轨迹</strong>：在一个回合（从开始到结束）中智能体观测到的所有状态、动作、奖励：<span
class="math inline">\(s_1,a_1,r_1,s_2,a_2,r_2,s_3,a_3,r_3...\)</span></p>
<h3 id="回报与折扣回报">6.回报与折扣回报</h3>
<p><strong>回报</strong>：从当前时刻开始到本回合结束所有奖励的总和，也叫作累积奖励。假设本回合在时刻<span
class="math inline">\(n\)</span>结束，则<span
class="math inline">\(t\)</span>时刻的回报定义为： <span
class="math display">\[U_t=R_{t\to end}=R_t+R_{t+1}+...+R_n\]</span>
<strong>折扣回报</strong>：越久远的未来的回报越不重要，所以应该随时间乘上相应的折扣率<span
class="math inline">\(\gamma\in[0,1]\)</span>，折扣回报： <span
class="math display">\[U_t=R_t+\gamma \cdot R_{t+1}+\gamma^2 \cdot
R_{t+2}+...\]</span> 可以将其理解为得到了一个新的奖励函数，<span
class="math inline">\(R_{t+i}=\gamma^i\cdot R_{t+i}\)</span></p>
<h3 id="价值函数重中之重">7.价值函数（重中之重！！！）</h3>
<p>价值函数是回报的期望，价值函数值越大，说明现状越有利；
<strong>动作价值函数</strong>： <span
class="math display">\[Q_{\pi}(s_t,a_t)=E_{S_{t+1},A_{t+1},...,S_n,A_n}[U_t|S_t=s_t,A_t=a_t]\]</span>
表示已经观测到了<span
class="math inline">\(S_t,A_t\)</span>的值，即观测到状态<span
class="math inline">\(s_t\)</span>,选中动作<span
class="math inline">\(a_t\)</span>，原来<span
class="math inline">\(U_t\)</span>中的随机性来自<span
class="math inline">\(t+1\)</span>时刻起所有的状态和动作：<span
class="math inline">\(S_{t+1},A_{t+1},...,S_n,A_n\)</span>,而动作价值函数对它们求期望，简单理解就是找出它们的所有情况，算出<span
class="math inline">\(U_t\)</span>求平均，这样就消除它们的影响。 <span
class="math inline">\(\qquad\)</span><span
class="math inline">\(t\)</span>时刻的动作价值函数<span
class="math inline">\(Q_\pi(s_t,a_t)\)</span>依赖于以下三个因素：
（1）当前状态<span
class="math inline">\(s_t\)</span>：当前状态越好，<span
class="math inline">\(Q_\pi(s_t,a_t)\)</span>越大 （2）当前动作<span
class="math inline">\(a_t\)</span>：智能体执行的动作越好，<span
class="math inline">\(Q_\pi(s_t,a_t)\)</span>越大 （3）策略函数<span
class="math inline">\(\pi\)</span>：<span
class="math inline">\(S_{t+1},A_{t+1},...,S_n,A_n\)</span>由策略决定，所以对它们求期望最终的结果受到策略的影响。
<strong>最优动作价值函数</strong>： 为了排除策略<span
class="math inline">\(\pi\)</span>的影响，可以使： <span
class="math display">\[\pi^*=\mathop{argmax}\limits_{\pi}
Q_\pi(s_t,a_t),\forall s_t\in\mathcal{S},a_t\in\mathcal{A}\]</span>
即选取一个当前为任何状态、执行任何动作的情况下都最优的策略，这样策略就确定了，也就排除了策略<span
class="math inline">\(\pi\)</span>的影响，<span
class="math inline">\(Q_*(s_t,a_t)\)</span>就是最优动作价值函数。
<strong>状态价值函数</strong>: <span
class="math display">\[V_\pi(s_t)=E_{A_t\sim\pi(\cdot|s_t)}[Q_\pi(s_t,A_t)]=\sum_{a\in\mathcal{A}}^{}\pi(a|s_t)
\cdot Q_\pi(s_t,a)\]</span>
状态价值函数可以理解为在动作价值函数的基础上，动作<span
class="math inline">\(A_t\)</span>不再确定，而是随机变量，对动作<span
class="math inline">\(A_t\)</span>求期望以消除动作的影响，使得状态价值函数只依赖于策略函数<span
class="math inline">\(\pi\)</span>和状态<span
class="math inline">\(s_t\)</span>的好坏</p>
<p><strong>两者比较</strong>： <span class="math display">\[
Q_{\pi}(s_t,a_t)=E_{S_{t+1},A_{t+1},...,S_n,A_n}[U_t|S_t=s_t,A_t=a_t]
\]</span></p>
<p><span class="math display">\[
V_{\pi}(s_t)=E_{A_t,S_{t+1},A_{t+1},...,S_n,A_n}[U_t|S_t=s_t]
\]</span></p>
<p>强化学习分为（1）基于模型的方法 （2）无模型方法</p>
<p>无模型方法：价值学习、策略学习</p>
<p>基于模型的方法：AlphaGo</p>
]]></content>
      <categories>
        <category>深度强化学习读书笔记</category>
      </categories>
      <tags>
        <tag>人工智能、强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>第二章、蒙特卡洛方法</title>
    <url>/2024/03/31/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E3%80%81%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<ol type="1">
<li><strong>随机变量</strong>记作<span
class="math inline">\(X\)</span>,<strong>观测值</strong>记作<span
class="math inline">\(x\)</span>,观测值只是数字而已，没有随机性,如<span
class="math inline">\(P(X=0)=\frac{1}{2}\)</span>中的<span
class="math inline">\(X\)</span>为大写；</li>
<li>给定随机变量<span
class="math inline">\(X\)</span>,它的<strong>累积分布函数</strong>（即<strong>概率分布函数</strong>）（CDF）是函数<span
class="math inline">\(F_X:R\to[0,1]\)</span>,定义为： <span
class="math display">\[F_X(x)=P(X\le x)\]</span></li>
<li>对于<strong>离散概率分布</strong>，有<strong>概率质量函数</strong><span
class="math inline">\(p(x)\)</span>,假设随机变量<span
class="math inline">\(X\)</span>取值范围是集合<span
class="math inline">\(\chi\)</span> 则有： <span
class="math display">\[\sum_{x\in \chi}^{} p(x)=1\]</span> ,<span
class="math inline">\(X\)</span>的概率质量函数为<span
class="math inline">\(p(\cdot)\)</span>,则函数<span
class="math inline">\(h(X)\)</span>关于变量<span
class="math inline">\(X\)</span>的期望是 <span
class="math display">\[E_{X\sim p(\cdot)}[h(X)]=\sum_{x\in
\chi}^{}p(x)\cdot h(x)\]</span></li>
<li>对于<strong>连续概率发布</strong>，有<strong>概率密度函数</strong><span
class="math inline">\(p(x)\)</span>,随机变量<span
class="math inline">\(X\)</span>的取值范围<span
class="math inline">\(\chi\)</span>是连续集合，则有：<span
class="math display">\[\int_{-\infty }^{x} p(u)du=F_X(x)=P(X\le
x)\]</span><span class="math display">\[\int_{-\infty }^{+\infty}
p(u)du=1\]</span>,<span
class="math inline">\(X\)</span>的概率密度函数为<span
class="math inline">\(p(\cdot)\)</span>,则函数<span
class="math inline">\(h(X)\)</span>关于变量<span
class="math inline">\(X\)</span>的期望是 <span
class="math display">\[E_{X\sim p(\cdot)}[h(X)]=\int_{\chi}p(x)\cdot
h(x)dx\]</span>
<img src="/2024/03/31/第二章、蒙特卡洛方法/image-20240401134538137.png"  alt="image-20240401134538137" style="zoom: 33%;" /></li>
</ol>
<p>总的来说，就是<span
class="math inline">\(\frac{抽中次数}{总抽样数}=精确的理论概率\)</span></p>
<h3 id="例一近似pi值">例一、近似<span
class="math inline">\(\pi\)</span>值</h3>
<p><img src="/2024/03/31/第二章、蒙特卡洛方法/image-20240401134618614.png"  alt="image-20240401134618614" style="zoom:33%;" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">m = <span class="number">0</span></span><br><span class="line">n = <span class="number">100000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    x,y = <span class="number">2</span>*torch.rand(<span class="number">1</span>)-<span class="number">1</span>,<span class="number">2</span>*torch.rand(<span class="number">1</span>)-<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span>  torch.<span class="built_in">pow</span>(x,<span class="number">2</span>)+ torch.<span class="built_in">pow</span>(y,<span class="number">2</span>) &lt;= <span class="number">1</span>:</span><br><span class="line">        m = m+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">pi = <span class="number">4</span> * m / n</span><br><span class="line"><span class="built_in">print</span>(pi)</span><br></pre></td></tr></table></figure>
<p>输出：3.13528 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.2节，蒙特卡洛近似计算圆周率。</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">approxiate_pi</span>(<span class="params">n: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="comment"># 在[-1, 1] x [-1, 1]的空间中随机取n个点。</span></span><br><span class="line">    x_lst = np.random.uniform(-<span class="number">1</span>, <span class="number">1</span>, size=n)</span><br><span class="line">    y_lst = np.random.uniform(-<span class="number">1</span>, <span class="number">1</span>, size=n)</span><br><span class="line">    <span class="comment"># 统计距离圆心距离在1以内的点。</span></span><br><span class="line">    m = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_lst, y_lst):</span><br><span class="line">        <span class="keyword">if</span> x ** <span class="number">2</span> + y ** <span class="number">2</span> &lt;= <span class="number">1</span>:</span><br><span class="line">            m += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 近似计算圆周率。</span></span><br><span class="line">    pi = <span class="number">4</span> * m / n</span><br><span class="line">    <span class="keyword">return</span> pi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    pi = approxiate_pi(<span class="number">100</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;100个点近似的圆周率：&quot;</span>, pi)</span><br><span class="line"></span><br><span class="line">    pi = approxiate_pi(<span class="number">10000</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;10000个点近似的圆周率：&quot;</span>, pi)</span><br><span class="line"></span><br><span class="line">    pi = approxiate_pi(<span class="number">1000000</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;1000000个点近似的圆周率：&quot;</span>, pi)</span><br><span class="line"></span><br></pre></td></tr></table></figure> 输出：100个点近似的圆周率： 3.08
10000个点近似的圆周率： 3.1352 1000000个点近似的圆周率： 3.141</p>
<h3 id="例二计算阴影部分面积">例二、计算阴影部分面积</h3>
<p><img src="/2024/03/31/第二章、蒙特卡洛方法/image-20240401134642296.png"  alt="image-20240401134642296" style="zoom:33%;" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">m = <span class="number">0</span></span><br><span class="line">n = <span class="number">100000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    x,y = <span class="number">2</span>*torch.rand(<span class="number">1</span>),<span class="number">2</span>*torch.rand(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span>  ((x-<span class="number">1</span>)**<span class="number">2</span>+(y-<span class="number">1</span>)**<span class="number">2</span>&lt;=<span class="number">1</span>) &amp; (x**<span class="number">2</span>+y**<span class="number">2</span>&gt;<span class="number">4</span>):</span><br><span class="line">        m = m+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">s = <span class="number">4</span> * m / n</span><br><span class="line"><span class="built_in">print</span>(s)</span><br></pre></td></tr></table></figure>
<p>输出：0.59632</p>
<h3 id="例三计算近似定积分期望">例三、计算近似定积分、期望</h3>
<p><strong>一元函数的定积分</strong>：抽样函数的平均值乘以区间长度，即
<span class="math display">\[
I=\int_{a}^{b}f(x)dx \approx
q_n=(b-a)\cdot\frac{1}{n}\sum\limits_{i=1}^{n}f(x_i)
\]</span>
<strong>多元函数的定积分</strong>：抽样函数的平均值乘以积分集合的体积，即
<span class="math display">\[
I=\int_{\Omega }^{}f(x)dx \approx
q_n=V\cdot\frac{1}{n}\sum\limits_{i=1}^{n}f(x_i)=\int_{\Omega}^{}dx
\cdot\frac{1}{n}\sum\limits_{i=1}^{n}f(x_i)
\]</span> 求<strong>期望</strong>：计算 <span class="math display">\[
E_{X\sim p(\cdot)}[f(X)]=\int_{\Omega}^{}p(x)\cdot f(x)dx
\]</span>
，可按照变量服从的概率分布抽样，求函数平均值即可；当然也可利用定积分，把其中的<span
class="math inline">\(f(x_i)\)</span>换成<span
class="math inline">\(p(x_i)\cdot f(x_i)\)</span>即可；</p>
<p>假设用期望计算<span
class="math inline">\(\int_{0}^{3}x^\frac{2}{3}dx\)</span> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">q=<span class="number">0</span></span><br><span class="line">t=<span class="number">1</span></span><br><span class="line">n=<span class="number">10000</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x ** (<span class="number">2</span>/<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    a = torch.rand(<span class="number">1</span>) * <span class="number">3</span></span><br><span class="line">    q = (<span class="number">1</span>-<span class="number">1</span>/t) * q + (<span class="number">1</span>/t) * f(a)</span><br><span class="line">    t = t+<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;期望&quot;</span>,q)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;积分&quot;</span>,<span class="number">3</span>*q)</span><br></pre></td></tr></table></figure>
输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">期望 tensor([1.2539])</span><br><span class="line">积分 tensor([3.7617])</span><br><span class="line">3.744150881493428</span><br></pre></td></tr></table></figure>
<h3 id="第二章习题2.2">第二章习题2.2</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 方法1</span></span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">n = <span class="number">10000</span></span><br><span class="line">f = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    x = torch.normal(mean=<span class="number">1</span>,std=<span class="number">2</span>,size=(<span class="number">1</span>,))</span><br><span class="line">    f = <span class="number">2</span>*x+<span class="number">10</span>*torch.sqrt(torch.<span class="built_in">abs</span>(x))+<span class="number">3</span>+f</span><br><span class="line"><span class="built_in">print</span>(f/n)</span><br></pre></td></tr></table></figure>
<p>输出：tensor([17.3412])</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">```python</span><br><span class="line"> <span class="comment"># 方法2</span></span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">n = <span class="number">10000</span></span><br><span class="line">q=<span class="number">0</span></span><br><span class="line">t=<span class="number">1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span>  <span class="number">2</span>*x+<span class="number">10</span>*torch.sqrt(torch.<span class="built_in">abs</span>(x))+<span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    x = torch.normal(mean=<span class="number">1</span>,std=<span class="number">2</span>,size=(<span class="number">1</span>,))</span><br><span class="line">    q = (<span class="number">1</span>-<span class="number">1</span>/t)*q+<span class="number">1</span>/t * f(x)</span><br><span class="line">    t = t+<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(q)</span><br></pre></td></tr></table></figure>
<p>输出：tensor([17.3020])</p>
]]></content>
      <categories>
        <category>深度强化学习读书笔记</category>
      </categories>
      <tags>
        <tag>人工智能、强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>第五章、SARSA算法</title>
    <url>/2024/04/12/%E7%AC%AC%E4%BA%94%E7%AB%A0%E3%80%81SARSA%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<hr />
<p>Q-learning和SARSA都是时间差分算法（TD），但前者目标是学习最优动作价值函数<span
class="math inline">\(Q_*\)</span>，为异策略，可以使用经验回放；而后者目标是学习动作价值函数<span
class="math inline">\(Q_\pi(s,a)\)</span>,为同策略，不能使用经验回放。下面分别介绍表格形式和神经网络形式的SARSA算法；</p>
<h4 id="表格形式的sarsa算法">1.表格形式的SARSA算法</h4>
<p>贝尔曼方程 <span class="math display">\[
Q_\pi(s_t,a_t)=E_{S_{t+1},A_{t+1}}[R_t+\gamma\cdot
Q_\pi(S_{t+1},A_{t+1})|S_t=s_t,A_t=a_t]
\]</span> 训练所用的<strong>五元组</strong>数据<span
class="math inline">\((s_t,a_t,r_t,s_{t+1},\widetilde{a}_{t+1})\)</span>的来源：</p>
<p>观测到当前状态为<span
class="math inline">\(s_t\)</span>,根据当前策略做抽样得到<span
class="math inline">\(a_t\sim\pi_{now}(\cdot|s_t)\)</span>,再由状态转移函数得到<span
class="math inline">\(s_{t+1}\)</span>,如此可计算出<span
class="math inline">\(r_t=r(s_t,a_t,s_{t+1})\)</span></p>
<p>再根据当前策略做抽样得到<span
class="math inline">\(\widetilde{a}_{t+1}\sim\pi_{now}(\cdot|s_{t+1})\)</span>,（注意，这个动作只是假想的动作，智能体不予执行）这样就得到<span
class="math inline">\((s_t,a_t,r_t,s_{t+1},\widetilde{a}_{t+1})\)</span></p>
<p>对贝尔曼方程做蒙特卡洛近似得到： <span class="math display">\[
q(s_t,a_t)=r_t+\gamma\cdot q(s_{t+1},\widetilde{a}_{t+1})
\]</span>
等式左边为原预测，右边和为TD目标，两项分别为部分观测和补充预测，TD目标<span
class="math inline">\(\hat{y_t}=r_t+\gamma\cdot
q(s_{t+1},\widetilde{a}_{t+1})\)</span>；</p>
<p>更新公式： <span class="math display">\[
q(s_t,a_t)=(1-\alpha)\cdot q(s_t,a_t)+\alpha\cdot \hat{y_t}
\]</span></p>
<h4 id="神经网络形式的sarsa算法">2.神经网络形式的SARSA算法</h4>
<p>用神经网络<span
class="math inline">\(q(s,a;w)\)</span>(价值网络)来近似<span
class="math inline">\(Q_\pi(s,a)\)</span>,即<span
class="math inline">\(q(s,a;w)\to Q_\pi(s,a),\forall s\in\mathcal
S,a\in\mathcal A\)</span></p>
<p><img src="/2024/04/12/第五章、SARSA算法/image-20240412114045675.png"  alt="image-20240412114045675" style="zoom:50%;" />
<span class="math display">\[
\hat{q}_t=q(s_t,a_t;w_{now})\to\hat{y}_t=r_t+\gamma\cdot
\hat{q}_{t+1}=r_t+\gamma\cdot q(s_{t+1},\widetilde{a}_{t+1};w_{now})
\]</span></p>
<p><span class="math display">\[
损失函数：L(w)=\frac{1}{2}[q(s_t,a_t;w_{now})-\hat{y}_t]^2
,\delta_t=\hat{q}_t-\hat{y}_t
\]</span></p>
<p>更新价值网络参数 <span class="math display">\[
w_{new}=w_{now}-\alpha\cdot \delta_t\cdot\nabla _wq(s_t,a_t;w_{now})
\]</span></p>
<h4 id="多步td目标">3.多步TD目标</h4>
<p><span class="math display">\[
\hat{q}_t=q(s_t,a_t;w_{now})\to\hat{y}_t=\sum\limits_{i=0}^{m-1}\gamma^ir_{t+i}+\gamma^m\cdot
\hat{q}_{t+m}=\sum\limits_{i=0}^{m-1}\gamma^ir_{t+i}+\gamma^m\cdot
q(s_{t+m},a_{t+m};w_{now})
\]</span></p>
<ul>
<li>回报<span class="math inline">\(u_t\)</span>的书写形式：<span
class="math inline">\(u_t=\sum_{i=0}^{n-t}\gamma^ir_{t+i}=\sum_{i=t}^{n}\gamma^{i-t}r_i\)</span></li>
</ul>
<h4 id="对比">4.对比</h4>
<p>蒙特卡洛方法：用实际观测值去近似期望，特点是它是期望的无偏估计，但实际观测有可能距离期望较远，即方差大，收敛较慢；</p>
<p>自举：用估算去更新改进估算本身，自举方法中单步TD的特点是方差小收敛快，但它是期望的有偏估计，自举会让偏差从<span
class="math inline">\((s_{t+1},a_{t+1})\)</span>传播到<span
class="math inline">\((s_t,a_t)\)</span>;多步TD介于两者之间；</p>
]]></content>
      <categories>
        <category>深度强化学习读书笔记</category>
      </categories>
      <tags>
        <tag>强化学习、人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title>第八章、带基线的策略梯度方法</title>
    <url>/2024/04/15/%E7%AC%AC%E5%85%AB%E7%AB%A0%E3%80%81%E5%B8%A6%E5%9F%BA%E7%BA%BF%E7%9A%84%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<hr />
<h4 id="基线的引入">1.基线的引入</h4>
<p>策略梯度定理： <span class="math display">\[
\nabla_{\theta}J(\theta)=E_S[E_{A\sim\pi(\cdot|S;\theta)}[Q_{\pi}(S,A)\cdot\nabla_{\theta}ln\pi(A|S;\theta)]]
\]</span> 设<span class="math inline">\(b\)</span>为不依赖于动作<span
class="math inline">\(A\)</span>的任意函数，则有： <span
class="math display">\[
\nabla_{\theta}J(\theta)=E_S[E_{A\sim\pi(\cdot|S;\theta)}[(Q_{\pi}(S,A)-b)\cdot\nabla_{\theta}ln\pi(A|S;\theta)]]
\]</span> 其原因在于： <span class="math display">\[
\nabla_{\theta}J(\theta)=E_S[E_{A\sim\pi(\cdot|S;\theta)}[b\cdot\nabla_{\theta}ln\pi(A|S;\theta)]]=0
\]</span> 本质上是：对于任意的<span
class="math inline">\(s\)</span>,有<span
class="math inline">\(E_{A\sim\pi(\cdot|s;\theta)}[b\cdot\nabla_{\theta}ln\pi(A|s;\theta)]=0\)</span></p>
<p>下面证明：</p>
<p><strong>证明</strong>： <span class="math display">\[
E_{A\sim\pi(\cdot|s;\theta)}[b\cdot\nabla_{\theta}ln\pi(A|s;\theta)]\\
=b\cdot E_{A\sim\pi(\cdot|s;\theta)}[\frac{\partial\
ln\pi(A|s;\theta)}{\partial\theta}]\\
=b\cdot\sum\limits_{a\in\mathcal{A}}^{}\pi(a|s;\theta)\cdot\frac{\partial\
ln\pi(A|s;\theta)}{\partial\theta}\\
=b\cdot\sum\limits_{a\in\mathcal{A}}^{}\pi(a|s;\theta)\cdot\frac{1}{\pi(a|s;\theta)}\cdot\frac{\partial\pi(a|s;\theta)}{\partial\theta}\\
=b\cdot\sum\limits_{a\in\mathcal{A}}^{}\frac{\partial\pi(a|s;\theta)}{\partial\theta}\\
=b\cdot\frac{\partial}{\partial\theta}\sum\limits_{a\in\mathcal{A}}^{}\pi(a|s;\theta)\\
=b\cdot\frac{\partial1}{\partial\theta}\\
=0
\]</span></p>
<h4 id="带基线的reinforce算法">2.带基线的REINFORCE算法</h4>
<p>带基线的策略梯度定理 <span class="math display">\[
\nabla_{\theta}J(\theta)=E_S[E_{A\sim\pi(\cdot|S;\theta)}[(Q_{\pi}(S,A)-b)\cdot\nabla_{\theta}ln\pi(A|S;\theta)]]\\
\]</span> 使用状态价值函数<span
class="math inline">\(V_\pi(s)\)</span>作为基线，并且对其做蒙特卡罗方法近似：
<span class="math display">\[
g(s,a;\theta)=(Q_{\pi}(s,a)-V_\pi(s))\cdot\nabla_{\theta}ln\pi(a|s;\theta)
\]</span> 带基线的REINFORCE进一步用实际观测的回报<span
class="math inline">\(u\)</span>代替<span
class="math inline">\(Q_\pi(s,a)\)</span>,并使用神经网络<span
class="math inline">\(v(s;w)\)</span>近似<span
class="math inline">\(V_\pi(s)\)</span>,得到： <span
class="math display">\[
\widetilde{g}(s,a;\theta)=(u-v(s;w))\cdot\nabla_{\theta}ln\pi(a|s;\theta)
\]</span> <strong>REINFORCE with baseline训练流程</strong>：</p>
<p>（1）REINFORCE属于同策略，用策略网络<span
class="math inline">\(\pi(\cdot|S;\theta_{now})\)</span>控制智能体从头开始一个回合，收集得到训练数据：
<span class="math display">\[
s_1,a_1,r_1,s_2,a_2,r_2,...,s_n,a_n,r_n
\]</span> (2) 计算所有回报： <span class="math display">\[
u_t=\sum\limits_{k=t}^{n}\gamma^{k-t}\cdot r_k,\forall t=1,2...,n
\]</span> (3)让价值网络<span
class="math inline">\(v(s;w)\)</span>做出预测 <span
class="math display">\[
\hat{v}_t=v(s_t;w_{now}),\forall t=1,2...,n
\]</span> (4)计算价值网络的预测和观测到的回报之间的误差： <span
class="math display">\[
\delta_t=\hat{v}_t-u_t,\forall t=1,2...,n
\]</span> (5)用<span
class="math inline">\(\{s_t\}^n_{t=1}\)</span>作为价值网络的输入，做反向传播计算梯度：
<span class="math display">\[
\nabla_{w}v(s_t;w_{now}),\forall t=1,2,...,n.
\]</span> (6)更新价值网络参数： <span class="math display">\[
w_{new}=w_{now}-\alpha\cdot\sum\limits_{t=1}^{n}\delta_t\cdot\nabla_wv(s_t;w_{now})
\]</span> (7)用<span
class="math inline">\(\{(s_t,a_t)\}_{t=1}^{n}\)</span>作为数据，做反向传播计算：
<span class="math display">\[
\nabla_\theta ln\pi(a_t|s_t;\theta_{now}),\forall t=1,2...n
\]</span> (8)做随机梯度上升更新策略网络参数： <span
class="math display">\[
\theta_{new}=\theta_{now}-\beta\cdot\sum\limits_{t=1}^{n}\gamma^{t-1}\cdot
\delta_t\cdot\nabla_\theta ln\pi(a_t|s_t;\theta_{now})
\]</span></p>
<p>减号是因为<span
class="math inline">\(\delta_t=\hat{v}_t-u_t\)</span>的形式和原策略梯度定理中的形式<span
class="math inline">\(u_t-\hat{v}_t\)</span>差个负号，其实上式依然是随机梯度上升。</p>
<h4 id="advantage-actor-critica2c">4.advantage actor-critic(A2C)</h4>
<p>(1)观测到当前状态<span
class="math inline">\(s_t\)</span>,根据策略网络做决策：<span
class="math inline">\(a_t\sim\pi(\cdot|s_t;\theta_{now})\)</span>,<strong>并让智能体执行动作<span
class="math inline">\(a_t\)</span></strong>;</p>
<p>(2)从环境中观测到新的状态<span
class="math inline">\(s_{t+1}\)</span>和奖励<span
class="math inline">\(r_t\)</span>;</p>
<p>(3)让价值网络打分： <span class="math display">\[
\hat{v}_t=v(s_t;w_{now}),\hat{v}_{t+1}=v(s_{t+1};w_{now})
\]</span> (4)计算TD目标和TD误差： <span class="math display">\[
\hat{y}_t=r_t+\gamma\cdot \hat{v}_{t+1},\delta_t=\hat{v}_t-\hat{y}_t
\]</span> (6)更新价值网络： <span class="math display">\[
w_{new}=w_{now}-\alpha\cdot\delta_t\cdot\nabla_wv(s_t;w_{now})
\]</span> (7)更新策略网络： <span class="math display">\[
\theta_{new}=\theta_{now}-\beta\cdot\delta_t\cdot\nabla_\theta
ln\pi(a_t|s_t;\theta_{now})
\]</span> 注：A2C的价值网络存在自举问题，对于<span
class="math inline">\(\hat{v}_{t+1}\)</span>，可通过目标网络<span
class="math inline">\(v(s;w^-)\)</span>近似；</p>
]]></content>
      <categories>
        <category>深度强化学习读书笔记</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>第六章、价值学习高级技巧</title>
    <url>/2024/04/12/%E7%AC%AC%E5%85%AD%E7%AB%A0%E3%80%81%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0%E9%AB%98%E7%BA%A7%E6%8A%80%E5%B7%A7/</url>
    <content><![CDATA[<hr />
<hr />
<hr />
<hr />
<p>DQN、SARSA都属于价值学习，本章介绍价值学习的高级技巧，它们可以应用于多种价值学习和策略学习方法中以提升算法的效果。</p>
<h4 id="经验回放">1.经验回放</h4>
<ul>
<li><strong>经验回放</strong>是指把智能体与环境交互的记录（即经验）<span
class="math inline">\([(s_t,a_t,r_t,s_{t+1}),(s_{t+1},a_{t+1},r_{t+1},s_{t+2}),...]\)</span>存储到一个缓存里，事后反复利用这些经验训练智能体，这个缓存就是<strong>经验回放缓存</strong>;</li>
<li>缓存的大小b是个超参数，需要人为指定，缓存中只保留最近b条数据，当缓存存满之后，删除最旧的数据，补充新的数据。在实践中，要等到回放缓存中有足够多的四元组时，才开始做经验回放更新DQN；</li>
<li>经验回放的优点：（1）每次更新从缓存中随机抽取一个四元组，消除了相关性；（2）重复利用经验，减小所需的样本数量；</li>
<li>经验回放的缺点：只适用于异策略，如Q学习、确定性策略梯度（DPG）等；不适用于同策略，如SARSA,REINFORCE,A2C等；</li>
<li>优先经验回放：给四元组样本设置权重，让权重较大的抽样概率大且学习率较小，更新时按照概率做加权随机抽样；</li>
</ul>
<p><span class="math display">\[
p_j\propto |\delta_j|+\epsilon=|Q(s_j,a_j;w_{now})-[r_j+\gamma\cdot
\underset{\alpha\in\mathcal A}{max}Q(s_{j+1},a;w_{now})]|+\epsilon
\]</span></p>
<h4 id="高估问题及其解决方法">2.高估问题及其解决方法</h4>
<ul>
<li><p>自举导致偏差传播；</p></li>
<li><p>最大化导致高估：在一系列数据中加入均值为0的随机噪声，将其中的最大值对噪声求期望，其结果大于原有数据的最大值；导致</p>
<p><span class="math inline">\(Q(s,a;w)\)</span>高估<span
class="math inline">\(Q_*(s,a)\)</span>.</p></li>
</ul>
<h4 id="使用目标网络切断自举">3.使用目标网络切断”自举“</h4>
<p>Q学习算法计算TD目标：<span
class="math inline">\(\hat{y}_j=r_j+\gamma\cdot
\underset{\alpha\in\mathcal A}{max}Q(s_{j+1},a;w)\)</span></p>
<p>其中的<span
class="math inline">\(Q(s_{j+1},a;w)\)</span>是DQN自己计算出的，属于自举，因此可以用一个新的网络来计算它，即目标网络<span
class="math inline">\(Q(s,a;w^-)\)</span>,其神经网络结构与DQN完全相同，但其参数<span
class="math inline">\(w^-\)</span>不同于<span
class="math inline">\(w\)</span>;</p>
<p>此时TD目标变为：<span class="math inline">\(\hat{y}_j=r_j+\gamma\cdot
\underset{\alpha\in\mathcal A}{max}Q(s_{j+1},a;w^-_{now})\)</span></p>
<p>其余计算步骤与常规DQN相同，用以下公式进行目标网络参数的更新：</p>
<p><span class="math inline">\(w^-_{new}=\tau \cdot
w_{new}+(1-\tau)\cdot w^-_{now}\)</span>,其中<span
class="math inline">\(\tau\in(0,1)\)</span>是需要手动调节的超参数。</p>
<h4 id="双q学习算法ddqn">4.双Q学习算法（DDQN)</h4>
<p>把最大化分成两个步骤：（1）选择最佳动作（在DQN中）：<span
class="math inline">\(a^*=\underset{\alpha\in\mathcal{A}}{argmax}Q(s_{j+1},a;w_{now})\)</span></p>
<p>​ （2）求值（在目标网络中）：<span
class="math inline">\(\hat{q}_{j+1}=Q(s_{j+1},a^*;w^-_{now})\)</span></p>
<p><strong>总结</strong>：</p>
<p><img src="/2024/04/12/第六章、价值学习高级技巧/image-20240413140249926.png"  alt="image-20240413140249926" style="zoom:50%;" /></p>
<hr />
<h4 id="对决网络">5.对决网络</h4>
<p>首先理清相关的概念：</p>
<p><strong>动作价值函数</strong>：<span
class="math inline">\(Q_\pi(s,a)=E[U_t|S_t=s,A_t=a]\)</span>,其中的期望是对<span
class="math inline">\(S_{t+1},A_{t+1},S_{t+2},A_{t+2},...,S_n,A_n\)</span>求的；</p>
<p><strong>最优动作价值函数</strong>：<span
class="math inline">\(Q_*(s,a)=\underset{\pi}{max}\ Q_\pi(s,a),\forall
s\in\mathcal{S},a\in\mathcal{A}\)</span></p>
<p><strong>状态价值函数</strong>：<span
class="math inline">\(V_\pi(s)=E_{A\sim\pi}[Q_\pi(s,A)],\forall s\in
\mathcal{S}\)</span></p>
<p><strong>最优状态价值函数</strong>：<span
class="math inline">\(V_*(s)=\underset{\pi}{max}V_{\pi}(s),\forall
s\in\mathcal{S}\)</span></p>
<p><strong>最优优势函数</strong>：<span
class="math inline">\(D_*(s,a)=Q_*(s,a)-V_*(s)\)</span></p>
<p><strong>最优优势函数定理</strong>：<span
class="math inline">\(Q_*(s,a)=V_*(s)+D_*(s,a)-\underset{a\in\mathcal{A}}{max}D_*(s,a),\forall
s\in\mathcal{S},a\in\mathcal{A}\)</span>,其中最后一项恒等于0；</p>
<hr />
<p><strong>对决网络的结构</strong>：</p>
<p>对决网络由两个神经网络组成，一个神经网络记作<span
class="math inline">\(D(s,a;w^D)\)</span>以近似最优优势函数<span
class="math inline">\(D_*(s,a)\)</span>;另一个神经网络记作<span
class="math inline">\(V(s;w^V)\)</span>以近似最优状态价值函数<span
class="math inline">\(V_*(s)\)</span>.两个神经网络共享部分卷积层，如图所示：</p>
<p><img src="/2024/04/12/第六章、价值学习高级技巧/image-20240413160528494.png"  alt="image-20240413160528494" style="zoom:50%;" /></p>
<p>于是： <span class="math display">\[
Q(s,a;w)=V(s;w^V)+D(s,a;w^D)-\underset{a\in\mathcal{A}}{max}D(s,a;w^D)
\]</span> 举例说明计算过程：动作空间为<span
class="math inline">\(\mathcal{A}=\{左、右、上\}\)</span>，输入状态<span
class="math inline">\(s\)</span>,由卷积神经网络提取特征向量，其（1）经”优势头“全连接神经网络映射为<span
class="math inline">\(|\mathcal{A}|\)</span>维向量，其元素代表每个动作的优势值，比如分别为：
<span class="math display">\[
D(s,左;w^D)=-90,D(s,右;w^D)=-420,D(s,上;w^D)=30
\]</span>
（2）经”状态价值头“全连接神经网络映射为一个标量，代表状态价值，比如<span
class="math inline">\(V(s;w^V)=300\)</span></p>
<p>由公式可知，<span
class="math inline">\(\underset{a\in\mathcal{A}}{max}D(s,a;w^D)=max\{-90,-420,-30\}=30\)</span>
<span class="math display">\[
Q(s,左;w)=300-90-30=180,Q(s,右;w)=300-420-30=-150,Q(s,上;w)=300+30-30=300
\]</span> 在实际实现中，一般使用mean代替max： <span
class="math display">\[
Q(s,a;w)=V(s;w^V)+D(s,a;w^D)-\underset{a\in\mathcal{A}}{mean}D(s,a;w^D)
\]</span></p>
<p>需要说明的是，对决网络和DQN都是输入状态s，输出每个动作的最优动作价值函数，只不过对决网络的参数为<span
class="math inline">\((w^D,w^V)\)</span>.因此两者的训练和决策完全相同，比如都可以使用目标网络、双Q学习训练，也会出现高估问题；</p>
<h4 id="噪声网络">6.噪声网络</h4>
<p>将神经网络中的参数<span class="math inline">\(w\)</span>改为<span
class="math inline">\(w=\mu+\sigma\cdot \xi\)</span>,其中<span
class="math inline">\(\mu,\sigma,\xi\)</span>的形状与<span
class="math inline">\(w\)</span>完全相同，<span
class="math inline">\(\mu\)</span>代表均值，<span
class="math inline">\(\sigma\)</span>代表标准差，<span
class="math inline">\(\xi\)</span>是随机噪声，它的每个元素独立从标准正态分布<span
class="math inline">\(\mathcal{N}(0,1)\)</span>中随机抽取。于是，DQN网络由<span
class="math inline">\(Q(s,a;w)\)</span>变成,<span
class="math inline">\(\widetilde{Q}(s,a,\xi;\mu,\sigma)=Q(s,a;\mu+\sigma\cdot\xi)\)</span>，参数为<span
class="math inline">\(\mu\)</span>和<span
class="math inline">\(\sigma\)</span>,每次要用到<span
class="math inline">\(Q\)</span>的时候都需要重新抽取<span
class="math inline">\(\xi\)</span>;</p>
<p>整个使用流程概述：</p>
<ol type="1">
<li><p>收集经验：</p>
<p>由于噪声DQN本身具有随机性，可以鼓励探索，故将原先的行为策略：<span
class="math inline">\(\epsilon-greedy\)</span>策略改为：</p>
<p><span class="math inline">\(a_t=\underset{a\in\mathcal{A}}{argmax}\
\widetilde{Q}(s,a,\xi;\mu,\sigma)\)</span></p></li>
<li><p>训练</p>
<p>训练过程需要使用优先经验回放、双Q学习、对决网络、噪声DQN这四种方法；</p></li>
<li><p>决策</p>
<p>做决策时不再需要噪声，将<span
class="math inline">\(\sigma\)</span>设为全0，只保留参数<span
class="math inline">\(\mu\)</span>,即用<span
class="math inline">\(Q(s,a;\mu)\)</span>做决策即可。</p></li>
</ol>
]]></content>
      <categories>
        <category>深度强化学习读书笔记</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>线性表</title>
    <url>/2024/04/13/%E7%BA%BF%E6%80%A7%E8%A1%A8/</url>
    <content><![CDATA[<hr />
<h3 id="线性表的基本概念">1.线性表的基本概念</h3>
<ul>
<li>线性表的定义：线性表是具有相同数据类型的n个数据元素的有限序列；</li>
<li>线性表按存储结构可分为顺序表和链表；</li>
</ul>
<h3 id="顺序表">2.顺序表</h3>
<p>线性表的顺序存储称为顺序表，其逻辑上相邻的两个元素在物理位置上也相邻，即逻辑顺序和物理顺序相同，可以随机存取，但插入操作需要将其后元素后移，删除操作需要将其后元素前移；</p>
<ul>
<li>顺序表插入操作的平均时间复杂度为：<span
class="math inline">\(O(n)\)</span></li>
<li>顺序表删除操作的平均时间复杂度为：<span
class="math inline">\(O(n)\)</span></li>
<li>顺序表按值查找的平均时间复杂度为：<span
class="math inline">\(O(n)\)</span></li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;climits&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span>  <span class="keyword">struct</span> </span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> * data;</span><br><span class="line">    <span class="type">int</span> MaxSize,length;</span><br><span class="line">&#125;SeqList;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ListInsert</span><span class="params">(SeqList &amp;L,<span class="type">int</span> i,<span class="type">int</span> e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (i&lt;<span class="number">1</span> || i&gt;L.length+<span class="number">1</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (L.length&gt;=L.MaxSize) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j=L.length;j&gt;=i;j--)</span><br><span class="line">    &#123;</span><br><span class="line">        L.data[j] = L.data[j<span class="number">-1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    L.data[i<span class="number">-1</span>] = e;</span><br><span class="line">    L.length++;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ListDelete</span><span class="params">(SeqList &amp;L,<span class="type">int</span> i,<span class="type">int</span> &amp;e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (i&lt;<span class="number">1</span>||i&gt;L.length) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    e = L.data[i<span class="number">-1</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j=i;j&lt;L.length;j++)</span><br><span class="line">    &#123;</span><br><span class="line">        L.data[j<span class="number">-1</span>]=L.data[j];</span><br><span class="line">    &#125;</span><br><span class="line">    L.length--;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">LocateElem</span><span class="params">(SeqList L,<span class="type">int</span> e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>;i&lt;L.length;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (L.data[i]==e)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> i+<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    </span><br><span class="line">    SeqList L;</span><br><span class="line">    L.MaxSize = <span class="number">50</span>;</span><br><span class="line">    L.data = <span class="keyword">new</span> <span class="type">int</span>[<span class="number">10</span>];</span><br><span class="line">    L.length = <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>;i&lt;<span class="number">10</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        L.data[i]=i;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>;i&lt;<span class="number">10</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        cout &lt;&lt; L.data[i] &lt;&lt;<span class="string">&quot;\t&quot;</span>;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;L.length=&quot;</span> &lt;&lt; L.length;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    cout &lt;&lt;endl;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="built_in">ListInsert</span>(L,<span class="number">5</span>,<span class="number">999</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>;i&lt;<span class="number">10</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        cout &lt;&lt; L.data[i] &lt;&lt;<span class="string">&quot;\t&quot;</span>;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;L.length=&quot;</span> &lt;&lt; L.length&lt;&lt;endl;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> s = <span class="built_in">LocateElem</span>(L,<span class="number">999</span>);</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;999的次序是&quot;</span> &lt;&lt; s &lt;&lt; endl;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>;i&lt;<span class="number">10</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        cout &lt;&lt; L.data[i] &lt;&lt;<span class="string">&quot;\t&quot;</span>;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;L.length=&quot;</span> &lt;&lt; L.length;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> e;</span><br><span class="line">    <span class="built_in">ListDelete</span>(L,<span class="number">5</span>,e);</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;返回 &quot;</span> &lt;&lt; e &lt;&lt;endl;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>;i&lt;<span class="number">10</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        cout &lt;&lt; L.data[i] &lt;&lt;<span class="string">&quot;\t&quot;</span>;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;L.length=&quot;</span> &lt;&lt; L.length;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="链表">3.链表</h3>
<h4 id="单链表">3.1 单链表</h4>
<p>线性表的链式存储称为单链表，对每个链表结点，除存放元素自身的信息外，还需要存放一个指向其后继结点的指针；链表的插入和删除不需要移动元素，时间复杂度为<span
class="math inline">\(O(1)\)</span>,但无法随机存取，存取的时间复杂度为<span
class="math inline">\(O(n)\)</span>。</p>
<ul>
<li><p>头插法建立单链表</p>
<p>若带头结点，操作为<code>s-&gt;data=x;s-&gt;next=L-&gt;next;L-&gt;next=s;</code>;</p>
<p>若不带头结点，操作为<code>s-&gt;next=L;L=s;</code>;</p>
<p>单个结点的插入时间为<span
class="math inline">\(O(1)\)</span>,建立表长为n的单链表总时间复杂度为<span
class="math inline">\(O(n)\)</span>;</p></li>
<li><p>尾插法建立单链表</p>
<p>需要尾指针的帮助<code>s-&gt;data=x;r-&gt;next=s;r=s;</code>时间复杂度为<span
class="math inline">\(O(n)\)</span>;</p></li>
<li><p>按序号查找结点，按值查找结点，求表长，使用while循环<code>p=p-&gt;next</code>遍历即可；时间复杂度为<span
class="math inline">\(O(n)\)</span>;</p></li>
<li><p>插入结点，删除结点，同理注意指针的变动顺序即可；时间复杂度为<span
class="math inline">\(O(n)\)</span>;</p></li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 链表结点及链表定义</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">LNode</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> data;</span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">LNode</span> * next;</span><br><span class="line">&#125;LNode,*LinkList;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 头插法建立单链表（带头结点）</span></span><br><span class="line"><span class="function">LinkList <span class="title">List_HeadInsert</span><span class="params">(LinkList &amp;L)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    LNode *s;<span class="type">int</span> x;</span><br><span class="line">    L = (LinkList)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(LNode));</span><br><span class="line">    L-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    cin &gt;&gt; x;</span><br><span class="line">    <span class="keyword">while</span> (x!=<span class="number">9999</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        s = (LNode*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(LNode));</span><br><span class="line">        s-&gt;data = x;</span><br><span class="line">        s-&gt;next = L-&gt;next;</span><br><span class="line">        L-&gt;next = s;</span><br><span class="line">        cin &gt;&gt; x;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> L;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 头插法建立单链表（不带头结点）</span></span><br><span class="line"><span class="function">LinkList <span class="title">List_HeadInsert_nohead</span><span class="params">(LinkList &amp;L)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    LNode *s;<span class="type">int</span> x;</span><br><span class="line">    L = (LNode*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(LNode));</span><br><span class="line">    L = <span class="literal">NULL</span>;</span><br><span class="line">    cin &gt;&gt; x;</span><br><span class="line">    <span class="keyword">while</span> (x!=<span class="number">9999</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        s = (LNode*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(LNode));</span><br><span class="line">        s-&gt;data = x;</span><br><span class="line">        s-&gt;next=L;</span><br><span class="line">        L = s;</span><br><span class="line">        cin &gt;&gt; x;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> L;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 尾插法建立单链表</span></span><br><span class="line"><span class="function">LinkList <span class="title">List_TailInsert</span><span class="params">(LinkList &amp;L)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> x;</span><br><span class="line">    L = (LinkList)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(LNode));</span><br><span class="line">    LNode *s,*r=L;</span><br><span class="line">    cin &gt;&gt; x;</span><br><span class="line">    <span class="keyword">while</span> (x!=<span class="number">9999</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        s = (LNode*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(LNode));</span><br><span class="line">        </span><br><span class="line">        s-&gt;data = x;</span><br><span class="line">        r-&gt;next = s;</span><br><span class="line">        r = s;</span><br><span class="line">        cin &gt;&gt; x;</span><br><span class="line">    &#125;</span><br><span class="line">    r-&gt;next=<span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">return</span> L;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 按序号查找结点值</span></span><br><span class="line"><span class="function">LNode *<span class="title">GetElem</span><span class="params">(LinkList L,<span class="type">int</span> i)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> j =<span class="number">1</span>;</span><br><span class="line">    LNode *p=L-&gt;next;</span><br><span class="line">    <span class="keyword">if</span> (i==<span class="number">0</span>) <span class="keyword">return</span> L;</span><br><span class="line">    <span class="keyword">if</span> (i&lt;<span class="number">1</span>) <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">while</span> (p&amp;&amp;j&lt;i)</span><br><span class="line">    &#123;</span><br><span class="line">        p = p-&gt;next;</span><br><span class="line">        j++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> p;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 按值查找结点</span></span><br><span class="line"><span class="function">LNode *<span class="title">LocateElem</span><span class="params">(LinkList L,<span class="type">int</span> e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    LNode * p = L-&gt;next;</span><br><span class="line">    <span class="keyword">while</span> (p&amp;&amp;p-&gt;data!=e)</span><br><span class="line">    &#123;</span><br><span class="line">        p = p-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> p;</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 插入结点操作</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">InsertNode</span><span class="params">(LinkList &amp;L,<span class="type">int</span> i,LNode *s)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    LNode * p = <span class="built_in">GetElem</span>(L,i<span class="number">-1</span>);</span><br><span class="line">    s-&gt;next = p-&gt;next;</span><br><span class="line">    p-&gt;next = s;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除结点操作</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">DeleteNode</span><span class="params">(LinkList &amp;L,<span class="type">int</span> i)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    LNode *p=<span class="built_in">GetElem</span>(L,i<span class="number">-1</span>);</span><br><span class="line">    LNode * q = p-&gt;next;</span><br><span class="line">    p-&gt;next = q-&gt;next;</span><br><span class="line">    <span class="built_in">free</span>(q);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 求表长</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">List_Length</span><span class="params">(LinkList L)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> length = <span class="number">0</span>;</span><br><span class="line">    LNode *p = L-&gt;next;</span><br><span class="line">    <span class="keyword">while</span> (p)</span><br><span class="line">    &#123;</span><br><span class="line">        length++;</span><br><span class="line">        p = p-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> length;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//头插法（带头结点）</span></span><br><span class="line">    <span class="comment">/*LinkList L;</span></span><br><span class="line"><span class="comment">    List_HeadInsert(L);</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    while(L)</span></span><br><span class="line"><span class="comment">    &#123;</span></span><br><span class="line"><span class="comment">        L = L-&gt;next;</span></span><br><span class="line"><span class="comment">        if(L)</span></span><br><span class="line"><span class="comment">        cout &lt;&lt; L-&gt;data &lt;&lt;&quot;\t&quot;;      </span></span><br><span class="line"><span class="comment">    &#125;*/</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//头插法（不带头结点）</span></span><br><span class="line">    <span class="comment">/*LinkList L_1;</span></span><br><span class="line"><span class="comment">    List_HeadInsert_nohead(L_1);</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    while(L_1)</span></span><br><span class="line"><span class="comment">    &#123;</span></span><br><span class="line"><span class="comment">        </span></span><br><span class="line"><span class="comment">        cout &lt;&lt; L_1-&gt;data &lt;&lt;&quot;\t&quot;;     </span></span><br><span class="line"><span class="comment">        L_1 = L_1-&gt;next; </span></span><br><span class="line"><span class="comment">    &#125;*/</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//尾插法（带头结点）</span></span><br><span class="line">    LinkList L;</span><br><span class="line">    <span class="built_in">List_TailInsert</span>(L);</span><br><span class="line">    LNode * L_ = L;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(L_)</span><br><span class="line">    &#123;</span><br><span class="line">        L_ = L_-&gt;next;</span><br><span class="line">        <span class="keyword">if</span>(L_)</span><br><span class="line">        cout &lt;&lt; L_-&gt;data &lt;&lt;<span class="string">&quot;\t&quot;</span>;      </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 按序号查找结点</span></span><br><span class="line">    LNode * L_GetElem =<span class="built_in">GetElem</span>(L,<span class="number">3</span>);</span><br><span class="line">    <span class="keyword">if</span> (L_GetElem) cout &lt;&lt; L_GetElem-&gt;data &lt;&lt; endl; </span><br><span class="line"></span><br><span class="line">    <span class="comment">// 按值查找结点</span></span><br><span class="line">    LNode * L_LocateElem =<span class="built_in">LocateElem</span>(L,<span class="number">444</span>);</span><br><span class="line">    <span class="keyword">if</span> (L_LocateElem&amp;&amp;L_LocateElem-&gt;data==<span class="number">444</span>) cout &lt;&lt; <span class="string">&quot;成功啦&quot;</span> &lt;&lt; endl; </span><br><span class="line"></span><br><span class="line">    <span class="comment">// 插入结点</span></span><br><span class="line">    LNode * s;</span><br><span class="line">    s = (LNode*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(LNode));</span><br><span class="line">    s-&gt;data = <span class="number">88888</span>;</span><br><span class="line">    s-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="built_in">InsertNode</span>(L,<span class="number">3</span>,s);</span><br><span class="line"></span><br><span class="line">    L_ = L;</span><br><span class="line">    <span class="keyword">while</span>(L_)</span><br><span class="line">    &#123;</span><br><span class="line">        L_ = L_-&gt;next;</span><br><span class="line">        <span class="keyword">if</span>(L_)</span><br><span class="line">        cout &lt;&lt; L_-&gt;data &lt;&lt;<span class="string">&quot;\t&quot;</span>;      </span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 删除结点</span></span><br><span class="line">    <span class="built_in">DeleteNode</span>(L,<span class="number">3</span>);</span><br><span class="line">    L_ = L;</span><br><span class="line">    <span class="keyword">while</span>(L_)</span><br><span class="line">    &#123;</span><br><span class="line">        L_ = L_-&gt;next;</span><br><span class="line">        <span class="keyword">if</span>(L_)</span><br><span class="line">        cout &lt;&lt; L_-&gt;data &lt;&lt;<span class="string">&quot;\t&quot;</span>;      </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; endl &lt;&lt; <span class="string">&quot;length=&quot;</span> &lt;&lt; <span class="built_in">List_Length</span>(L);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="双链表">3.2 双链表</h4>
<p>双链表结点中有两个指针prior和next，分别指向其前驱结点和后继结点；双链表插入和删除结点的时间复杂度为<span
class="math inline">\(O(1)\)</span>;</p>
<p><strong>插入结点</strong></p>
<p>(1)将结点s插入到结点p之后</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">S-&gt;next=p-&gt;next;</span><br><span class="line">p-&gt;next-&gt;prior=s;</span><br><span class="line">s-&gt;prior=p;</span><br><span class="line">p-&gt;next=s;</span><br></pre></td></tr></table></figure>
<p>1和2必须在4之前；</p>
<p>(2)将结点s插入结点p之前</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">s-&gt;next=p;</span><br><span class="line">s-&gt;prior=p-&gt;prior;</span><br><span class="line">p-&gt;prior-&gt;next=s;</span><br><span class="line">p-&gt;prior=s;</span><br></pre></td></tr></table></figure>
<p>2和3必须在4之前；</p>
<p><strong>删除结点</strong></p>
<p>（1）删除结点p的后继结点q</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">p-&gt;next=q-&gt;next;</span><br><span class="line">q-&gt;next-&gt;prior=p;</span><br><span class="line"><span class="built_in">free</span>(q);</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>删除结点q的前驱结点p</li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">p-&gt;prior-&gt;next=q;</span><br><span class="line">q-&gt;prior=p-&gt;prior;</span><br><span class="line"><span class="built_in">free</span>(p);</span><br></pre></td></tr></table></figure>
<h4 id="循环链表">3.3 循环链表</h4>
<ul>
<li><p>循环单链表：最后一个结点的指针指向头结点，判空操作<code>L-&gt;next==L;</code>,一般设尾指针；</p></li>
<li><p>循环双链表：尾结点的next指向头结点，头结点的prior指向尾结点；判空操作<code>p-&gt;prior==p-&gt;next==L;</code></p></li>
</ul>
]]></content>
      <categories>
        <category>重温数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>第四章、DQN与Q学习</title>
    <url>/2024/04/06/%E7%AC%AC%E5%9B%9B%E7%AB%A0%E3%80%81DQN%E4%B8%8EQ%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<hr />
<p>前面我们知道最优动作价值函数<span
class="math inline">\(Q_*(s_t,a_t)\)</span>可以预知选取<span
class="math inline">\(a_t\)</span>这个动作后回报<span
class="math inline">\(U_t\)</span>的期望的上限，但在实践中我们并不知道<span
class="math inline">\(Q_*\)</span>的函数表达式，因此DQN使用神经网络对其进行近似。</p>
<h4 id="dqndeep-q-network">DQN(Deep Q Network)</h4>
<p>深度Q网络DQN记作<span
class="math inline">\(Q(s,a;w)\)</span>,其结构如图所示，其中<span
class="math inline">\(w\)</span>代表神经网络的参数，学习的目标是对于所有的<span
class="math inline">\(s\)</span>和<span
class="math inline">\(a\)</span>, DQN的预测<span
class="math inline">\(Q(s,a;w)\)</span></p>
<p>尽量接近<span
class="math inline">\(Q_*(s,a)\)</span>;DQN的输入是状态<span
class="math inline">\(s\)</span>,输出是<span
class="math inline">\(|\mathcal{A}
|\)</span>维的向量，每一个元素对应动作空间中每个动作的<span
class="math inline">\(Q\)</span>值；</p>
<p><img src="/2024/04/06/第四章、DQN与Q学习/image-20240407110019564.png"  alt="image-20240407110019564" style="zoom:50%;" /></p>
<h4 id="时间差分tdtemporal-difference算法">时间差分（TD，temporal
difference）算法</h4>
<p>训练DQN常使用TD算法，因此先了解TD算法；</p>
<p>假设原预测为<span
class="math inline">\(\hat{q}\)</span>,部分观测为<span
class="math inline">\(r\)</span>，补充预测为<span
class="math inline">\(\hat{q}’\)</span>，那么<strong>TD目标</strong><span
class="math inline">\(\hat{y}=r+\hat{q}’\)</span>,两者之间的差值<span
class="math inline">\(\delta=(\hat{q}-\hat{y})\)</span>称为<strong>TD误差</strong>，</p>
<p>令损失函数为<span
class="math inline">\(L(w)=\frac{1}{2}\delta^2=\frac{1}{2}(\hat{q}-\hat{y})^2\)</span>,虽然<span
class="math inline">\(\hat{y}=r+\hat{q}’\)</span>的<span
class="math inline">\(\hat{q}’\)</span>包含神经网络参数<span
class="math inline">\(w\)</span>，是<span
class="math inline">\(w\)</span>的函数，但通常情况下对损失函数求参数的梯度时将<span
class="math inline">\(\hat{y}\)</span>看作常数，因此，<span
class="math inline">\(w=w-\alpha\cdot\delta\cdot\nabla
_{w}\hat{q}=w-\alpha\cdot(\hat{q}-\hat{y}{})\cdot\nabla
_{w}\hat{q}\)</span></p>
<h4 id="用td算法之q学习算法训练dqn">用TD算法之Q学习算法训练DQN</h4>
<p><span class="math display">\[
U_t = R_t+\gamma R_{t+1}+...+\gamma^{n-t}R_n
\]</span></p>
<p><span class="math display">\[
U_{t+1} = R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-t-1}R_n
\]</span></p>
<p>故 <span class="math display">\[
U_t=R_t+\gamma U_{t+1}
\]</span> 又因为最优动作价值函数<span
class="math inline">\(Q_*(s_t,a_t)=\underset{\pi }{max}\
E[U_t|S_t=s_t,A_t=a_t]\)</span></p>
<p>经过一系列数学推导： <span class="math display">\[
Q_*(s_t,a_t)=E_{S_{t+1}\sim
p(\cdot|s_t,a_t)}[R_t+\gamma\cdot\underset{A\in\mathcal A}{max}\
Q_*(S_{t+1},A)|S_t=s_t,A_t=a_t ]
\]</span>
通过采样的方法可以对这个期望做蒙特卡洛近似，采样的目的是为了收集四元组<span
class="math inline">\((s_t,a_t,r_t,s_{t+1})\)</span>,这个<strong>四元组</strong>产生的流程是：</p>
<p>在当前状态<span class="math inline">\(s_t\)</span>下执行动作<span
class="math inline">\(a_t\)</span>,这是已知的两个数据，然后环境通过状态转移函数<span
class="math inline">\(p(s_{t+1}|s_t,a_t)\)</span>计算出新的状态<span
class="math inline">\(s_{t+1}\)</span>,(这本质上就是蒙特卡洛近似的过程)，这样，奖励<span
class="math inline">\(r_t=r(s_t,a_t,s_{t+1})\)</span>,至此得到四元组<span
class="math inline">\((s_t,a_t,r_t,s_{t+1})\)</span>；</p>
<p>于是有： <span class="math display">\[
Q_*(s_t,a_t)\approx r_t+\gamma\cdot\underset{a\in\mathcal A}{max}\
Q_*(s_{t+1},a)
\]</span> 我们并不知道<span
class="math inline">\(Q_*\)</span>的值，因此用前面所讲的DQN神经网络对其进行近似：
<span class="math display">\[
Q(s_t,a_t;w)\approx r_t+\gamma\cdot\underset{a\in\mathcal A}{max}\
Q(s_{t+1},a;w)
\]</span> 左边即前面所说的原预测<span
class="math inline">\(\hat{q}\)</span>,右边两项分别为部分观测<span
class="math inline">\(r\)</span>,补充预测<span
class="math inline">\(\hat{q}’\)</span>，两项之和为<strong>TD目标</strong><span
class="math inline">\(\hat{y}=r+\hat{q}’\)</span>，左边减右边即<strong>TD误差</strong>，训练DQN的目的是尽量使得左边趋近右边；</p>
<h4 id="dqn训练的基本流程">DQN训练的基本流程</h4>
<p>首先需要明确的是，训练DQN只需要四元组<span
class="math inline">\((s_t,a_t,r_t,s_{t+1})\)</span>，DQN属于异策略，即控制智能体与环境交互以收集用于训练的四元组数据的<strong>策略</strong>和我们正在优化的<strong>策略</strong>可以不同（DQN中并没有”显式“的要优化的策略，实际上，我们要优化的<span
class="math inline">\(Q(s,a;w)\)</span>可以看成”策略“），前者称为<strong>行为策略</strong>，后者称为<strong>目标策略</strong>；</p>
<ul>
<li><p><strong>第一步：使用行为策略收集训练数据</strong></p>
<p>行为策略可以是任何策略，常用的为<span
class="math inline">\(\epsilon-\)</span>策略： <span
class="math display">\[
\begin{cases}
  argmax_a\ Q(s_t,a;w)&amp; \text{  } 以概率(1-\epsilon)  \\
  均匀抽取\mathcal A中的每一个动作&amp; \text{  } 以概率\epsilon
\end{cases}
\]</span></p></li>
</ul>
<p>把收集到的一条条四元组<span
class="math inline">\((s_t,a_t,r_t,s_{t+1})\)</span>存入缓存，称为”经验回放缓存“；</p>
<ul>
<li><p><strong>第二步：更新参数w</strong></p>
<p>从经验回放缓存中随机抽取一个四元组，记作<span
class="math inline">\((s_j,a_j,r_j,s_{j+1})\)</span>,原参数为<span
class="math inline">\(w_{now}\)</span>,更新后为<span
class="math inline">\(w_{new}\)</span></p>
<p>（1）对DQN神经网络做正向传播，得到Q值，即原预测<span
class="math inline">\(\hat{q_j}=Q(s_j,a_j;w_{now})\)</span>和补充预测<span
class="math inline">\(\hat{q}_{j+1}=\underset{a\in\mathcal
A}{max}Q(s_{j+1},a;w_{now})\)</span>；</p></li>
</ul>
<p>​ (2) 计算TD目标和TD误差： <span class="math display">\[
\hat{y_j}=r_j+\gamma\cdot\hat{q}_{j+1}
\]</span></p>
<p><span class="math display">\[
\delta_j=\hat{q_j}-\hat{y_j}
\]</span></p>
<p>​ （3）对DQN做反向传播，计算梯度 <span class="math display">\[
g_j=\nabla_wQ(s_j,a_j;w_{now})
\]</span> ​ 梯度的形状和<span class="math inline">\(w\)</span>相同；</p>
<p>（4）通过梯度下降更新DQN的参数w <span class="math display">\[
w_{new}=w_{now}-\alpha\cdot\delta_j\cdot g_j
\]</span></p>
<h4 id="传统表格形式的q学习">传统表格形式的Q学习</h4>
<p>DQN其实是神经网络形式的Q学习，现在我们介绍传统的表格形式的Q学习，它只适用于状态集合<span
class="math inline">\(\mathcal S\)</span>和动作集合<span
class="math inline">\(\mathcal A\)</span>都有限的情况。</p>
<p><img src="/2024/04/06/第四章、DQN与Q学习/image-20240407152338113.png"  alt="image-20240407152338113" style="zoom:50%;" /></p>
<p>传统Q学习理解就是，不断更新如上图所示的表格，每次更新表格中一个元素值，使得其中的元素值<span
class="math inline">\(\widetilde{Q}\)</span>不断趋近于<span
class="math inline">\(Q_*\)</span></p>
<ul>
<li><p><strong>第一步：使用行为策略收集训练数据</strong></p>
<p>行为策略可以是任何策略，常用的为<span
class="math inline">\(\epsilon-\)</span>策略： <span
class="math display">\[
\begin{cases}
  argmax_a\ \widetilde{Q}(s_t,a)&amp; \text{  } 以概率(1-\epsilon)  \\
  均匀抽取\mathcal A中的每一个动作&amp; \text{  } 以概率\epsilon
\end{cases}
\]</span></p></li>
</ul>
<p>把收集到的一条条四元组<span
class="math inline">\((s_t,a_t,r_t,s_{t+1})\)</span>存入缓存，称为”经验回放缓存“；</p>
<ul>
<li><p><strong>第二步：更新参数w</strong></p>
<p>从经验回放缓存中随机抽取一个四元组，记作<span
class="math inline">\((s_j,a_j,r_j,s_{j+1})\)</span>,设当前表格为<span
class="math inline">\(\widetilde{Q}_{now}\)</span>,这样的一条四元组可以更新表格中的一个”格子“</p>
<p>即：<span
class="math inline">\(\widetilde{Q}_{now}(s_j,a_j)\)</span>更新为<span
class="math inline">\(\widetilde{Q}_{new}(s_j,a_j)\)</span>,更新过程如下：</p>
<p>（1）旧表格<span
class="math inline">\(\widetilde{Q}_{now}\)</span>中<span
class="math inline">\((s_j,a_j)\)</span>位置上数据为<span
class="math inline">\(\hat{q_j}=\widetilde{Q}_{now}(s_j,a_j)\)</span></p>
<p>第<span class="math inline">\(s_{j+1}\)</span>行中最大值为<span
class="math inline">\(\hat{q}_{j+1}=\underset{a\in\mathcal A}{max}\
\widetilde{Q}_{now}(s_{j+1},a)\)</span>；</p></li>
</ul>
<p>​ (2) 计算TD目标和TD误差： <span class="math display">\[
\hat{y_j}=r_j+\gamma\cdot\hat{q}_{j+1}
\]</span></p>
<p><span class="math display">\[
\delta_j=\hat{q_j}-\hat{y_j}
\]</span></p>
<p>​ （3）更新表格中<span
class="math inline">\((s_j,a_j)\)</span>位置上的元素： <span
class="math display">\[
\widetilde{Q}_{new}(s_j,a_j)=(1-\alpha)\cdot\widetilde{Q}_{now}(s_j,a_j)+\alpha\cdot\hat{y_j}=\widetilde{Q}_{now}(s_j,a_j)-\alpha\cdot\delta_{j}
\]</span>
可以看到传统的表格形式的Q学习在算出各个值之后就直接以简单的比例形式去更新Q值，这种简单的更新方法的效果注定比较一般。</p>
<h4
id="同策略on-policy与异策略off-policy">同策略(on-policy)与异策略(off-policy)</h4>
<ul>
<li><p>行为策略(behavior
policy):控制智能体与环境交互，收集经验数据；</p></li>
<li><p>目标策略(target
policy):我们正在优化的策略；比如在DQN中，目标策略为： <span
class="math display">\[
a_t=\underset{a}{argmax}\ Q(s_t,a;w)
\]</span></p></li>
</ul>
<h4 id="经验回放">经验回放</h4>
<p>将智能体与环境交互的记录暂时保存，然后从中采样和学习的训练方式称为<strong>经验回放</strong>，需要注意的是，经验回放只适用于异策略，如DQN；不适用于同策略，为什么呢，这就像是尝试在当前的工作项目上使用你几年前旧工作的解决方案。</p>
<p><a href="https://Acoder123wew.github.io/">DQN代码实现</a></p>
]]></content>
      <categories>
        <category>深度强化学习读书笔记</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
</search>
